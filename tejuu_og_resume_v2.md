# ðŸ“„ Data Engineer Resume â€“ Sai Tejaswini Thumpala (Tejuu)

**LinkedIn:** linkedin.com/in/saitejaswinithumpala
**Email:** saitejaswinithumpala@gmail.com
**Phone:** 469-473-8568
**Location:** Dallas, TX

---

## Summary

Data Engineer with 6+ years' experience designing and optimizing large-scale data pipelines and modern data warehouses. Skilled in Azure (Synapse, Data Factory, Databricks) and Microsoft Fabric with expertise in PySpark, dbt, SQL, Power BI, and Tableau. Proven record delivering medallion architectures, automating ELT workflows, and enabling enterprise-wide analytics and reporting across healthcare, banking, and retail domains.

---

## Experience

**Data Engineer | Central Bank of Missouri | Dec 2023 â€“ Present**
Built enterprise-scale data pipelines using Microsoft Fabric and Azure Data Factory, migrating 12M+ legacy records to Azure SQL with automated validation, reducing manual processing by 50% and improving data accuracy to 99.8%.
Created 50+ fact/dimension tables in dbt for governed data warehouse supporting Power BI dashboards, cutting ETL processing time by 60% and enabling self-service analytics for 200+ business users.
Migrated legacy SSIS packages to Azure Data Factory and modernized ETL workflows with ARM templates, improving maintainability by 40% and reducing deployment time from 2 hours to 15 minutes.
Optimized Azure Synapse warehouse through intelligent partitioning and columnstore indexing, reducing data refresh from 4 hours to 45 minutes and improving query throughput by 200%.
Established row-level security and data-quality validation frameworks achieving 99.8% accuracy and 100% regulatory compliance, supporting SOX audits and enabling secure multi-tenant analytics.

**Data Engineer | Stryker | Jan 2022 â€“ Dec 2023**
Developed PySpark pipelines in Azure Databricks processing 15M+ records monthly with Delta Lake optimization, cutting runtime by 45% and reducing cloud costs by 35%.
Contributed to migration of global sales data from Oracle/Excel to Azure Synapse through ADF orchestration, unlocking $2M+ in reporting efficiency gains and enabling real-time analytics for 500+ field users.
Built partitioned ELT pipelines with optimized job clusters and Delta Lake ACID transactions, supporting 500+ field users while achieving 35% cost reduction and 99.9% data consistency.
Created comprehensive data lineage documentation and KPI guardrails using Azure Purview, accelerating month-end close by 30% and improving trust in CFO reporting packages.

**Product Data Analyst | CVS Health | May 2020 â€“ Jan 2022**
Migrated pharmacy reporting from Access to Azure SQL and automated ETL to Power BI, reducing manual reconciliation by 40% and improving data accuracy to 98.5% across 2,000+ pharmacy locations.
Developed Python data processing scripts for large-scale pharmacy data, implementing incremental processing patterns and improving data accuracy by 15% with automated quality checks.
Tuned complex SQL transformations and Power Query refreshes, saving 25-30 analyst hours monthly and stabilizing recurring dashboards with 99.5% uptime.

**Data Analyst | Colruyt IT Group | May 2018 â€“ Dec 2019**
Consolidated Excel/Access reports into Power BI and Tableau dashboards with reusable DAX and SQL models, standardizing finance KPIs and reducing report generation time by 50%.
Designed dimensional models for finance and merchandising using SQL Server, cutting month-end close cycles by 30% and improving data consistency to 99.2%.

---

## Skills

Data Engineering & ETL/ELT: Databricks, PySpark, Azure Data Factory, dbt, Microsoft Fabric, Delta Lake, SSIS
Cloud Platforms: Azure (Synapse, Data Factory, DevOps), Microsoft Fabric, AWS (S3, Glue, Lambda)
Databases & Storage: SQL Server, Azure SQL, Delta Lake, Oracle, PostgreSQL
Analytics & BI: Power BI, DAX, Tableau, KPI Dashboards, Data Modeling
Programming & Automation: Python, PySpark, SQL, Jinja, Bash
DevOps & Orchestration: Azure DevOps, GitHub Actions, CI/CD, ARM Templates, Terraform (basic)
Governance & Quality: Data Validation, Lineage, Row-Level Security, Schema Drift Monitoring

---

## Education

**MS Data Science** | 2020  
University of North Texas, Denton, TX

**B.Tech Information Technology** | 2018  
JNTU, Hyderabad, India

## Certifications

Microsoft Certified: Azure Data Engineer Associate (2023)  
Databricks Certified Data Engineer Professional (2022)  
Microsoft Certified: Azure Fundamentals (2021)

## Key Achievements

- Optimized data pipelines cutting overall runtime by 45% and cloud costs by 35% across three enterprise environments
- Delivered enterprise warehouse solutions supporting 500+ users across healthcare, banking, and retail industries
- Achieved 99.8% data accuracy and 100% regulatory compliance across all data platforms and reporting systems
- Migrated 12M+ legacy records to modern Azure architecture with zero data loss and 50% processing improvement

---

# ðŸ“‘ Resume Optimization Framework â€“ v2

## 1. Structure & Bullet Rules

**5-4-3-2 Rule**

- Central Bank â†’ 5 bullets | Present tense | Data modernization + self-service focus | Core tech (Fabric, dbt, Power BI)
- Stryker â†’ 4 bullets | Past tense | Data engineering + migration focus | PySpark + Databricks + Azure
- CVS â†’ 3 bullets | Past tense | Analytics enablement + optimization | Power BI + SQL + PySpark
- Colruyt â†’ 2 bullets | Past tense | BI consolidation outcomes | Power BI + SQL | Efficiency gains

**Character Constraint Rule**

- Each bullet = **220â€“240 characters**
- Never <215, never >240

**ATS-Optimized Bullet Structure**

- **Action Verb + Technology + Business Impact + Quantified Result**
- Example: "Engineered Azure Data Factory pipelines processing 12M+ records, reducing manual intervention by 40%"
- Always include specific technologies (Databricks, PySpark, Azure Synapse)
- Always include measurable outcomes (percentages, scale numbers, time savings)
- Use industry-standard terminology for ATS keyword matching

---

## 2. Domain Adaptation Layer

Adapt **vocabulary + emphasis** per JD:

- Healthcare/Compliance â†’ audit-ready datasets, regulatory workflows, compliance monitoring
- Retail/Audit â†’ financial integrity, recovery audits, supplier agreements, reconciliation
- Tech/Data Platforms â†’ streaming pipelines, real-time analytics, Kafka/Event Hubs, ML integration
- Finance/AI/ML â†’ PySpark models, AWS Glue, risk compliance, forecasting, ML deployment

---

## 3. Skills Optimization Layer

**Always keep 6 OG categories:**

1. Data Engineering & ELT
2. Cloud Platforms
3. Databases & Storage
4. Analytics & BI
5. DevOps & Orchestration
6. Governance & Security

**JD Alignment Examples:**

- **Azure-Heavy JD**

  - Data Engineering & ELT: Databricks, PySpark, ADF, dbt, Microsoft Fabric
  - Cloud Platforms: Azure (Synapse, Data Factory, DevOps), Microsoft Fabric
  - Databases & Storage: SQL Server, Delta Lake, Azure SQL
  - Analytics & BI: Power BI, DAX, Tableau
  - DevOps & Orchestration: Azure DevOps, GitHub Actions, CI/CD
  - Governance & Security: Data Lineage, Row-Level Security, Data Quality Validation

- **AWS-Heavy JD**

  - Data Engineering & ELT: PySpark, dbt, AWS Glue, Airflow
  - Cloud Platforms: AWS (Glue, Lambda, S3, Redshift), Azure (basic)
  - Databases & Storage: PostgreSQL, SQL Server, Redshift
  - Analytics & BI: Tableau, Power BI
  - DevOps & Orchestration: GitHub Actions, Docker, Kubernetes
  - Governance & Security: Data Lineage, PII Masking, Compliance Frameworks

- **BI/Analytics-Heavy JD**

  - Data Engineering & ELT: SQL, dbt, Power BI, Azure Data Factory
  - Cloud Platforms: Azure (Synapse, Data Factory), Microsoft Fabric
  - Databases & Storage: SQL Server, Delta Lake
  - Analytics & BI: Power BI, DAX, Tableau, KPI Dashboards
  - DevOps & Orchestration: Azure DevOps, GitHub Actions
  - Governance & Security: Data Quality Validation, Row-Level Security, Lineage Tracking

---

## 4. Verb Rotation Rule

Rotate verbs to avoid repetition: **Engineer, Optimize, Design, Implement, Develop, Build, Create, Scale, Orchestrate, Modernize, Migrate, Consolidate, Deploy, Automate, Streamline**

## 5. Enhanced ATS Optimization

**Critical ATS Keywords to Include:**
- **Data Engineering:** ETL/ELT, data pipelines, data warehousing, data lakes, data modeling
- **Cloud Technologies:** Azure Synapse, Databricks, AWS Glue, Snowflake, Google BigQuery
- **Programming:** Python, PySpark, SQL, Scala, Java
- **Big Data:** Apache Spark, Hadoop, Kafka, Delta Lake, Apache Airflow
- **DevOps:** CI/CD, Docker, Kubernetes, Terraform, Git
- **Certifications:** AWS, Azure, Google Cloud certifications

**Missing Elements to Consider Adding:**
- **Certifications Section** (if applicable)
- **Projects Section** (for additional technical depth)
- **Technical Skills Matrix** (proficiency levels)
- **Industry-Specific Keywords** based on target roles

## 6. STAR Method Integration

Structure bullets using Situation-Task-Action-Result:
- **Situation:** Business context/problem
- **Task:** Your responsibility
- **Action:** Specific technologies and methods used
- **Result:** Quantified business impact

---

## 7. Metrics Bank Rule

Every bullet ties to outcome + metric:

- % improvements (25% efficiency boost, 45% runtime reduction)
- Scale (15M+ transactions, 12M+ records)
- Time savings (25-30 hours monthly)
- Financial impact ($2M+ revenue opportunities)
- Adoption (1K+ users, 95% user satisfaction)

---

# ðŸ“‹ JD â†’ Resume Adaptation Checklist

1. **Read JD closely** â†’ Highlight cloud platform (AWS vs Azure), pipeline type (batch vs streaming), domain (healthcare, retail, finance).
2. **Apply Domain Layer** â†’ Swap vocabulary to mirror JD (audit, compliance, ML, streaming, etc).
3. **Reorder Skills** â†’ Keep 6 categories but move JD-priority tools to the front.
4. **Rewrite Bullets** â†’ Use 5-4-3-2 rule, adjust tech names, keep impact + metrics fixed.
5. **Rotate Verbs** â†’ Swap starting verbs per bullet to keep flow strong.
6. **Check Character Count** â†’ Ensure 220â€“240 characters per bullet.
7. **ATS Optimization** â†’ Include critical keywords, use industry terminology, avoid acronyms without expansion.
8. **STAR Method Check** â†’ Each bullet has clear situation, task, action, result structure.
9. **Technical Depth** â†’ Include specific tools, frameworks, and methodologies.
10. **Final Scan** â†’ ATS keywords match JD, measurable outcomes present, tense alignment correct.

---

# ðŸŽ¯ JD-Specific Summary Templates

**Azure-Heavy JD (Microsoft ecosystem, enterprise)**
Data Engineer with 6+ years' experience designing and scaling data platforms on Azure and Microsoft Fabric. Skilled in PySpark, Databricks, Power BI, and dbt with proven success optimizing costs, enabling self-service analytics, and delivering enterprise-scale data solutions at 500+ user scale.

**AWS-Heavy JD (Moody's, Possible Finance, startups)**
Data Engineer with 6+ years' experience building ELT pipelines on AWS using Glue, Lambda, S3, and Redshift. Expertise in PySpark, dbt, and SQL with a strong record of optimizing performance, reducing costs, and delivering self-service analytics platforms across finance and healthcare.

**BI/Analytics-Heavy JD (Analytics Engineer / BI Dev focus)**
Data Engineer with 6+ years' experience bridging data engineering and business intelligence. Designed Power BI dashboards, dbt models, and automated ETL pipelines powering self-service analytics for 500+ users. Skilled in Microsoft Fabric, Azure, and governance frameworks to deliver accurate, business-ready insights.

---
