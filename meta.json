[
  {
    "id": "mlops_pipeline_0",
    "text": "# MLOps Pipeline Architecture\n\n## Overview\nMLOps (Machine Learning Operations) encompasses the practices and tools needed to deploy, monitor, and maintain machine learning models in production environments.\n\n## Core Components\n\n### 1. Model Development\n- **Experiment Tracking**: Use MLflow, Weights & Biases, or TensorBoard\n- **Version Control**: Track code, data, and model versions\n- **Reproducibility**: Ensure consistent environments and dependencies\n\n### 2. Model Training Pipeline\n- **Data Pipeline**: Automated data ingestion, validation, and preprocessing\n- **Training Orchestration**: Use Airflow, Prefect, or Kubeflow Pipelines\n- **Hyperparameter Optimization**: Automated tuning with Optuna or Ray Tune\n- **Model Validation**: Automated testing and performance evaluation\n\n### 3. Model Deployment\n- **Containerization**: Package models in Docker containers\n- **Orchestration**: Deploy on Kubernetes or cloud services\n- **API Gateway**: Expose models through REST or GraphQL APIs\n- **Load Balancing**: Distribute traffic across multiple model instances\n\n### 4. Model Monitoring\n- **Performance Monitoring**: Track accuracy, latency, and throughput\n- **Data Drift Detection**: Monitor input data distribution changes\n- **Model Drift Detection**: Track model performance degradation\n- **Alerting**: Set up notifications for critical issues\n\n## MLflow Integration\n\n### Experiment Tracking\n```python\nimport mlflow\n\n# Start experiment\nmlflow.set_experiment(\"customer-churn-prediction\")\n\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_param(\"epochs\", 100)\n    \n    # Train model\n    model = train_model(X_train, y_train)\n    \n    # Log metrics\n    accuracy = evaluate_model(model, X_test, y_test)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n```\n\n### Model Registry\n- **Model Versioning**: Track different model versions\n- **Staging Workflow**: Promote models through dev → staging → prod\n- **Model Metadata**: Store model descriptions, tags, and lineage\n- **Automated Deployment**: Trigger deployments based on model performance\n\n## Kubernetes Deployment\n\n### Model Serving with KServe\n```yaml\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: customer-churn-model\nspec:\n  predictor:\n    sklearn:\n      storageUri: gs://model-registry/churn-model/1\n      resources:\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        limits:\n          cpu: \"2\"\n          memory: \"4Gi\"\n```\n\n### Auto-scaling Configuration\n- **Horizontal Pod Autoscaler**: Scale based on CPU/memory usage\n- **Vertical Pod Autoscaler**: Optimize resource allocation\n- **Custom Metrics**: Scale based on request latency or queue depth\n\n## Data Pipeline Integration\n\n### Feature Engineering Pipeline\n- **Feature Store**: Centralized feature management (Feast, Tecton)\n- **Feature Validation**: Ensure feature quality and consistency\n- **Feature Serving**: Low-latency feature retrieval for inference\n- **Feature Lineage**: Track feature dependencies and transformations\n\n### Data Validation\n- **Schema Validation**: Ensure data conforms to expected schema\n- **Statistical Validation**: Check data distributions and anomalies\n- **Temporal Validation**: Monitor data freshness and completeness\n- **Quality Metrics**: Track data quality scores over time\n\n## Monitoring and Observability\n\n### Performance Metrics\n- **Latency**: Track prediction response times\n- **Throughput**: Monitor requests per second\n- **Error Rates**: Track prediction failures and exceptions\n- **Resource Usage**: Monitor CPU, memory, and GPU utilization\n\n### Model Quality Metrics\n- **Accuracy**: Track model performance on validation data\n- **Precision/Recall**: Monitor classification metrics\n- **A/B Testing**: Compare model versions in production\n- **Statistical Significance**: Ensure metric differences are meaningful\n\n### Data Drift Detection\n- **Statistical Tests**: Use KS test, PSI, or KL divergence\n- **Feature Importance**: Track changes in feature importance\n- **Distribution Monitoring**: Visualize feature distributions over time\n- **Alerting Thresholds**: Set up alerts for significant drift\n\n## Best Practices\n\n### Model Lifecycle Management\n- **Automated Retraining**: Trigger retraining based on performance degradation\n- **Gradual Rollout**: Deploy new models to small traffic percentages first\n- **Rollback Strategy**: Quick rollback to previous model version\n- **Blue-Green Deployment**: Zero-downtime model updates\n\n### Security and Compliance\n- **Model Encryption**: Encrypt models at rest and in transit\n- **Access Control**: Implement role-based access to models and data",
    "metadata": {
      "tags": [
        "mlops",
        "mlflow",
        "deployment",
        "monitoring"
      ],
      "persona": "ai",
      "file_path": "ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md"
    }
  },
  {
    "id": "mlops_pipeline_1",
    "text": "performance on validation data\n- **Precision/Recall**: Monitor classification metrics\n- **A/B Testing**: Compare model versions in production\n- **Statistical Significance**: Ensure metric differences are meaningful\n\n### Data Drift Detection\n- **Statistical Tests**: Use KS test, PSI, or KL divergence\n- **Feature Importance**: Track changes in feature importance\n- **Distribution Monitoring**: Visualize feature distributions over time\n- **Alerting Thresholds**: Set up alerts for significant drift\n\n## Best Practices\n\n### Model Lifecycle Management\n- **Automated Retraining**: Trigger retraining based on performance degradation\n- **Gradual Rollout**: Deploy new models to small traffic percentages first\n- **Rollback Strategy**: Quick rollback to previous model version\n- **Blue-Green Deployment**: Zero-downtime model updates\n\n### Security and Compliance\n- **Model Encryption**: Encrypt models at rest and in transit\n- **Access Control**: Implement role-based access to models and data\n- **Audit Logging**: Track all model access and predictions\n- **GDPR Compliance**: Handle data privacy and right to be forgotten\n\n### Cost Optimization\n- **Resource Right-sizing**: Optimize CPU/memory allocation\n- **Spot Instances**: Use spot instances for training workloads\n- **Model Compression**: Reduce model size with quantization or pruning\n- **Caching**: Cache predictions for repeated queries\n\n## Tools and Platforms\n\n### Open Source\n- **MLflow**: Experiment tracking and model registry\n- **Kubeflow**: End-to-end ML workflow orchestration\n- **Seldon Core**: Model serving and deployment\n- **Evidently**: Model monitoring and drift detection\n\n### Cloud Platforms\n- **Azure ML**: Managed ML platform with MLOps capabilities\n- **AWS SageMaker**: End-to-end ML platform\n- **Google Vertex AI**: Managed ML platform with AutoML\n- **Databricks**: Unified analytics platform for ML\n\n## Implementation Strategy\n\n### Phase 1: Foundation\n- Set up experiment tracking and model registry\n- Implement basic CI/CD pipeline\n- Deploy models as REST APIs\n\n### Phase 2: Automation\n- Automate model training and validation\n- Implement automated deployment pipelines\n- Add basic monitoring and alerting\n\n### Phase 3: Advanced Features\n- Implement advanced monitoring and drift detection\n- Add A/B testing and gradual rollout capabilities\n- Optimize for cost and performance",
    "metadata": {
      "tags": [
        "mlops",
        "mlflow",
        "deployment",
        "monitoring"
      ],
      "persona": "ai",
      "file_path": "ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md"
    }
  },
  {
    "id": "rag_systems_0",
    "text": "# RAG Systems Architecture\n\n## Overview\nRetrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to provide more accurate and up-to-date responses.\n\n## Key Components\n\n### 1. Document Ingestion Pipeline\n- **Text Chunking**: Split documents into overlapping chunks (typically 512-1024 tokens)\n- **Embedding Generation**: Use models like `text-embedding-3-small` or `text-embedding-ada-002`\n- **Vector Storage**: Store embeddings in vector databases (Pinecone, Weaviate, FAISS)\n\n### 2. Retrieval Strategy\n- **Similarity Search**: Use cosine similarity or dot product for semantic matching\n- **Hybrid Search**: Combine semantic search with keyword-based BM25\n- **Re-ranking**: Apply cross-encoder models to improve relevance\n\n### 3. Generation Process\n- **Context Injection**: Include retrieved chunks in the prompt\n- **Prompt Engineering**: Structure prompts to leverage retrieved information\n- **Response Synthesis**: Generate answers grounded in retrieved context\n\n## Implementation Best Practices\n\n### Chunking Strategies\n- **Fixed-size chunks**: Simple but may split important information\n- **Semantic chunking**: Use sentence transformers to find natural boundaries\n- **Hierarchical chunking**: Store both fine and coarse-grained representations\n\n### Embedding Models\n- **General purpose**: OpenAI embeddings work well for most use cases\n- **Domain-specific**: Fine-tune embeddings on your specific domain\n- **Multilingual**: Consider models like `paraphrase-multilingual-MiniLM-L12-v2`\n\n### Vector Database Selection\n- **Pinecone**: Managed service, good for production\n- **Weaviate**: Open source, supports hybrid search\n- **FAISS**: Facebook's library, good for research and prototyping\n- **Chroma**: Lightweight, easy to get started\n\n## Performance Optimization\n\n### Query Processing\n- **Query expansion**: Generate multiple query variations\n- **Query routing**: Direct queries to specialized indexes\n- **Caching**: Cache frequent queries and their results\n\n### Retrieval Tuning\n- **Top-k selection**: Experiment with different numbers of retrieved chunks\n- **Score thresholds**: Filter out low-relevance results\n- **Diversity sampling**: Ensure retrieved chunks cover different aspects\n\n## Common Challenges\n\n### Information Retrieval Issues\n- **Semantic mismatch**: Query and documents use different terminology\n- **Context loss**: Important information split across chunks\n- **Hallucination**: Model generates information not in retrieved context\n\n### Solutions\n- **Query preprocessing**: Expand queries with synonyms and related terms\n- **Overlapping chunks**: Ensure important information appears in multiple chunks\n- **Grounding verification**: Check if generated content is supported by retrieved chunks\n\n## Evaluation Metrics\n\n### Retrieval Quality\n- **Precision@K**: Fraction of retrieved items that are relevant\n- **Recall@K**: Fraction of relevant items that are retrieved\n- **MRR**: Mean reciprocal rank of first relevant item\n\n### Generation Quality\n- **Faithfulness**: Generated content is supported by retrieved context\n- **Relevance**: Generated content answers the user's question\n- **Completeness**: Generated content covers all aspects of the question\n\n## Production Considerations\n\n### Scalability\n- **Batch processing**: Process large document collections efficiently\n- **Incremental updates**: Add new documents without rebuilding entire index\n- **Load balancing**: Distribute queries across multiple instances\n\n### Monitoring\n- **Query latency**: Track response times for different query types\n- **Retrieval quality**: Monitor precision and recall metrics\n- **Generation quality**: Track user satisfaction and feedback\n\n### Security\n- **Access control**: Restrict access to sensitive documents\n- **Query sanitization**: Prevent injection attacks through user queries\n- **Audit logging**: Track all queries and responses for compliance",
    "metadata": {
      "tags": [
        "rag",
        "retrieval",
        "embeddings",
        "vector-db"
      ],
      "persona": "ai",
      "file_path": "ai_ml/rag_systems.md",
      "file_name": "rag_systems.md"
    }
  },
  {
    "id": "ai_ml_gen_ai_0",
    "text": "# 📄 OG Resume – Krishna Sathvik (AI/ML & GenAI Version)\n\n---\n\n## Project Intro (Real-World AI/ML/GenAI Example)\n\nIn my recent AI/ML project at Walgreens, I worked as an AI/ML Engineer to design and deploy a **GenAI-powered knowledge assistant** for pharmacy operations. The solution integrated a **Retrieval-Augmented Generation (RAG) pipeline** where customer FAQs and policy documents were ingested, chunked, and stored in a **vector database (Pinecone)**. We built embeddings using **OpenAI + Hugging Face models**, enabling pharmacists to query compliance or medication rules in natural language with citations. The ingestion pipelines were built in **Databricks + PySpark**, transforming structured and unstructured data (text, PDFs, call transcripts) into embedding-friendly formats.\n\nFor orchestration, we leveraged **Airflow** to automate nightly ingestion, retraining, and re-indexing jobs. The inference pipeline was deployed via **Azure Kubernetes Service (AKS)** with scalable APIs. We used **MLflow** for experiment tracking and **Azure DevOps CI/CD** to deploy models into production with rollback safety. Governance was enforced with **Unity Catalog + custom PII scrubbing** before embedding sensitive data. Performance tuning included prompt optimization, few-shot examples, and caching embeddings for high-frequency queries, which reduced inference latency by 40%.\n\nThis project gave me **end-to-end GenAI + ML Ops experience**: ingestion, embeddings, vector search, model deployment, monitoring, and governance. It also showcased the ability to bridge **data engineering foundations with AI/ML innovation**, aligning with enterprise needs for compliance, scalability, and cost efficiency.\n\n---\n\n### 📄 OG Resume Summary – AI/ML & GenAI Version\n\nAI/ML Engineer with **5+ years of experience** building end-to-end machine learning and generative AI solutions. Skilled in developing data pipelines, training and deploying ML models, and implementing RAG/LLM systems with embeddings and vector databases. Proven ability to deliver cost-efficient, production-ready AI platforms that scale across domains including healthcare, retail, and finance.\n\n## Experience\n\n**AI/ML Engineer | TCS (Walgreens, USA) | Feb 2022 – Present**\n\nDesigned and deployed RAG pipeline with Databricks + Pinecone + OpenAI, enabling pharmacists to query compliance docs in natural language with source-cited answers\n\nEngineered ingestion of structured/unstructured data (PDFs, transcripts) via PySpark pipelines, transforming data into embeddings and cutting manual search effort by 60%\n\nAutomated retraining and re-indexing workflows using Airflow, improving model freshness and reducing knowledge gaps across pharmacy operations\n\nDeployed scalable inference APIs on AKS with CI/CD in Azure DevOps, ensuring zero-downtime rollouts and 35% latency reduction with optimized caching + prompts\n\nImplemented governance with Unity Catalog and PII scrubbing, securing sensitive patient data while maintaining HIPAA compliance in embeddings and responses\n\n**ML Engineer | CVS Health (USA) | Jan 2021 – Jan 2022**\n\nBuilt demand forecasting models in PySpark + TensorFlow predicting sales and supply trends, improving procurement accuracy and saving \\$15M annually\n\nEngineered feature pipelines with dbt + Databricks for model-ready datasets, cutting preparation time by 40% and ensuring consistency across teams\n\nTracked experiments with MLflow and automated retraining pipelines, boosting model performance by 18% over baseline\n\nDeployed models to production via Azure ML, implementing monitoring for drift and automating retraining triggers\n\n**Data Science Intern | McKesson (USA) | May 2020 – Dec 2020**\n\nDeveloped ETL + ML scripts in Python reducing ingestion latency 50%, powering dashboards for executive decisions on financial integrity\n\nBuilt regression + time series models forecasting patient demand, preventing supply mismatches and reducing stockouts by 22%\n\nProduced compliance-focused ML insights aligning with audit requirements and informing leadership’s strategic reviews\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nBuilt backend APIs and optimized SQL queries for ERP modules, strengthening transactional accuracy and improving response times by 35%\n\nDesigned reporting modules surfacing anomalies in contracts and payments, reducing manual reconciliation workload\n\n---\n\n## Skills\n\n- **AI/ML & GenAI:** PyTorch, TensorFlow, Hugging Face, OpenAI API, LangChain, MLflow, RAG pipelines, Vector DBs (Pinecone, FAISS, Weaviate)\n- **Data Engineering & ELT:** PySpark, Databricks, Airflow, dbt\n- **Cloud Platforms:** Azure (ML, Data Factory, AKS, DevOps), AWS (SageMaker, Lambda, S3)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle,",
    "metadata": {
      "persona": "ai",
      "file_path": "ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md"
    }
  },
  {
    "id": "ai_ml_gen_ai_1",
    "text": "22%\n\nProduced compliance-focused ML insights aligning with audit requirements and informing leadership’s strategic reviews\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nBuilt backend APIs and optimized SQL queries for ERP modules, strengthening transactional accuracy and improving response times by 35%\n\nDesigned reporting modules surfacing anomalies in contracts and payments, reducing manual reconciliation workload\n\n---\n\n## Skills\n\n- **AI/ML & GenAI:** PyTorch, TensorFlow, Hugging Face, OpenAI API, LangChain, MLflow, RAG pipelines, Vector DBs (Pinecone, FAISS, Weaviate)\n- **Data Engineering & ELT:** PySpark, Databricks, Airflow, dbt\n- **Cloud Platforms:** Azure (ML, Data Factory, AKS, DevOps), AWS (SageMaker, Lambda, S3)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle, Vector DBs\n- **Analytics & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Governance:** Azure DevOps, GitHub Actions, Docker, Kubernetes, Unity Catalog, PII Masking, Model Monitoring\n\n---\n\n# 📑 Resume Optimization Framework – AI/ML/GenAI Version\n\n## 1. Structure & Bullet Rules\n\n**5-4-3-2 Rule**\n\n- Walgreens → 5 bullets | Present tense | Outcome + metrics | Core tech (RAG, GenAI, PySpark, Databricks)\n- CVS → 4 bullets | Past tense | ML pipelines + forecasting | TensorFlow, Databricks, dbt\n- McKesson → 3 bullets | Past tense | Forecasting + compliance outcomes | Python + ML models\n- Inditek → 2 bullets | Past tense | Developer foundation | APIs + SQL\n\n**Character Constraint Rule**\n\n- Each bullet = **220–240 characters**\n- Never <215, never >240\n\n---\n\n## 2. Domain Adaptation Layer\n\nAdapt **vocabulary + emphasis** per JD:\n\n- GenAI/LLMs → RAG, vector DBs, embeddings, LangChain, prompt optimization\n- ML Engineering → model training, feature pipelines, MLflow, deployment\n- Data + AI → hybrid roles bridging ingestion, ELT, and ML ops\n- Compliance/Healthcare → HIPAA, lineage, governance for AI outputs\n\n---\n\n## 3. Skills Optimization Layer\n\n**Always keep 6 categories:**\n\n1. AI/ML & GenAI\n2. Data Engineering & ELT\n3. Cloud Platforms\n4. Databases & Storage\n5. Analytics & BI\n6. DevOps & Governance\n\n**JD Alignment:**\n\n- Re-order tools inside categories (AWS ML before Azure ML if JD says AWS)\n- Highlight GenAI (LangChain, OpenAI, vector DBs) when relevant\n- Strictly ATS-optimized, no jargon\n\n---\n\n## 4. Verb Rotation Rule\n\nRotate verbs: **Engineer, Optimize, Automate, Deploy, Develop, Train, Fine-tune, Implement, Orchestrate, Scale, Monitor**\n\n---\n\n## 5. Metrics Bank Rule\n\nEvery bullet ties to measurable outcome:\n\n- % improvements (latency -40%, accuracy +18%)\n- Scale (10TB+, 200+ models tracked, 1M+ embeddings)\n- Cost savings (\\$15M+ savings, 30% infra cost cut)\n- Risk/Compliance outcomes (HIPAA alignment, audit-ready insights)\n- User impact (pharmacists saved 60% manual search time)\n\n---\n\n# 📋 JD → Resume Adaptation Checklist\n\n1. **Read JD closely** → Look for AI/ML vs GenAI emphasis (RAG, embeddings, ML pipelines).\n2. **Apply Domain Layer** → Mirror vocabulary (e.g., LLM + LangChain vs forecasting + TensorFlow).\n3. **Reorder Skills** → Keep 6 categories, move JD-priority tools up front.\n4. **Rewrite Bullets** → Use 5-4-3-2, adjust tech names, keep measurable outcomes.\n5. **Rotate Verbs** → Ensure diversity, no repetition.\n6. **Check Character Count** → 220–240 characters each.\n7. **Final Scan** → ATS keywords aligned, outcomes present, tense correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**GenAI/LLM-Heavy JD**\\\nAI/ML Engineer with 6+ years’ experience delivering RAG pipelines, vector database search, and LLM-powered assistants. Skilled in Databricks, PySpark, OpenAI, and Hugging Face with proven impact on compliance, scalability, and latency reduction.\n\n**ML Engineering JD**\\\nMachine Learning Engineer with 6+ years’ experience designing feature pipelines, training models",
    "metadata": {
      "persona": "ai",
      "file_path": "ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md"
    }
  },
  {
    "id": "ai_ml_gen_ai_2",
    "text": "LangChain vs forecasting + TensorFlow).\n3. **Reorder Skills** → Keep 6 categories, move JD-priority tools up front.\n4. **Rewrite Bullets** → Use 5-4-3-2, adjust tech names, keep measurable outcomes.\n5. **Rotate Verbs** → Ensure diversity, no repetition.\n6. **Check Character Count** → 220–240 characters each.\n7. **Final Scan** → ATS keywords aligned, outcomes present, tense correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**GenAI/LLM-Heavy JD**\\\nAI/ML Engineer with 6+ years’ experience delivering RAG pipelines, vector database search, and LLM-powered assistants. Skilled in Databricks, PySpark, OpenAI, and Hugging Face with proven impact on compliance, scalability, and latency reduction.\n\n**ML Engineering JD**\\\nMachine Learning Engineer with 6+ years’ experience designing feature pipelines, training models in TensorFlow/PyTorch, and deploying production systems via Azure ML & SageMaker. Strong background in Databricks, MLflow, and cost-optimized ML Ops.\n\n**Hybrid Data+AI JD**\\\nData & AI Engineer with 6+ years bridging ELT pipelines and ML/GenAI systems. Skilled in PySpark, dbt, Databricks, and OpenAI APIs with experience in feature engineering, governance, and deploying ML + GenAI solutions to production at scale.\n\n---",
    "metadata": {
      "persona": "ai",
      "file_path": "ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md"
    }
  },
  {
    "id": "databricks_pyspark_0",
    "text": "# Databricks and PySpark Optimization\n\n## Overview\nDatabricks provides a unified analytics platform built on Apache Spark, offering optimized performance for big data processing with Delta Lake for ACID transactions and data versioning.\n\n## PySpark Performance Optimization\n\n### 1. Data Partitioning Strategies\n```python\n# Partition by date for time-series data\ndf.write.partitionBy(\"date\").mode(\"overwrite\").parquet(\"path/to/data\")\n\n# Partition by business key for joins\ndf.write.partitionBy(\"customer_id\").mode(\"overwrite\").parquet(\"path/to/data\")\n\n# Coalesce to reduce number of files\ndf.coalesce(10).write.mode(\"overwrite\").parquet(\"path/to/data\")\n```\n\n### 2. Caching and Persistence\n```python\n# Cache frequently accessed DataFrames\ndf.cache()\n\n# Persist with different storage levels\ndf.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n# Unpersist when done\ndf.unpersist()\n```\n\n### 3. Broadcast Joins\n```python\n# Broadcast small tables for joins\nfrom pyspark.sql.functions import broadcast\n\nlarge_df.join(broadcast(small_df), \"key\", \"inner\")\n```\n\n## Delta Lake Best Practices\n\n### 1. Delta Table Management\n```python\n# Create Delta table with partitioning\ndf.write.format(\"delta\").partitionBy(\"date\").saveAsTable(\"sales_data\")\n\n# Optimize Delta table\nspark.sql(\"OPTIMIZE sales_data ZORDER BY (customer_id, product_id)\")\n\n# Vacuum old files\nspark.sql(\"VACUUM sales_data RETAIN 168 HOURS\")\n```\n\n### 2. Time Travel and Versioning\n```python\n# Read from specific version\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 5).table(\"sales_data\")\n\n# Read from specific timestamp\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2023-01-01\").table(\"sales_data\")\n\n# Show table history\nspark.sql(\"DESCRIBE HISTORY sales_data\").show()\n```\n\n### 3. Merge Operations\n```python\n# Upsert data using merge\nfrom delta.tables import DeltaTable\n\ndelta_table = DeltaTable.forName(spark, \"sales_data\")\n\ndelta_table.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.id = source.id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Cluster Configuration\n\n### 1. Cluster Sizing\n- **Driver Node**: 2-4 cores, 8-16GB RAM for small to medium workloads\n- **Worker Nodes**: 4-8 cores, 16-32GB RAM per node\n- **Storage**: Use instance stores for temporary data, EBS for persistent data\n- **Network**: Enable enhanced networking for better performance\n\n### 2. Spark Configuration\n```python\n# Optimize for large datasets\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n\n# Optimize for joins\nspark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n```\n\n### 3. Dynamic Allocation\n```python\n# Enable dynamic allocation\nspark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\nspark.conf.set(\"spark.dynamicAllocation.minExecutors\", \"2\")\nspark.conf.set(\"spark.dynamicAllocation.maxExecutors\", \"20\")\nspark.conf.set(\"spark.dynamicAllocation.initialExecutors\", \"5\")\n```\n\n## Data Processing Patterns\n\n### 1. Streaming Data Processing\n```python\n# Read from Kafka\ndf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").load()\n\n# Process streaming data\nprocessed_df = df.select(\n    col(\"value\").cast(\"string\").alias(\"json\"),\n    col(\"timestamp\")\n).select(\n    from_json(col(\"json\"), schema).alias(\"data\"),\n    col(\"timestamp\")\n).select(\"data.*\", \"timestamp\")\n\n# Write to Delta Lake\nprocessed_df.writeStream.format(\"delta\").option(\"checkpointLocation\", \"/path/to/checkpoint\").start()\n```\n\n### 2. Batch Processing with Window Functions\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, lag, lead\n\n# Define window specification\nwindow_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n\n# Apply window functions\ndf.withColumn(\"row_num\", row_number().over(window_spec)) \\\n  .withColumn(\"prev_order_date\", lag(\"order_date\").over(window_spec)) \\\n  .withColumn(\"order_rank\", rank().over",
    "metadata": {
      "tags": [
        "databricks",
        "pyspark",
        "delta-lake",
        "big-data"
      ],
      "persona": "de",
      "file_path": "data_eng/databricks_pyspark.md",
      "file_name": "databricks_pyspark.md"
    }
  },
  {
    "id": "databricks_pyspark_1",
    "text": ":9092\").load()\n\n# Process streaming data\nprocessed_df = df.select(\n    col(\"value\").cast(\"string\").alias(\"json\"),\n    col(\"timestamp\")\n).select(\n    from_json(col(\"json\"), schema).alias(\"data\"),\n    col(\"timestamp\")\n).select(\"data.*\", \"timestamp\")\n\n# Write to Delta Lake\nprocessed_df.writeStream.format(\"delta\").option(\"checkpointLocation\", \"/path/to/checkpoint\").start()\n```\n\n### 2. Batch Processing with Window Functions\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, lag, lead\n\n# Define window specification\nwindow_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n\n# Apply window functions\ndf.withColumn(\"row_num\", row_number().over(window_spec)) \\\n  .withColumn(\"prev_order_date\", lag(\"order_date\").over(window_spec)) \\\n  .withColumn(\"order_rank\", rank().over(window_spec))\n```\n\n### 3. Data Quality Checks\n```python\n# Define data quality rules\nquality_rules = [\n    col(\"customer_id\").isNotNull(),\n    col(\"order_date\").isNotNull(),\n    col(\"amount\") > 0,\n    col(\"amount\") < 10000\n]\n\n# Apply quality checks\nvalid_df = df.filter(reduce(lambda a, b: a & b, quality_rules))\ninvalid_df = df.filter(~reduce(lambda a, b: a & b, quality_rules))\n\n# Log quality issues\ninvalid_df.write.mode(\"append\").saveAsTable(\"data_quality_issues\")\n```\n\n## Monitoring and Observability\n\n### 1. Spark UI Monitoring\n- **Jobs Tab**: Monitor job execution and stages\n- **Stages Tab**: Analyze stage performance and bottlenecks\n- **Storage Tab**: Check DataFrame caching and persistence\n- **Environment Tab**: Review Spark configuration\n\n### 2. Custom Metrics\n```python\n# Track custom metrics\nfrom pyspark.sql.functions import count, sum\n\n# Record processing metrics\nmetrics = df.agg(\n    count(\"*\").alias(\"total_records\"),\n    sum(\"amount\").alias(\"total_amount\")\n).collect()[0]\n\n# Log metrics to external system\nlog_metrics({\n    \"total_records\": metrics[\"total_records\"],\n    \"total_amount\": metrics[\"total_amount\"],\n    \"timestamp\": datetime.now()\n})\n```\n\n### 3. Error Handling\n```python\n# Implement robust error handling\ntry:\n    result_df = process_data(input_df)\n    result_df.write.mode(\"overwrite\").saveAsTable(\"processed_data\")\nexcept Exception as e:\n    # Log error details\n    error_details = {\n        \"error\": str(e),\n        \"timestamp\": datetime.now(),\n        \"input_record_count\": input_df.count()\n    }\n    \n    # Write error to error table\n    error_df = spark.createDataFrame([error_details])\n    error_df.write.mode(\"append\").saveAsTable(\"processing_errors\")\n    \n    # Re-raise exception\n    raise e\n```\n\n## Data Lakehouse Architecture\n\n### 1. Bronze Layer (Raw Data)\n```python\n# Ingest raw data\nraw_df = spark.read.format(\"json\").load(\"path/to/raw/data\")\n\n# Store in Bronze layer\nraw_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronze.raw_events\")\n```\n\n### 2. Silver Layer (Cleaned Data)\n```python\n# Clean and validate data\ncleaned_df = raw_df.filter(\n    col(\"event_id\").isNotNull() &\n    col(\"timestamp\").isNotNull() &\n    col(\"user_id\").isNotNull()\n).withColumn(\"processed_date\", current_timestamp())\n\n# Store in Silver layer\ncleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"silver.cleaned_events\")\n```\n\n### 3. Gold Layer (Business Logic)\n```python\n# Apply business logic and aggregations\ngold_df = cleaned_df.groupBy(\"user_id\", \"date\").agg(\n    count(\"*\").alias(\"event_count\"),\n    sum(\"value\").alias(\"total_value\"),\n    avg(\"value\").alias(\"avg_value\")\n)\n\n# Store in Gold layer\ngold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.user_daily_metrics\")\n```\n\n## Cost Optimization\n\n### 1. Cluster Management\n- **Auto-scaling**: Use Databricks auto-scaling for variable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Cluster Pools**: Pre-allocate clusters for faster startup\n- **Termination Policies**: Auto-terminate idle clusters\n\n### 2. Storage Optimization\n- **Compression**: Use Parquet with Snappy compression\n- **Partitioning**: Partition large tables by date or key columns\n- **File Sizing**: Optimize file sizes (128MB-1GB per",
    "metadata": {
      "tags": [
        "databricks",
        "pyspark",
        "delta-lake",
        "big-data"
      ],
      "persona": "de",
      "file_path": "data_eng/databricks_pyspark.md",
      "file_name": "databricks_pyspark.md"
    }
  },
  {
    "id": "databricks_pyspark_2",
    "text": "business logic and aggregations\ngold_df = cleaned_df.groupBy(\"user_id\", \"date\").agg(\n    count(\"*\").alias(\"event_count\"),\n    sum(\"value\").alias(\"total_value\"),\n    avg(\"value\").alias(\"avg_value\")\n)\n\n# Store in Gold layer\ngold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.user_daily_metrics\")\n```\n\n## Cost Optimization\n\n### 1. Cluster Management\n- **Auto-scaling**: Use Databricks auto-scaling for variable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Cluster Pools**: Pre-allocate clusters for faster startup\n- **Termination Policies**: Auto-terminate idle clusters\n\n### 2. Storage Optimization\n- **Compression**: Use Parquet with Snappy compression\n- **Partitioning**: Partition large tables by date or key columns\n- **File Sizing**: Optimize file sizes (128MB-1GB per file)\n- **Delta Optimization**: Regular OPTIMIZE and VACUUM operations\n\n### 3. Query Optimization\n- **Predicate Pushdown**: Use filters early in the query\n- **Column Pruning**: Select only needed columns\n- **Join Optimization**: Use broadcast joins for small tables\n- **Caching**: Cache frequently accessed DataFrames\n\n## Security and Governance\n\n### 1. Access Control\n- **Unity Catalog**: Centralized data governance\n- **Table ACLs**: Fine-grained access control\n- **Column-level Security**: Mask sensitive columns\n- **Row-level Security**: Filter data based on user context\n\n### 2. Data Lineage\n- **Lineage Tracking**: Track data transformations\n- **Metadata Management**: Document table schemas and business logic\n- **Impact Analysis**: Understand downstream dependencies\n- **Compliance**: Meet regulatory requirements\n\n### 3. Audit and Monitoring\n- **Query Logging**: Log all data access\n- **Performance Monitoring**: Track query performance\n- **Resource Usage**: Monitor compute and storage usage\n- **Cost Tracking**: Track costs by user and project",
    "metadata": {
      "tags": [
        "databricks",
        "pyspark",
        "delta-lake",
        "big-data"
      ],
      "persona": "de",
      "file_path": "data_eng/databricks_pyspark.md",
      "file_name": "databricks_pyspark.md"
    }
  },
  {
    "id": "krishna_intro_0",
    "text": "# Krishna - Data Engineer Profile & Experience\n\n## Professional Overview\n\nKrishna is a Data Engineer with extensive experience at Walgreens, specializing in Azure and Databricks technologies. He has built and optimized large-scale data pipelines processing over 10TB of data monthly across multiple business domains.\n\n## Current Role & Responsibilities\n\n### **Primary Focus Areas**\n- **Large-Scale Data Processing**: Handling 10TB+ monthly data volume\n- **Modern Data Architecture**: Implementing medallion architecture (Bronze, Silver, Gold)\n- **Cross-Domain Expertise**: Sales, customer loyalty, pharmacy, and supply chain data\n- **Legacy Migration**: Transitioning from traditional systems to cloud-native solutions\n\n### **Technical Leadership**\n- **Subject Matter Expert**: Databricks and PySpark specialist\n- **Team Mentorship**: Guiding offshore developers on best practices\n- **Code Review**: Ensuring quality and performance standards\n- **Best Practices**: Driving data engineering standards across teams\n\n## Technical Expertise\n\n### **Core Technologies**\n- **Azure Data Factory (ADF)**: Orchestration and workflow management\n- **Databricks**: Notebook development and cluster management\n- **PySpark**: Large-scale data transformations and processing\n- **Delta Lake**: ACID transactions, schema enforcement, time travel\n- **Unity Catalog**: Data governance and access control\n- **Azure DevOps**: CI/CD pipeline implementation\n\n### **Data Architecture Patterns**\n- **Medallion Architecture**: Bronze (raw), Silver (standardized), Gold (modeled)\n- **Delta Lake Features**: Schema evolution, ACID compliance, time travel\n- **Partitioning Strategies**: Date-based, state-based, product-based\n- **Data Quality Frameworks**: Validation, monitoring, and alerting\n\n## Project Experience\n\n### **Walgreens Data Platform Migration**\n**Scope**: End-to-end data engineering for enterprise migration\n**Scale**: 10TB+ monthly processing across multiple business domains\n**Architecture**: Modern medallion architecture implementation\n\n#### **Bronze Layer (Raw Data Ingestion)**\n- Ingested raw data from multiple sources\n- Implemented schema-on-read for flexibility\n- Handled various data formats and structures\n- Set up automated ingestion pipelines\n\n#### **Silver Layer (Standardized Data)**\n- Applied schema enforcement and validation\n- Implemented deduplication logic\n- Data cleansing and standardization\n- Quality checks and error handling\n\n#### **Gold Layer (Business Models)**\n- Built fact and dimension tables\n- Created KPI and reporting models\n- Optimized for business consumption\n- Integrated with Power BI dashboards\n\n## Performance Optimization Achievements\n\n### **Significant Performance Improvements**\n- **Runtime Reduction**: Cut job execution time from 2+ hours to under 40 minutes\n- **Cost Optimization**: 30% reduction in processing costs\n- **SLA Compliance**: Improved pipeline reliability and performance\n\n### **Optimization Techniques**\n- **Partitioning**: Strategic partitioning by date, state, and product\n- **Skew Resolution**: Implemented salting for skewed data joins\n- **Join Optimization**: Broadcast joins for small lookup tables\n- **File Compaction**: Optimized Delta file sizes for better performance\n- **Resource Tuning**: Executor memory and CPU optimization\n\n## Data Governance & Security\n\n### **Unity Catalog Implementation**\n- **Role-Based Access Control**: Granular permissions management\n- **PII Protection**: Data masking and tokenization\n- **Lineage Tracking**: Complete data lineage for audits\n- **Compliance**: Meeting regulatory requirements\n\n### **Security Measures**\n- **Data Masking**: Sensitive field protection (emails, customer IDs)\n- **Tokenization**: Secure handling of PII data\n- **Access Controls**: Business teams access only anonymized data\n- **Audit Trails**: Complete tracking of data access and changes\n\n## CI/CD & DevOps\n\n### **Automated Deployment Pipeline**\n- **Version Control**: Git-based notebook and pipeline management\n- **Environment Management**: Automated deployments across dev, test, prod\n- **Error Reduction**: Eliminated manual deployment errors\n- **Repeatability**: Consistent deployment processes\n\n### **Quality Assurance**\n- **Automated Testing**: CI/CD integration with validation checks\n- **Code Review**: Systematic review processes\n- **Deployment Validation**: Automated testing in each environment\n\n## Collaboration & Mentoring\n\n### **Team Leadership**\n- **Offshore Team Mentorship**: Guided international developers\n- **Technical Training**: PySpark and ADF best practices\n- **Code Reviews**: Ensuring quality and performance standards\n- **Knowledge Sharing**: Documented processes and procedures\n\n### **Stakeholder Engagement**\n- **Business Translation**: Converting KPI requirements to data models\n- **Cross-Team Collaboration**: Working with business and IT teams\n- **Requirements Gathering**: Understanding business needs and priorities\n\n## Monitoring & Data Quality\n\n### **",
    "metadata": {
      "tags": [
        "krishna",
        "walgreens",
        "data-engineer",
        "azure",
        "databricks",
        "pyspark",
        "medallion-architecture",
        "delta-lake",
        "azure-data-factory",
        "unity-catalog",
        "performance-optimization"
      ],
      "persona": "de",
      "file_path": "data_eng/krishna_intro.md",
      "file_name": "krishna_intro.md"
    }
  },
  {
    "id": "krishna_intro_1",
    "text": "Control**: Git-based notebook and pipeline management\n- **Environment Management**: Automated deployments across dev, test, prod\n- **Error Reduction**: Eliminated manual deployment errors\n- **Repeatability**: Consistent deployment processes\n\n### **Quality Assurance**\n- **Automated Testing**: CI/CD integration with validation checks\n- **Code Review**: Systematic review processes\n- **Deployment Validation**: Automated testing in each environment\n\n## Collaboration & Mentoring\n\n### **Team Leadership**\n- **Offshore Team Mentorship**: Guided international developers\n- **Technical Training**: PySpark and ADF best practices\n- **Code Reviews**: Ensuring quality and performance standards\n- **Knowledge Sharing**: Documented processes and procedures\n\n### **Stakeholder Engagement**\n- **Business Translation**: Converting KPI requirements to data models\n- **Cross-Team Collaboration**: Working with business and IT teams\n- **Requirements Gathering**: Understanding business needs and priorities\n\n## Monitoring & Data Quality\n\n### **Data Quality Framework**\n- **Validation Checks**: Automated data quality monitoring\n- **Error Logging**: Comprehensive error tracking and reporting\n- **Monitoring Tables**: Centralized quality metrics storage\n- **Real-Time Dashboards**: Power BI integration for business visibility\n\n### **Operational Excellence**\n- **Pipeline Health**: Real-time monitoring of data pipeline status\n- **Business Visibility**: Dashboards for business stakeholders\n- **Proactive Alerting**: Early detection of data quality issues\n- **Performance Tracking**: Continuous optimization monitoring\n\n## Key Achievements & Impact\n\n### **Technical Achievements**\n- **Scale**: Successfully handling 10TB+ monthly data processing\n- **Performance**: 75% reduction in job runtime (2+ hours to 40 minutes)\n- **Cost**: 30% reduction in processing costs through optimization\n- **Reliability**: Improved SLA compliance and system stability\n\n### **Business Impact**\n- **Modernization**: Successfully migrated legacy systems\n- **Governance**: Implemented enterprise-grade data governance\n- **Collaboration**: Enhanced cross-team productivity\n- **Quality**: Improved data quality and business confidence\n\n## Skills Summary\n\n### **Technical Skills**\n- **Big Data**: PySpark, Databricks, Delta Lake\n- **Cloud**: Azure Data Factory, Azure DevOps\n- **Governance**: Unity Catalog, data lineage, security\n- **Performance**: Optimization, tuning, monitoring\n- **DevOps**: CI/CD, version control, automation\n\n### **Soft Skills**\n- **Leadership**: Team mentoring and technical guidance\n- **Communication**: Cross-functional collaboration\n- **Problem Solving**: Complex technical challenges\n- **Project Management**: End-to-end delivery responsibility\n\n## Professional Philosophy\n\nKrishna believes in building reliable, scalable, and maintainable data solutions that serve both technical and business needs. His approach combines hands-on technical expertise with strong collaboration skills to drive successful data platform implementations.\n\n---\n\n*This profile represents Krishna's comprehensive experience in modern data engineering, combining technical depth with business impact and team leadership.*",
    "metadata": {
      "tags": [
        "krishna",
        "walgreens",
        "data-engineer",
        "azure",
        "databricks",
        "pyspark",
        "medallion-architecture",
        "delta-lake",
        "azure-data-factory",
        "unity-catalog",
        "performance-optimization"
      ],
      "persona": "de",
      "file_path": "data_eng/krishna_intro.md",
      "file_name": "krishna_intro.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_0",
    "text": "# Comprehensive Data Engineering Interview Preparation\n\n## Latest Interview Questions & Topics\n\nOver the past few months, I've been actively preparing for Data Engineering interviews, and I thought of sharing some of the latest questions I came across. These focus on Spark, SQL, Python, and Databricks, the core skills for most modern data engineering roles.\n\n### 🔹 Spark Questions\n\n**Explain Spark architecture – how do driver, executors, and cluster manager interact?**\n- Driver program coordinates the job execution and communicates with cluster manager\n- Cluster manager (YARN, Mesos, or standalone) allocates resources across the cluster\n- Executors run tasks and store data in memory/disk, report back to driver\n- Driver sends tasks to executors, collects results, and manages the overall job\n\n**Difference between narrow vs. wide transformations with examples.**\n- Narrow transformations: map(), filter(), union() - no data movement between partitions\n- Wide transformations: groupBy(), join(), distinct() - requires shuffling data across partitions\n- Wide transformations are expensive and should be minimized for performance\n\n**How do you optimize Spark jobs (partitioning, caching, broadcast joins)?**\n- Partition data appropriately based on query patterns (usually date-based)\n- Cache frequently accessed DataFrames in memory\n- Use broadcast joins for small lookup tables\n- Tune spark.sql.shuffle.partitions based on cluster size\n- Use coalesce() instead of repartition() when reducing partitions\n\n**What happens when you run df.explain() in Spark?**\n- Shows the logical and physical execution plans\n- Displays optimization decisions made by Catalyst optimizer\n- Helps identify bottlenecks, unnecessary shuffles, or missing optimizations\n- Can use df.explain(true) for extended physical plan details\n\n### 🔹 SQL Questions\n\n**Write a query to get the second highest salary without using TOP or LIMIT.**\n```sql\n-- Using window function\nSELECT salary \nFROM (\n    SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) as rn\n    FROM employees\n) ranked\nWHERE rn = 2;\n\n-- Using subquery\nSELECT MAX(salary) \nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n\n-- Using self-join\nSELECT DISTINCT e1.salary\nFROM employees e1, employees e2\nWHERE e1.salary < e2.salary\nGROUP BY e1.salary\nHAVING COUNT(*) = 1;\n```\n\n**Explain QUALIFY with an example.**\n```sql\n-- QUALIFY filters results of window functions without subquery\nSELECT name, salary, department,\n       ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rn\nFROM employees\nQUALIFY rn <= 3;  -- Top 3 earners per department\n```\n\n**Difference between INNER JOIN, LEFT JOIN, FULL JOIN, and CROSS JOIN.**\n- INNER JOIN: Only matching rows from both tables\n- LEFT JOIN: All rows from left table + matching rows from right table\n- FULL JOIN: All rows from both tables (union of LEFT and RIGHT)\n- CROSS JOIN: Cartesian product of all rows from both tables\n\n**How do you handle incremental loads in SQL?**\n- Use watermark columns (modified_date, created_date)\n- Implement CDC (Change Data Capture) patterns\n- Use MERGE statements for upsert operations\n- Track last processed timestamp in control tables\n\n### 🔹 Python Questions\n\n**How would you reverse a string without using built-in functions?**\n```python\ndef reverse_string(s):\n    result = \"\"\n    for i in range(len(s) - 1, -1, -1):\n        result += s[i]\n    return result\n\n# Alternative approach\ndef reverse_string_slicing(s):\n    return s[::-1]\n```\n\n**Write code to find duplicates in a list.**\n```python\ndef find_duplicates(lst):\n    seen = set()\n    duplicates = set()\n    for item in lst:\n        if item in seen:\n            duplicates.add(item)\n        else:\n            seen.add(item)\n    return list(duplicates)\n\n# Using collections.Counter\nfrom collections import Counter\ndef find_duplicates_counter(lst):\n    counts = Counter(lst)\n    return [item for item, count in counts.items() if count > 1]\n```\n\n**Difference between @staticmethod, @classmethod, and instance methods.**\n- Instance methods: Take self, access instance attributes\n- Class methods: Take cls, access class attributes, can create new instances\n- Static methods: Don't take self or cls, utility functions that don't need class/instance data\n\n**Explain Python's GIL (Global Interpreter Lock).**\n- GIL prevents multiple threads from executing Python bytecode simultaneously\n- Only one thread can execute Python code at a time\n- Affects CPU-bound tasks but not I/O-bound tasks\n- Can be bypassed using multiprocessing or C extensions\n\n### 🔹 Databricks",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_1",
    "text": "in lst:\n        if item in seen:\n            duplicates.add(item)\n        else:\n            seen.add(item)\n    return list(duplicates)\n\n# Using collections.Counter\nfrom collections import Counter\ndef find_duplicates_counter(lst):\n    counts = Counter(lst)\n    return [item for item, count in counts.items() if count > 1]\n```\n\n**Difference between @staticmethod, @classmethod, and instance methods.**\n- Instance methods: Take self, access instance attributes\n- Class methods: Take cls, access class attributes, can create new instances\n- Static methods: Don't take self or cls, utility functions that don't need class/instance data\n\n**Explain Python's GIL (Global Interpreter Lock).**\n- GIL prevents multiple threads from executing Python bytecode simultaneously\n- Only one thread can execute Python code at a time\n- Affects CPU-bound tasks but not I/O-bound tasks\n- Can be bypassed using multiprocessing or C extensions\n\n### 🔹 Databricks Questions\n\n**How do you implement Bronze-Silver-Gold architecture in Databricks?**\n- Bronze: Raw ingested data with schema flexibility\n- Silver: Cleaned, validated, deduplicated data with enforced schemas\n- Gold: Business-ready aggregated and modeled data for analytics\n- Use Delta Lake for ACID transactions and schema evolution\n\n**Difference between Delta Lake and traditional parquet tables.**\n- Delta Lake: ACID transactions, schema enforcement, time travel, upserts\n- Parquet: Immutable, append-only, no built-in transaction support\n- Delta Lake provides better reliability and governance capabilities\n\n**How do you manage incremental data load using Delta Lake?**\n```python\n# Using MERGE for upserts\ndf.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/path/to/table\")\n\n# Or using merge operation\nfrom delta.tables import DeltaTable\ndeltaTable = DeltaTable.forPath(spark, \"/path/to/table\")\ndeltaTable.alias(\"target\").merge(\n    source_df.alias(\"source\"),\n    \"target.id = source.id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n**Explain Unity Catalog and its role in governance.**\n- Centralized metadata store for data governance\n- Provides fine-grained access control at table/column level\n- Enables data lineage tracking and discovery\n- Supports PII masking and row-level security\n- Integrates with external identity providers\n\n## 🔍 Why Most People Struggle in Data Engineering Interviews\n\nI've seen this pattern again and again — candidates know the tools, but the moment the interviewer shifts from \"what is Spark?\" to \"design a pipeline for streaming data,\" things start to fall apart.\n\n**The truth?** Data Engineering interviews test how you think, not just what you know.\n\n### Mental Framework for Interviews:\n\n1️⃣ **SQL is your foundation** → If you can't join, aggregate, and optimize queries, nothing else will stand.\n\n2️⃣ **Model the data** → Understand when to use star schema vs. snowflake, OLTP vs. OLAP, and how to handle Slowly Changing Dimensions.\n\n3️⃣ **Think scale** → Spark, partitioning, shuffles, streaming late-arrival data — these aren't buzzwords, they're the backbone of real-world DE systems.\n\n4️⃣ **Design pipelines, not scripts** → Batch vs. streaming, orchestration with Airflow, partitioning strategies — show that you can think like an architect.\n\n5️⃣ **Zoom out** → Cloud (S3, Redshift, BigQuery, Synapse), data lake vs. warehouse vs. lakehouse — interviews often end with \"how would you design this end-to-end?\"\n\n## Databricks-Specific Interview Preparation\n\nDatabricks is the home of Apache Spark and the Lakehouse Platform. Engineers here focus on real-time data, ML pipelines, and large-scale analytics.\n\n### 10 Key Databricks Questions:\n\n1️⃣ **Explain how Delta Lake improves reliability over a traditional data lake.**\n- ACID transactions ensure data consistency\n- Schema enforcement prevents data corruption\n- Time travel enables data recovery and auditing\n- Upsert capabilities support CDC patterns\n\n2️⃣ **How would you design a streaming pipeline with Structured Streaming in Spark?**\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder.appName(\"StreamingPipeline\").getOrCreate()\n\n# Read from Kafka\nstreaming_df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"events\") \\\n    .load()\n\n# Process and write to Delta Lake\nprocessed_df = streaming_df.select(\n    from_json(col(\"value\").cast(\"string\"),",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_2",
    "text": "real-time data, ML pipelines, and large-scale analytics.\n\n### 10 Key Databricks Questions:\n\n1️⃣ **Explain how Delta Lake improves reliability over a traditional data lake.**\n- ACID transactions ensure data consistency\n- Schema enforcement prevents data corruption\n- Time travel enables data recovery and auditing\n- Upsert capabilities support CDC patterns\n\n2️⃣ **How would you design a streaming pipeline with Structured Streaming in Spark?**\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder.appName(\"StreamingPipeline\").getOrCreate()\n\n# Read from Kafka\nstreaming_df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"events\") \\\n    .load()\n\n# Process and write to Delta Lake\nprocessed_df = streaming_df.select(\n    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\nquery = processed_df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\n    .outputMode(\"append\") \\\n    .start(\"/delta/events\")\n```\n\n3️⃣ **Walk me through optimizing a Spark job handling terabytes of data.**\n- Profile with Spark UI to identify bottlenecks\n- Optimize partitioning strategy (usually date-based)\n- Use broadcast joins for small lookup tables\n- Implement salting for skewed joins\n- Enable Adaptive Query Execution (AQE)\n- Cache frequently accessed DataFrames\n- Use columnar formats like Parquet/Delta\n\n4️⃣ **How would you enable ACID transactions in a big data pipeline?**\n- Use Delta Lake for ACID compliance\n- Implement proper partitioning strategies\n- Use MERGE operations for upserts\n- Configure appropriate isolation levels\n- Handle concurrent writes with optimistic concurrency control\n\n5️⃣ **Explain how you'd manage schema evolution with Delta Lake.**\n```python\n# Enable schema evolution\ndf.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .save(\"/delta/table\")\n\n# Or manually evolve schema\nspark.sql(\"ALTER TABLE delta_table ADD COLUMN new_column STRING\")\n```\n\n6️⃣ **What's your strategy for scaling ML model training pipelines in Databricks?**\n- Use MLflow for experiment tracking and model versioning\n- Implement distributed training with Spark MLlib\n- Use GPU clusters for deep learning workloads\n- Cache feature engineering results\n- Implement model serving with MLflow Model Registry\n\n7️⃣ **How would you integrate Databricks with external BI tools like Tableau or Power BI?**\n- Create SQL endpoints for direct query access\n- Use JDBC/ODBC connectors for BI tools\n- Implement data marts with optimized schemas\n- Use Delta Lake for consistent data access\n- Configure appropriate access controls\n\n8️⃣ **Describe how you'd secure a data lakehouse in a multi-tenant environment.**\n- Implement Unity Catalog for centralized governance\n- Use workspace-level isolation\n- Configure fine-grained access controls\n- Implement PII masking and encryption\n- Use Azure Active Directory integration\n\n9️⃣ **How do you debug performance bottlenecks in Spark?**\n- Use Spark UI to analyze DAG and stage execution\n- Check for data skew in shuffle operations\n- Monitor memory usage and garbage collection\n- Profile with Spark History Server\n- Use query plans to identify optimization opportunities\n\n🔟 **Tell me about a time you balanced speed vs. cost in large-scale data jobs.**\n- Implemented incremental processing instead of full loads\n- Used spot instances for non-critical workloads\n- Optimized partitioning to reduce shuffle overhead\n- Implemented auto-scaling based on workload patterns\n- Used columnar storage formats for better compression\n\n## Advanced Data Engineering Interview Questions\n\n### 20 Comprehensive Questions:\n\n1️⃣ **Write an SQL query to find the second highest salary from an employee table.**\n```sql\nSELECT MAX(salary) as second_highest_salary\nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n```\n\n2️⃣ **How do you handle NULL values in SQL joins while ensuring data integrity?**\n- Use COALESCE() or ISNULL() to provide default values\n- Implement proper NULL handling in business logic\n- Use INNER JOIN to exclude NULL matches if appropriate\n- Consider LEFT JOIN with NULL checks for data validation\n\n3️⃣ **Write an SQL query to calculate customer churn rate over the last 6 months.**\n```sql\nWITH customer_activity AS (\n    SELECT customer_id,\n           MAX(order_date) as last_order_date,\n           CASE WHEN MAX(order_date)",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_3",
    "text": "on workload patterns\n- Used columnar storage formats for better compression\n\n## Advanced Data Engineering Interview Questions\n\n### 20 Comprehensive Questions:\n\n1️⃣ **Write an SQL query to find the second highest salary from an employee table.**\n```sql\nSELECT MAX(salary) as second_highest_salary\nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n```\n\n2️⃣ **How do you handle NULL values in SQL joins while ensuring data integrity?**\n- Use COALESCE() or ISNULL() to provide default values\n- Implement proper NULL handling in business logic\n- Use INNER JOIN to exclude NULL matches if appropriate\n- Consider LEFT JOIN with NULL checks for data validation\n\n3️⃣ **Write an SQL query to calculate customer churn rate over the last 6 months.**\n```sql\nWITH customer_activity AS (\n    SELECT customer_id,\n           MAX(order_date) as last_order_date,\n           CASE WHEN MAX(order_date) < DATE_SUB(CURRENT_DATE, INTERVAL 6 MONTH) \n                THEN 1 ELSE 0 END as is_churned\n    FROM orders\n    GROUP BY customer_id\n)\nSELECT \n    COUNT(*) as total_customers,\n    SUM(is_churned) as churned_customers,\n    ROUND(SUM(is_churned) * 100.0 / COUNT(*), 2) as churn_rate_percentage\nFROM customer_activity;\n```\n\n4️⃣ **Design a fact table for an e-commerce platform – what dimensions and measures would you include?**\n- **Dimensions**: Date, Customer, Product, Store, Payment Method\n- **Measures**: Sales Amount, Quantity, Discount, Tax, Shipping Cost\n- **Grain**: One row per transaction line item\n- **Design**: Star schema with fact table in center\n\n5️⃣ **Explain the difference between star schema and snowflake schema and when to choose each.**\n- **Star Schema**: Single fact table with denormalized dimensions\n- **Snowflake Schema**: Normalized dimensions with multiple levels\n- **Choose Star**: When query performance is priority, simpler to understand\n- **Choose Snowflake**: When storage efficiency matters, complex hierarchies\n\n6️⃣ **Write a Python script to validate data quality and detect anomalies before loading.**\n```python\ndef validate_data_quality(df):\n    validation_results = {}\n    \n    # Check for nulls in critical columns\n    critical_columns = ['customer_id', 'order_date', 'amount']\n    for col in critical_columns:\n        null_count = df.filter(col(col).isNull()).count()\n        validation_results[f'{col}_nulls'] = null_count\n    \n    # Check for duplicates\n    duplicate_count = df.count() - df.dropDuplicates().count()\n    validation_results['duplicates'] = duplicate_count\n    \n    # Check for outliers (amount > 3 standard deviations)\n    amount_stats = df.select(mean('amount'), stddev('amount')).collect()[0]\n    outlier_count = df.filter(col('amount') > (amount_stats[0] + 3 * amount_stats[1])).count()\n    validation_results['outliers'] = outlier_count\n    \n    return validation_results\n```\n\n7️⃣ **In PySpark, how would you efficiently join two very large DataFrames to avoid skew?**\n```python\n# Method 1: Broadcast small DataFrame\nfrom pyspark.sql.functions import broadcast\nresult = large_df.join(broadcast(small_df), \"key\")\n\n# Method 2: Salting for skewed keys\ndef salt_key(df, salt_buckets=10):\n    return df.withColumn(\"salt\", (rand() * salt_buckets).cast(\"int\")) \\\n             .withColumn(\"salted_key\", concat(col(\"key\"), lit(\"_\"), col(\"salt\")))\n\n# Method 3: Repartition before join\ndf1_repartitioned = df1.repartition(200, \"join_key\")\ndf2_repartitioned = df2.repartition(200, \"join_key\")\nresult = df1_repartitioned.join(df2_repartitioned, \"join_key\")\n```\n\n8️⃣ **Write PySpark code to find the top 3 customers by revenue per region.**\n```python\nfrom pyspark.sql.functions import sum, rank, col\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"region\").orderBy(col(\"total_revenue\").desc())\n\ntop_customers = df.groupBy(\"customer_id\", \"region\") \\\n    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n    .withColumn(\"rank\", rank().over(window_spec)) \\\n    .filter(col(\"rank\") <= 3) \\\n    .orderBy(\"region\", \"rank\")\n```\n\n9️⃣ **You are processing real-time data from Event Hub into Delta tables. How would you implement",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_4",
    "text": ".repartition(200, \"join_key\")\ndf2_repartitioned = df2.repartition(200, \"join_key\")\nresult = df1_repartitioned.join(df2_repartitioned, \"join_key\")\n```\n\n8️⃣ **Write PySpark code to find the top 3 customers by revenue per region.**\n```python\nfrom pyspark.sql.functions import sum, rank, col\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"region\").orderBy(col(\"total_revenue\").desc())\n\ntop_customers = df.groupBy(\"customer_id\", \"region\") \\\n    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n    .withColumn(\"rank\", rank().over(window_spec)) \\\n    .filter(col(\"rank\") <= 3) \\\n    .orderBy(\"region\", \"rank\")\n```\n\n9️⃣ **You are processing real-time data from Event Hub into Delta tables. How would you implement this?**\n```python\n# Structured Streaming from Event Hub\nstreaming_df = spark.readStream \\\n    .format(\"eventhubs\") \\\n    .options(**event_hub_config) \\\n    .load()\n\n# Process and write to Delta Lake\nprocessed_stream = streaming_df.select(\n    from_json(col(\"body\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\nquery = processed_stream.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\n    .outputMode(\"append\") \\\n    .trigger(processingTime='10 seconds') \\\n    .start(\"/delta/real_time_data\")\n```\n\n🔟 **How do you implement schema evolution in Delta Lake without breaking existing jobs?**\n- Use `mergeSchema=true` option when writing\n- Add new columns with default values\n- Use `ALTER TABLE` for explicit schema changes\n- Test schema changes in development first\n- Use version control for schema definitions\n\n1️⃣1️⃣ **How would you implement Slowly Changing Dimensions (SCD Type 2) in a data warehouse?**\n```python\n# SCD Type 2 implementation with Delta Lake\ndef implement_scd_type2(current_df, historical_df, business_key):\n    # Add versioning columns\n    current_with_version = current_df.withColumn(\"effective_date\", current_date()) \\\n                                   .withColumn(\"end_date\", lit(None)) \\\n                                   .withColumn(\"is_current\", lit(True))\n    \n    # Merge logic\n    delta_table = DeltaTable.forPath(spark, historical_table_path)\n    delta_table.alias(\"target\").merge(\n        current_with_version.alias(\"source\"),\n        f\"target.{business_key} = source.{business_key}\"\n    ).whenMatchedUpdate(\n        condition=\"target.is_current = true AND target.hash_key != source.hash_key\",\n        set={\n            \"end_date\": current_date(),\n            \"is_current\": \"false\"\n        }\n    ).whenNotMatchedInsertAll().execute()\n```\n\n1️⃣2️⃣ **Late-arriving data is detected in a batch pipeline – how do you ensure correctness of historical reporting?**\n- Implement watermarking with configurable retention periods\n- Use Delta Lake's time travel for reprocessing\n- Design fact tables to handle late-arriving facts\n- Implement data quality monitoring for late arrivals\n- Use surrogate keys for dimension lookups\n\n1️⃣3️⃣ **How do you design a pipeline that supports both batch and streaming workloads simultaneously?**\n- Use Lambda architecture with batch and speed layers\n- Implement unified data models in Delta Lake\n- Use Structured Streaming for real-time processing\n- Batch layer handles historical data and corrections\n- Merge both layers for complete analytics\n\n1️⃣4️⃣ **What is your approach to building incremental data loads in Azure Data Factory pipelines?**\n- Use watermark columns to track last processed data\n- Implement lookup activities to retrieve watermarks\n- Use conditional activities for incremental vs. full loads\n- Store watermarks in Azure SQL Database or Key Vault\n- Implement error handling and retry policies\n\n1️⃣5️⃣ **Your PySpark job is failing due to skewed joins. Walk through your debugging and optimization steps.**\n1. Identify skew using Spark UI (look for tasks taking much longer)\n2. Analyze data distribution on join keys\n3. Apply salting technique for skewed keys\n4. Use broadcast joins for small tables\n5. Repartition data before joins\n6. Enable Adaptive Query Execution (AQE)\n\n1️⃣6️⃣ **Explain the architecture of Azure Databricks integrated with Delta Lake in a production environment.**\n- Databricks workspace with Unity Catalog for governance\n- Azure Data Lake Storage Gen2 for data",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_5",
    "text": "Azure Data Factory pipelines?**\n- Use watermark columns to track last processed data\n- Implement lookup activities to retrieve watermarks\n- Use conditional activities for incremental vs. full loads\n- Store watermarks in Azure SQL Database or Key Vault\n- Implement error handling and retry policies\n\n1️⃣5️⃣ **Your PySpark job is failing due to skewed joins. Walk through your debugging and optimization steps.**\n1. Identify skew using Spark UI (look for tasks taking much longer)\n2. Analyze data distribution on join keys\n3. Apply salting technique for skewed keys\n4. Use broadcast joins for small tables\n5. Repartition data before joins\n6. Enable Adaptive Query Execution (AQE)\n\n1️⃣6️⃣ **Explain the architecture of Azure Databricks integrated with Delta Lake in a production environment.**\n- Databricks workspace with Unity Catalog for governance\n- Azure Data Lake Storage Gen2 for data persistence\n- Azure Key Vault for secrets management\n- Azure Active Directory for authentication\n- Azure Monitor for logging and monitoring\n- CI/CD pipeline with Azure DevOps\n\n1️⃣7️⃣ **How do you optimize query performance and concurrency in a Synapse dedicated SQL pool?**\n- Implement proper distribution strategies (hash, round-robin, replicated)\n- Use columnstore indexes for analytical workloads\n- Optimize statistics and create covering indexes\n- Implement workload management with resource classes\n- Use result set caching for repeated queries\n\n1️⃣8️⃣ **Your data lake contains sensitive PII data. What best practices do you follow to secure data and manage secrets?**\n- Implement encryption at rest and in transit\n- Use Azure Key Vault for secret management\n- Apply row-level security and column-level encryption\n- Implement data masking for non-production environments\n- Use Azure Purview for data classification and lineage\n- Regular security audits and access reviews\n\n1️⃣9️⃣ **How do you establish end-to-end data lineage and governance using Microsoft Purview?**\n- Register data sources (Azure Data Lake, SQL databases, etc.)\n- Configure automated scanning for schema discovery\n- Set up data classification and sensitivity labels\n- Implement access policies and data retention rules\n- Create lineage maps showing data flow\n- Enable compliance reporting and auditing\n\n2️⃣0️⃣ **Design an end-to-end analytics pipeline that ingests from Event Hub, processes data in Databricks, stores it in Synapse, and powers dashboards in Power BI.**\n```\nEvent Hub → Azure Data Factory → Databricks (Streaming) → Delta Lake → \nSynapse (via PolyBase/Copy) → Power BI → Business Users\n\nComponents:\n- Event Hub: Real-time data ingestion\n- ADF: Orchestration and scheduling\n- Databricks: Stream processing and ML\n- Delta Lake: ACID transactions and schema evolution\n- Synapse: Data warehouse and analytics\n- Power BI: Visualization and reporting\n- Azure Key Vault: Secrets management\n- Azure Monitor: Monitoring and alerting\n```\n\n## Key Success Factors for Data Engineering Interviews\n\n1. **Technical Depth**: Understand the \"why\" behind tools and technologies\n2. **System Design**: Think about scalability, reliability, and maintainability\n3. **Problem Solving**: Break down complex problems into manageable pieces\n4. **Communication**: Explain technical concepts clearly and concisely\n5. **Real-world Experience**: Draw from actual project experiences and challenges\n6. **Continuous Learning**: Stay updated with latest trends and best practices\n\nRemember: Interviewers want to see how you think through problems, not just memorized answers. Focus on demonstrating your analytical approach and practical experience.",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "infa_migration_examples_0",
    "text": "# Informatica Migration Examples\n\n## Example: Expression + Filter + Lookup → PySpark\n\n```python\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nsrc = (spark.read.format(\"jdbc\")\n  .option(\"url\", jdbc_url)\n  .option(\"dbtable\", \"SRC_ORDERS\")\n  .load())\n\nlkp = (spark.read.format(\"jdbc\")\n  .option(\"url\", jdbc_url)\n  .option(\"dbtable\", \"DIM_PRODUCT\")\n  .load())\n\n# Expression (IIF) & standardization\ncur = (src\n  .withColumn(\"TRIM_SKU\", F.trim(\"SKU\"))\n  .withColumn(\"ORDER_DT\", F.to_timestamp(\"ORDER_TS\"))\n  .withColumn(\"IS_PRIORITY\",\n              F.when(F.col(\"PRIORITY_FLAG\") == \"Y\", F.lit(1)).otherwise(F.lit(0)))\n  .filter(\"STATUS = 'COMPLETE'\"))\n\n# Lookup (broadcast) + join\ncur = (cur.join(F.broadcast(lkp.select(\"SKU\",\"PRODUCT_ID\")), on=\"SKU\", how=\"left\")\n          .fillna({\"PRODUCT_ID\": -1}))\n\n# Surrogate key (sequence-like)\nw = Window.orderBy(F.monotonically_increasing_id())\ncur = cur.withColumn(\"LINE_ID\", F.row_number().over(w))\n\n# Final projection\nout_df = cur.select(\"LINE_ID\", \"ORDER_ID\", \"PRODUCT_ID\", \"ORDER_DT\", \"IS_PRIORITY\")\n```\n\n## Example: Write to Snowflake via Connector\n\n```python\nsfOptions = {\n  \"sfURL\": SF_URL,\n  \"sfUser\": SF_USER,\n  \"sfPassword\": SF_PWD,     # or OAuth\n  \"sfDatabase\": \"SALES\",\n  \"sfSchema\": \"CURATED\",\n  \"sfWarehouse\": \"WH_XL\",\n  \"sfRole\": \"SYSADMIN\"\n}\n(out_df\n  .write.format(\"snowflake\")\n  .options(**sfOptions)\n  .option(\"dbtable\", \"ORDERS_STAGE\")\n  .mode(\"append\")\n  .save())\n```\n\n## Example: Stage + COPY Pattern\n\n* Write partitioned Parquet to ADLS/S3 (e.g., by `order_dt`).\n* Call Snowflake:\n\n  ```sql\n  COPY INTO SALES.CURATED.ORDERS\n  FROM @ext_stage/orders/\n  FILE_FORMAT=(TYPE=PARQUET)\n  MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n  ON_ERROR=ABORT_STATEMENT;\n  ```\n* Wrap in task/pipe for automation.\n\n## Example: Aggregator → PySpark\n\n```python\n# Informatica Aggregator becomes:\nagg_df = (src_df\n  .groupBy(\"STORE_ID\", \"PRODUCT_CATEGORY\")\n  .agg(\n    F.sum(\"SALES_AMOUNT\").alias(\"TOTAL_SALES\"),\n    F.count(\"*\").alias(\"TRANSACTION_COUNT\"),\n    F.avg(\"UNIT_PRICE\").alias(\"AVG_PRICE\"),\n    F.max(\"SALE_DATE\").alias(\"LAST_SALE\")\n  )\n  .filter(\"TOTAL_SALES > 1000\"))  # Having clause\n```\n\n## Example: Router → Multiple Outputs\n\n```python\n# Router with multiple conditions\nhigh_value = df.filter(\"SALES_AMOUNT > 10000\")\nmedium_value = df.filter(\"SALES_AMOUNT BETWEEN 1000 AND 10000\")\nlow_value = df.filter(\"SALES_AMOUNT < 1000\")\n\n# Write to different targets\nhigh_value.write.mode(\"append\").saveAsTable(\"high_value_sales\")\nmedium_value.write.mode(\"append\").saveAsTable(\"medium_value_sales\")\nlow_value.write.mode(\"append\").saveAsTable(\"low_value_sales\")\n```\n\n## Example: Sequence Generator → Window Functions\n\n```python\n# Informatica sequence becomes:\nw = Window.partitionBy(\"ORDER_ID\").orderBy(\"LINE_NUMBER\")\ndf_with_seq = df.withColumn(\"SEQUENCE_ID\", F.row_number().over(w))\n\n# Or for Snowflake sequences on write:\ndf_with_seq = df.withColumn(\"SEQUENCE_ID\", F.expr(\"NEXTVAL('order_seq')\"))\n```",
    "metadata": {
      "tags": [
        "data-eng",
        "examples",
        "mapping",
        "pyspark",
        "snowflake"
      ],
      "persona": "de",
      "file_path": "data_eng/infa_migration_examples.md",
      "file_name": "infa_migration_examples.md"
    }
  },
  {
    "id": "sql_interview_questions_0",
    "text": "# SQL Interview Questions & Solutions\n\n## Finding Second Highest Salary Without TOP/LIMIT\n\n### Method 1: Using Subquery\n```sql\nSELECT MAX(salary) \nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n```\n\n### Method 2: Using Window Functions (RANK)\n```sql\nSELECT salary \nFROM (\n    SELECT salary, RANK() OVER (ORDER BY salary DESC) as salary_rank\n    FROM employees\n) ranked_salaries \nWHERE salary_rank = 2;\n```\n\n### Method 3: Using Window Functions (ROW_NUMBER)\n```sql\nSELECT salary \nFROM (\n    SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) as salary_rank\n    FROM employees\n) ranked_salaries \nWHERE salary_rank = 2;\n```\n\n### Method 4: Using Self Join\n```sql\nSELECT DISTINCT e1.salary\nFROM employees e1\nWHERE 1 = (SELECT COUNT(DISTINCT e2.salary) \n           FROM employees e2 \n           WHERE e2.salary > e1.salary);\n```\n\n## Common SQL Interview Patterns\n\n### Nth Highest Salary (Generic)\n```sql\nSELECT salary \nFROM (\n    SELECT salary, DENSE_RANK() OVER (ORDER BY salary DESC) as salary_rank\n    FROM employees\n) ranked_salaries \nWHERE salary_rank = N;\n```\n\n### Find Duplicates\n```sql\nSELECT column_name, COUNT(*)\nFROM table_name\nGROUP BY column_name\nHAVING COUNT(*) > 1;\n```\n\n### Delete Duplicates (Keep One)\n```sql\nDELETE e1 FROM employees e1\nINNER JOIN employees e2 \nWHERE e1.id > e2.id AND e1.email = e2.email;\n```\n\n### Find Employees with Same Salary\n```sql\nSELECT e1.name, e1.salary\nFROM employees e1\nINNER JOIN employees e2 ON e1.salary = e2.salary AND e1.id != e2.id;\n```",
    "metadata": {
      "tags": [
        "sql",
        "interview",
        "queries",
        "window-functions",
        "subqueries"
      ],
      "persona": "de",
      "file_path": "data_eng/sql_interview_questions.md",
      "file_name": "sql_interview_questions.md"
    }
  },
  {
    "id": "data_engineer_questions_0",
    "text": "# Data Engineering Interview Questions & Answers\n\n## Core PySpark + Databricks Questions\n\n### Q1. What is your experience with PySpark in your current project?\n**A:** In my Walgreens project, I built end-to-end PySpark pipelines in Databricks that processed over 10TB monthly. I handled ingestion of raw data into Bronze, applied transformations and schema validation in Silver, and modeled fact/dimension tables in Gold for reporting. I frequently used PySpark DataFrame APIs, window functions, UDFs, and joins. For example, I implemented deduplication logic using `dropDuplicates()` and window functions, ensuring the most recent transaction record is retained.\n\n### Q2. How do you handle large datasets in PySpark efficiently?\n**A:** First, I design partitioning strategies at ingestion (date-based partitions). Second, I tune Spark configurations like `spark.sql.shuffle.partitions` and executor memory. Third, I use optimizations such as broadcast joins for small lookup tables and salting for skewed keys. For large jobs, I enable Adaptive Query Execution. In one case, these changes reduced a pipeline runtime from 2+ hours to 40 minutes.\n\n### Q3. How do you debug PySpark jobs when they fail?\n**A:** I start with Spark UI to analyze DAGs, stages, and tasks. I check for skew (few tasks taking much longer) or memory errors. Then I check logs in Databricks for stack traces. I usually replay the job with a smaller dataset to isolate the failing transformation. For example, when a job failed due to a corrupt record, I added `_corrupt_record` handling and moved bad rows into a quarantine table.\n\n### Q4. How do you handle nested JSON in PySpark?\n**A:** I define a `StructType` schema, use `from_json` to parse the string column, and `explode` for arrays. Then I flatten with `withColumn` to extract nested attributes. For deeply nested JSON, I use recursive functions to normalize. Finally, I store it as a clean Delta table in Silver for downstream consumption.\n\n### Q5. How do you handle duplicates in PySpark?\n**A:** For simple duplicates, I use `dropDuplicates()`. For business-specific deduplication, I use window functions with `row_number()` ordered by timestamp, keeping only the latest row. In Walgreens, this was used for handling multiple prescription updates from pharmacy systems.\n\n### Q6. How do you handle schema drift in PySpark pipelines?\n**A:** In Bronze, I ingest with permissive mode to avoid job failure. In Silver, I enforce strict schema. If new columns appear, I add them with defaults in Silver after business validation. For Delta tables, I use `mergeSchema` in controlled deployments, never blindly. This allows flexibility but avoids breaking downstream queries.\n\n## Advanced PySpark & Delta Lake Questions\n\n### Q7. How do you design incremental loads in PySpark?\n**A:** I use watermarking (modified_date column) or surrogate keys. ADF passes parameters to notebooks for last processed date. In PySpark, I filter only incremental rows and apply Delta merge to update/inserts. This reduced daily runs from processing 10TB full to ~300GB delta, saving cost and runtime.\n\n### Q8. Can you explain time travel in Delta Lake? How have you used it?\n**A:** Time travel lets me query data at a specific version or timestamp. At Walgreens, one job overwrote 2 days of data. Using `VERSION AS OF` in Delta, I restored the table to its previous state in minutes without reloading raw files.\n\n### Q9. How do you handle slowly changing dimensions (SCD) in Databricks?\n**A:** I used Delta merge for Type 2. Old record is closed with an end date, new record inserted with active flag = 1. This keeps historical changes. For example, when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use PERMISSIVE mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.\n\n### Q13",
    "metadata": {
      "tags": [
        "pyspark",
        "databricks",
        "data-engineering",
        "etl",
        "dataframe",
        "delta-lake",
        "json",
        "schema-drift",
        "optimization",
        "debugging",
        "walgreens",
        "azure-data-factory"
      ],
      "persona": "de",
      "file_path": "data_eng/data_engineer_questions.md",
      "file_name": "data_engineer_questions.md"
    }
  },
  {
    "id": "data_engineer_questions_1",
    "text": "when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use PERMISSIVE mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.\n\n### Q13. How do you test PySpark pipelines?\n**A:** Unit tests validate transformations with small sample data. Row count reconciliation checks ingestion completeness. Schema validation checks enforce consistency. We automated these checks in CI/CD pipelines.\n\n### Q14. How do you manage dependencies across notebooks in Databricks?\n**A:** I modularize common logic (like validations, schema enforcement) in utility notebooks or .py files stored in repos. Then I import them into main notebooks. This avoids code duplication and keeps pipelines maintainable.\n\n### Q15. How do you handle late arriving data?\n**A:** I use watermarking in Delta tables, so late data is still merged if within X days. If outside retention, we load them manually after business approval.\n\n## Performance Optimization & Best Practices\n\n### Q16. How do you handle large joins across multiple datasets?\n**A:** First, partition both datasets on the join key. If one is small, broadcast it. If skew occurs, apply salting. If still heavy, break into smaller joins and cache intermediate results.\n\n### Q17. How do you manage PySpark code for reusability?\n**A:** I follow modular design: separate ingestion, transformation, validation, and load functions. I store configs in parameter files, not hardcoded. Reusable frameworks allowed offshore to easily plug in new sources with minimal code.\n\n### Q18. How do you optimize PySpark DataFrame transformations?\n**A:** Avoid wide transformations until necessary, use select instead of *, cache intermediate results when reused, and avoid UDFs unless unavoidable. Vectorized operations (pandas UDFs) are faster than row-wise ones.\n\n### Q19. How do you manage error handling in PySpark?\n**A:** I wrap critical transformations with try/except. Failures are logged into error tables. In ADF, we configure retries and failure alerts. This ensures job doesn't fail silently.\n\n### Q20. What's the biggest challenge you solved with PySpark at Walgreens?\n**A:** Optimizing a 10TB sales fact pipeline that originally took 2+ hours. By tuning partitioning, salting skewed joins, and compacting files, I reduced runtime to 40 minutes. This improved SLA compliance and cut costs by 30%.\n\n## Key Takeaways\n\n- **Performance**: Proper partitioning, broadcast joins, and adaptive query execution are crucial\n- **Data Quality**: Implement comprehensive error handling and data validation\n- **Monitoring**: Log metrics and create dashboards for pipeline health\n- **Testing**: Unit tests and automated validation ensure pipeline reliability\n- **Code Organization**: Modular design improves maintainability and reusability\n- **Delta Lake**: Leverage time travel and merge capabilities for data management",
    "metadata": {
      "tags": [
        "pyspark",
        "databricks",
        "data-engineering",
        "etl",
        "dataframe",
        "delta-lake",
        "json",
        "schema-drift",
        "optimization",
        "debugging",
        "walgreens",
        "azure-data-factory"
      ],
      "persona": "de",
      "file_path": "data_eng/data_engineer_questions.md",
      "file_name": "data_engineer_questions.md"
    }
  },
  {
    "id": "informatica_to_pyspark_0",
    "text": "# Informatica → PySpark Migration Playbook\n\n## Overall Approach\n1) **Inventory & lineage**: export mapping XML / repository report; list source/target tables, lookups, joins, aggregations, filters, expressions, sequences.\n2) **Semantics parity**: for each mapping, capture row-order assumptions, null-handling, case-sensitivity, timezone, numeric precision/scale, surrogate keys.\n3) **Transform translation**: map each Informatica transformation → PySpark function (see table).\n4) **Framework**: build a small PySpark \"operator\" layer so pipelines are declarative (config-driven).\n5) **Testing**: golden-data tests (row/column counts, nulls, min/max, checksums), sample-based parity, KPI comparisons.\n6) **Orchestration**: ADF or Databricks Workflows; parameterize envs; promote via DevOps CI/CD.\n7) **Performance**: partitioning, broadcast/AQE, file compaction; pushdown where possible.\n8) **Snowflake load**: choose **Spark → Snowflake connector** (batch) or **stage + COPY** (Snowpipe/auto-ingest) for high-throughput and decoupling.\n\n## Common Transformation Mapping\n- **Source Qualifier** → `spark.read` with predicates; pushdown via JDBC filter if possible.\n- **Expression** (IIF/DECODE/SUBSTR/UPPER) → `when/otherwise`, `expr()`, `substring`, `upper`, `coalesce`.\n- **Filter** → `df.filter(\"...\")`.\n- **Lookup** → left join with a broadcast-hint; handle not-found as default/null + `when` logic.\n- **Joiner** → `df.join(other, keys, \"inner/left/right/full\")`; use `broadcast()` for small tables.\n- **Aggregator** → `groupBy(...).agg(...)` with careful null-handling and data types.\n- **Router** → multiple filters writing to multiple sinks.\n- **Sequence Generator** → window functions with `row_number()` or Snowflake sequences on write.\n- **Union** → `unionByName` with `allowMissingColumns=True`.\n- **Sorter/Rank** → `orderBy`, window specs with `row_number`, `dense_rank`.\n\n## Null & Datatype Rules\n- Match Informatica's null rules explicitly; use `coalesce`, cast to target schema, standardize timestamp TZ and string trimming.\n\n## Incremental Patterns\n- Watermark on `last_update_ts`; Bronze → Silver Delta with `MERGE INTO`; or CDC using file-based change tables.\n- For Snowflake targets, prefer **idempotent upserts** using stage + `COPY INTO` to temp table + `MERGE`.\n\n## Migration Strategy\nStart with simple mappings (single source, basic transformations), validate parity, then tackle complex joins and aggregations. Use a staging approach where you run both systems in parallel for validation before switching over.",
    "metadata": {
      "tags": [
        "data-eng",
        "informatica",
        "migration",
        "pyspark",
        "mapping",
        "transformations",
        "snowflake"
      ],
      "persona": "de",
      "file_path": "data_eng/informatica_to_pyspark.md",
      "file_name": "informatica_to_pyspark.md"
    }
  },
  {
    "id": "snowflake_load_patterns_0",
    "text": "# Snowflake Load Patterns\n\n## A) Spark → Snowflake Connector (Simple)\n- Use `net.snowflake:spark-snowflake` + `net.snowflake:snowflake-jdbc`.\n- Write mode: `append` to a staging table; finalize with Snowflake-side `MERGE`.\n- Good for medium volumes; keeps pipeline in one place (Databricks).\n\n**Example write:**\n- options: `sfURL`, `sfUser`, `sfPassword`/`sfOAuth`, `sfDatabase`, `sfSchema`, `sfWarehouse`, `sfRole`.\n- `df.write.format(\"snowflake\").options(**sfOptions).option(\"dbtable\",\"STAGE_TABLE\").mode(\"append\").save()`\n\n## B) External Stage + COPY INTO (High-throughput / Decoupled)\n1) Spark writes Parquet to cloud storage (ADLS/S3) with partitioning.\n2) Snowflake `COPY INTO <table>` from the external stage with file pattern.\n3) Automate with **Snowpipe** or orchestration tool; track file manifests / checkpoints.\n\n**Pros:** better parallelism, resilient to retries, clear separation of compute concerns.\n**Cons:** more moving parts (stages, pipes, notifications).\n\n## Performance & Cost Tips\n- Use `COPY INTO` with `ON_ERROR='CONTINUE'` to isolate bad files; quarantine & reprocess.\n- Cluster keys only where pruning helps. Avoid over-clustering.\n- Compress & partition data output; align Snowflake micro-partitions with query patterns.\n- Prefer `MERGE` with small change sets; if very large upsert, consider swap: load to temp + swap/rename.\n\n## Load Patterns by Use Case\n- **Batch loads**: Spark connector for simplicity, stage+COPY for high volume\n- **Real-time**: Snowpipe with auto-ingest from cloud storage\n- **Upserts**: Stage temp table + MERGE for idempotency\n- **Historical reloads**: Direct COPY INTO with file pattern matching\n\n## Monitoring & Validation\n- Track file ingestion status via Snowflake metadata views\n- Monitor COPY performance and error rates\n- Validate row counts and data quality post-load\n- Set up alerts for failed loads or SLA breaches",
    "metadata": {
      "tags": [
        "data-eng",
        "snowflake",
        "spark-connector",
        "copy-into",
        "snowpipe",
        "performance"
      ],
      "persona": "de",
      "file_path": "data_eng/snowflake_load_patterns.md",
      "file_name": "snowflake_load_patterns.md"
    }
  },
  {
    "id": "azure_data_factory_0",
    "text": "# Azure Data Factory (ADF) Best Practices\n\n## Overview\nAzure Data Factory is a cloud-based data integration service that allows you to create, schedule, and manage data pipelines for moving and transforming data.\n\n## Core Concepts\n\n### 1. Data Factory Architecture\n- **Linked Services**: Connection strings to external data stores\n- **Datasets**: Structure of data within linked services\n- **Pipelines**: Logical grouping of activities that perform a task\n- **Activities**: Individual steps in a pipeline\n- **Triggers**: Schedule or event-based pipeline execution\n\n### 2. Data Movement Patterns\n- **Copy Activity**: Move data between data stores\n- **Data Flow**: Visual data transformation using Spark\n- **Custom Activities**: Execute custom code or scripts\n- **Stored Procedure Activity**: Execute SQL stored procedures\n\n## Pipeline Design Patterns\n\n### ETL vs ELT Architecture\n- **ETL (Extract-Transform-Load)**: Transform data before loading\n- **ELT (Extract-Load-Transform)**: Load raw data, then transform\n- **Hybrid Approach**: Use both patterns based on data characteristics\n\n### Incremental Data Processing\n```json\n{\n  \"source\": {\n    \"type\": \"SqlServerSource\",\n    \"sqlReaderQuery\": \"SELECT * FROM Orders WHERE ModifiedDate > '@{pipeline().parameters.WatermarkValue}'\"\n  },\n  \"sink\": {\n    \"type\": \"SqlServerSink\",\n    \"tableName\": \"Orders_Staging\"\n  }\n}\n```\n\n### Error Handling and Retry Logic\n- **Retry Policy**: Configure automatic retries for transient failures\n- **Error Output**: Route failed records to error tables\n- **Dead Letter Queue**: Handle permanently failed records\n- **Monitoring**: Set up alerts for pipeline failures\n\n## Data Flow Best Practices\n\n### Performance Optimization\n- **Partitioning**: Use appropriate partitioning strategies\n- **Caching**: Enable caching for frequently accessed data\n- **Data Skew**: Handle data skew in transformations\n- **Resource Allocation**: Right-size cluster resources\n\n### Transformation Patterns\n```sql\n-- Derived Column transformation\nCASE \n    WHEN [OrderDate] < DATEADD(day, -30, GETDATE()) THEN 'Historical'\n    WHEN [OrderDate] < DATEADD(day, -7, GETDATE()) THEN 'Recent'\n    ELSE 'Current'\nEND AS [OrderCategory]\n```\n\n### Data Quality Checks\n- **Null Value Handling**: Define strategies for missing data\n- **Data Type Validation**: Ensure data types match expectations\n- **Range Validation**: Check data falls within expected ranges\n- **Referential Integrity**: Validate foreign key relationships\n\n## Integration Patterns\n\n### Real-time Data Processing\n- **Event Grid**: Trigger pipelines on data changes\n- **Stream Analytics**: Process streaming data\n- **Change Data Capture**: Capture incremental changes\n- **IoT Hub Integration**: Process IoT device data\n\n### Batch Processing\n- **Schedule Triggers**: Regular batch processing schedules\n- **Tumbling Windows**: Fixed-size time windows\n- **Sliding Windows**: Overlapping time windows\n- **Session Windows**: Activity-based windows\n\n## Monitoring and Troubleshooting\n\n### Pipeline Monitoring\n- **Activity Runs**: Monitor individual activity execution\n- **Pipeline Runs**: Track overall pipeline performance\n- **Trigger Runs**: Monitor trigger-based executions\n- **Data Flow Runs**: Track data transformation performance\n\n### Performance Metrics\n- **Data Volume**: Track data processed per pipeline run\n- **Execution Time**: Monitor pipeline duration\n- **Success Rate**: Track pipeline success/failure rates\n- **Resource Utilization**: Monitor compute resource usage\n\n### Alerting and Notifications\n- **Email Alerts**: Notify on pipeline failures\n- **Azure Monitor**: Integrate with monitoring dashboards\n- **Custom Metrics**: Define business-specific KPIs\n- **Log Analytics**: Centralized logging and analysis\n\n## Security and Governance\n\n### Access Control\n- **Managed Identity**: Use Azure AD managed identities\n- **Role-Based Access**: Implement least privilege access\n- **Key Vault Integration**: Secure credential storage\n- **Private Endpoints**: Secure network connectivity\n\n### Data Lineage\n- **Data Catalog**: Track data lineage and dependencies\n- **Metadata Management**: Document data definitions\n- **Impact Analysis**: Understand downstream effects\n- **Compliance Tracking**: Meet regulatory requirements\n\n## Cost Optimization\n\n### Resource Management\n- **Auto-scaling**: Scale resources based on demand\n- **Reserved Capacity**: Use reserved instances for predictable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Resource Tagging**: Track costs by project or department\n\n### Pipeline Optimization\n- **Incremental Processing**: Process only changed data\n- **Parallel Execution**: Run independent activities in parallel\n- **Data Compression**: Compress data in transit and at rest\n- **Efficient Transformations**: Optimize transformation logic\n\n## Common Use Cases\n\n### Data",
    "metadata": {
      "tags": [
        "adf",
        "azure",
        "etl",
        "data-pipeline"
      ],
      "persona": "de",
      "file_path": "data_eng/azure_data_factory.md",
      "file_name": "azure_data_factory.md"
    }
  },
  {
    "id": "azure_data_factory_1",
    "text": "Azure AD managed identities\n- **Role-Based Access**: Implement least privilege access\n- **Key Vault Integration**: Secure credential storage\n- **Private Endpoints**: Secure network connectivity\n\n### Data Lineage\n- **Data Catalog**: Track data lineage and dependencies\n- **Metadata Management**: Document data definitions\n- **Impact Analysis**: Understand downstream effects\n- **Compliance Tracking**: Meet regulatory requirements\n\n## Cost Optimization\n\n### Resource Management\n- **Auto-scaling**: Scale resources based on demand\n- **Reserved Capacity**: Use reserved instances for predictable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Resource Tagging**: Track costs by project or department\n\n### Pipeline Optimization\n- **Incremental Processing**: Process only changed data\n- **Parallel Execution**: Run independent activities in parallel\n- **Data Compression**: Compress data in transit and at rest\n- **Efficient Transformations**: Optimize transformation logic\n\n## Common Use Cases\n\n### Data Lake Ingestion\n- **Multi-source Ingestion**: Ingest from various data sources\n- **Schema Evolution**: Handle changing data schemas\n- **Data Validation**: Ensure data quality during ingestion\n- **Metadata Extraction**: Capture and store metadata\n\n### Data Warehouse Loading\n- **Dimension Loading**: Load slowly changing dimensions\n- **Fact Table Loading**: Load fact tables with proper partitioning\n- **Aggregation Processing**: Pre-compute aggregations\n- **Data Mart Creation**: Create subject-specific data marts\n\n### Data Migration\n- **On-premises to Cloud**: Migrate from on-premises systems\n- **Cloud to Cloud**: Move between cloud providers\n- **Database Migration**: Migrate between database systems\n- **Application Migration**: Support application modernization\n\n## Troubleshooting Guide\n\n### Common Issues\n- **Connection Timeouts**: Check network connectivity and firewall rules\n- **Memory Issues**: Optimize data flow transformations\n- **Performance Degradation**: Review partitioning and caching strategies\n- **Data Quality Issues**: Implement data validation checks\n\n### Debugging Techniques\n- **Activity Run Details**: Review detailed execution logs\n- **Data Flow Debug**: Use debug mode for data flow development\n- **Sample Data**: Use sample data for testing\n- **Incremental Testing**: Test with small data subsets\n\n## Migration Strategies\n\n### Legacy System Integration\n- **API Integration**: Connect to legacy APIs\n- **File-based Integration**: Process legacy file formats\n- **Database Integration**: Connect to legacy databases\n- **Message Queue Integration**: Process legacy message queues\n\n### Cloud Migration\n- **Lift and Shift**: Move existing pipelines to cloud\n- **Cloud-native Redesign**: Redesign for cloud-native patterns\n- **Hybrid Approach**: Maintain some on-premises components\n- **Gradual Migration**: Migrate incrementally over time",
    "metadata": {
      "tags": [
        "adf",
        "azure",
        "etl",
        "data-pipeline"
      ],
      "persona": "de",
      "file_path": "data_eng/azure_data_factory.md",
      "file_name": "azure_data_factory.md"
    }
  },
  {
    "id": "interview_voice_0",
    "text": "# Interview Voice & Communication Style\n\n## Interview Cadence\nStart with context (one line), state approach (one line), give 2–3 concrete steps, note a trade-off, close with how to validate/monitor. Avoid jargon dumps. If pressed, go one level deeper (config, code primitive).\n\n## Response Structure\n1. **Context**: Brief understanding of the problem\n2. **Approach**: High-level strategy in one sentence\n3. **Steps**: 2-3 concrete implementation steps\n4. **Trade-offs**: Mention one key consideration or alternative\n5. **Validation**: How to verify or monitor the solution\n\n## Tone Guidelines\n- **Confident**: Direct, decisive language with clear recommendations\n- **Collaborative**: \"We can...\" language, asking for input\n- **Cautious**: \"I'd typically...\" with hedging and validation steps\n\n## Depth Levels\n- **Short**: 2-3 sentences, core approach only\n- **Mid**: 4-6 sentences with key steps and one trade-off\n- **Deep**: Detailed implementation with multiple options and validation\n\n## Common Patterns\n- Lead with the business impact or technical constraint\n- Use specific tools/technologies (PySpark, ADF, Delta Lake)\n- Mention concrete metrics (runtime, cost, SLA)\n- Reference Spark UI, lineage, or monitoring for validation\n- Acknowledge when multiple approaches exist and pick one with reasoning",
    "metadata": {
      "tags": [
        "style",
        "interview",
        "cadence",
        "communication"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_voice.md",
      "file_name": "interview_voice.md"
    }
  },
  {
    "id": "data enginner_0",
    "text": "# 📄 OG Resume – Krishna Sathvik (v2)\n\n---\n\n## Summary\n\nData Engineer with 6+ years’ experience evolving from backend development into advanced data engineering. Skilled in Databricks, PySpark, SQL, and Azure, with proven success delivering 10TB+ scale pipelines, medallion architectures, and governance frameworks. Recognized for performance tuning, cost optimization, and enabling analytics that improve decision-making across healthcare, retail, and finance domains.\n\n---\n\n## Experience\n\n**Data Engineer | TCS (Walgreens, USA) | Feb 2022 – Present**\n\nEngineered Delta Lakehouse pipelines in Databricks + PySpark, processing 10TB+ monthly data across sales, pharmacy, and supply chain, enabling real-time analytics and reducing ad-hoc reporting by 32%\n\nDesigned medallion architecture flows (Bronze, Silver, Gold) with schema enforcement, deduplication, and fact/dimension modeling, improving KPI reporting accuracy 30%\n\nAutomated orchestration with ADF triggering Databricks notebooks, implementing dynamic parameterization and CI/CD via Azure DevOps to achieve zero-downtime deployments\n\nImplemented Unity Catalog for role-based access, PII masking, and lineage tracking, ensuring compliance and secure access for regulated data and anonymized reporting\n\nOptimized partitioning strategies, resolved skew with salting and broadcast joins, and compacted small Delta files—reducing critical job runtime from 2+ hours to under 40 minutes\n\n**Analytics Engineer | CVS Health (USA) | Jan 2021 – Jan 2022**\n\nBuilt ingestion pipelines in ADF + Databricks integrating supply chain and sales data, producing audit-ready datasets powering KPI dashboards across 200+ retail sites\n\nModeled fact/dimension schemas in SQL + dbt, improving reconciliation agility and accuracy 20% while supporting finance and operations decision-making\n\nCollaborated with stakeholders to define business rules for replenishment and billing, surfacing 20+ KPIs leveraged by leadership for operational strategy\n\nOptimized Databricks jobs with incremental models and clustering, cutting compute costs 18% and improving pipeline performance 25%\n\n**Data Science Intern | McKesson (USA) | May 2020 – Dec 2020**\n\nAutomated ETL scripts in Python + SQL reducing ingestion latency 50%, accelerating delivery of compliance dashboards for executive monitoring\n\nBuilt forecasting models aligning patient demand with supply capacity, preventing mismatches and driving \\$20M+ savings in procurement\n\nProduced insights from claims and sales data supporting compliance reviews and informing leadership’s cost-recovery initiatives\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nDeveloped backend APIs and optimized SQL queries for ERP modules, improving system response 35% and strengthening transactional accuracy in client billing\n\nDesigned reporting modules surfacing missed payments and contract discrepancies, cutting manual reconciliation and improving transparency in logistics workflows\n\n---\n\n## Skills\n\n- **Data Engineering & ELT:** Databricks, PySpark, dbt, Airflow, ADF\n- **Cloud Platforms:** Azure (Synapse, Data Factory, DevOps), Snowflake, AWS (Glue, Lambda)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle\n- **Analytics & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Orchestration:** GitHub Actions, Azure DevOps, Docker, Kubernetes\n- **Governance & Security:** Data Quality Validation, PII Masking, Unity Catalog\n\n---\n\n# 📑 Resume Optimization Framework – v2\n\n## 1. Structure & Bullet Rules\n\n**5-4-3-2 Rule**\n\n- Walgreens → 5 bullets | Present tense | Outcome + metrics | Core tech (Databricks, PySpark, dbt, Azure)\n- CVS → 4 bullets | Past tense | Analytics enablement focus + optimization | ADF + dbt + SQL\n- McKesson → 3 bullets | Past tense | Forecasting + audit outcomes | Python + SQL + models\n- Inditek → 2 bullets | Past tense | ERP/API outcomes | SQL + APIs | Efficiency gains\n\n**Character Constraint Rule**\n\n- Each bullet = **220–240 characters**\n- Never <215, never >240\n\n---\n\n## 2. Domain Adaptation Layer\n\nAdapt **vocabulary + emphasis** per JD:\n\n- Healthcare/Compliance → audit-ready datasets, regulatory workflows, compliance monitoring\n- Retail/Audit → financial integrity, recovery audits, supplier agreements, reconciliation\n- Tech/Data Platforms → streaming pipelines, real-time analytics, Kafka/Event Hubs, ML integration\n- Finance/AI/ML → PySpark models, AWS Glue, risk compliance, forecasting, ML deployment\n\n---\n\n## 3. Skills Optimization Layer\n\n**Always keep 6 OG categories:**\n\n1. Data Engineering",
    "metadata": {
      "persona": "de",
      "file_path": "data_eng/data enginner.md",
      "file_name": "data enginner.md"
    }
  },
  {
    "id": "data enginner_1",
    "text": "+ optimization | ADF + dbt + SQL\n- McKesson → 3 bullets | Past tense | Forecasting + audit outcomes | Python + SQL + models\n- Inditek → 2 bullets | Past tense | ERP/API outcomes | SQL + APIs | Efficiency gains\n\n**Character Constraint Rule**\n\n- Each bullet = **220–240 characters**\n- Never <215, never >240\n\n---\n\n## 2. Domain Adaptation Layer\n\nAdapt **vocabulary + emphasis** per JD:\n\n- Healthcare/Compliance → audit-ready datasets, regulatory workflows, compliance monitoring\n- Retail/Audit → financial integrity, recovery audits, supplier agreements, reconciliation\n- Tech/Data Platforms → streaming pipelines, real-time analytics, Kafka/Event Hubs, ML integration\n- Finance/AI/ML → PySpark models, AWS Glue, risk compliance, forecasting, ML deployment\n\n---\n\n## 3. Skills Optimization Layer\n\n**Always keep 6 OG categories:**\n\n1. Data Engineering & ELT\n2. Cloud Platforms\n3. Databases & Storage\n4. Analytics & BI\n5. DevOps & Orchestration\n6. Governance & Security\n\n**JD Alignment Examples:**\n\n- **Azure-Heavy JD**\n\n  - Data Engineering & ELT: Databricks, PySpark, ADF, dbt, Airflow\n  - Cloud Platforms: Azure (Synapse, Data Factory, DevOps), Snowflake\n  - Databases & Storage: SQL Server, Delta Lake, PostgreSQL\n  - Analytics & BI: Power BI, Tableau\n  - DevOps & Orchestration: Azure DevOps, GitHub Actions, Docker\n  - Governance & Security: Unity Catalog, PII Masking, Data Quality Validation\n\n- **AWS-Heavy JD**\n\n  - Data Engineering & ELT: PySpark, dbt, Airflow, AWS Glue\n  - Cloud Platforms: AWS (Glue, Lambda, MSK/Kafka, S3), Snowflake\n  - Databases & Storage: PostgreSQL, SQL Server, Redshift\n  - Analytics & BI: Tableau, Power BI\n  - DevOps & Orchestration: GitHub Actions, Docker, Kubernetes\n  - Governance & Security: Data Lineage, PII Masking, Compliance Frameworks\n\n- **BI/Analytics-Heavy JD**\n\n  - Data Engineering & ELT: SQL, dbt, ADF, Databricks (light emphasis)\n  - Cloud Platforms: Azure (Synapse, Data Factory), AWS (basic S3/Glue)\n  - Databases & Storage: SQL Server, Delta Lake\n  - Analytics & BI: Power BI, Tableau, KPI Dashboards\n  - DevOps & Orchestration: Azure DevOps, GitHub Actions\n  - Governance & Security: Data Quality Validation, Anonymization, Lineage Tracking\n\n---\n\n## 4. Verb Rotation Rule\n\nRotate verbs to avoid repetition: **Engineer, Optimize, Automate, Deploy, Collaborate, Design, Implement, Develop, Create, Scale, Orchestrate, Modernize**\n\n---\n\n## 5. Metrics Bank Rule\n\nEvery bullet ties to outcome + metric:\n\n- % improvements (22% cost reduction, 30% reliability boost)\n- Scale (10TB+, 1.2B rows)\n- Time savings (30+ hours weekly)\n- Financial impact (\\$20M+ savings)\n- Adoption (200+ sites, 50+ pipelines)\n\n---\n\n# 📋 JD → Resume Adaptation Checklist\n\n1. **Read JD closely** → Highlight cloud platform (AWS vs Azure), pipeline type (batch vs streaming), domain (healthcare, retail, finance).\n2. **Apply Domain Layer** → Swap vocabulary to mirror JD (audit, compliance, ML, streaming, etc).\n3. **Reorder Skills** → Keep 6 categories but move JD-priority tools to the front.\n4. **Rewrite Bullets** → Use 5-4-3-2 rule, adjust tech names, keep impact + metrics fixed.\n5. **Rotate Verbs** → Swap starting verbs per bullet to keep flow strong.\n6. **Check Character Count** → Ensure 220–240 characters per bullet.\n7. **Final Scan** → ATS keywords match JD, measurable outcomes present, tense alignment correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**Azure-Heavy JD (Walgreens, CVS, Microsoft ecosystem)**\\\nData Engineer with 6+ years’ experience designing and scaling pipelines on Azure and Databricks. Skilled in PySpark, SQL, ADF, and medallion architecture with proven success optimizing costs, enforcing governance, and enabling real-time analytics at enterprise scale.\n\n**AWS-Heavy JD (Moody’s, Possible Finance, startups)**\\\nData Engineer with 6+ years’ experience building ELT pipelines on AWS using Glue,",
    "metadata": {
      "persona": "de",
      "file_path": "data_eng/data enginner.md",
      "file_name": "data enginner.md"
    }
  },
  {
    "id": "data enginner_2",
    "text": "4. **Rewrite Bullets** → Use 5-4-3-2 rule, adjust tech names, keep impact + metrics fixed.\n5. **Rotate Verbs** → Swap starting verbs per bullet to keep flow strong.\n6. **Check Character Count** → Ensure 220–240 characters per bullet.\n7. **Final Scan** → ATS keywords match JD, measurable outcomes present, tense alignment correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**Azure-Heavy JD (Walgreens, CVS, Microsoft ecosystem)**\\\nData Engineer with 6+ years’ experience designing and scaling pipelines on Azure and Databricks. Skilled in PySpark, SQL, ADF, and medallion architecture with proven success optimizing costs, enforcing governance, and enabling real-time analytics at enterprise scale.\n\n**AWS-Heavy JD (Moody’s, Possible Finance, startups)**\\\nData Engineer with 6+ years’ experience building ELT pipelines on AWS using Glue, Lambda, MSK/Kafka, and S3. Expertise in PySpark, dbt, and SQL with a strong record of optimizing performance, reducing costs, and delivering audit-ready data platforms across finance and retail.\n\n**BI/Analytics-Heavy JD (Analytics Engineer / BI Dev focus)**\\\nAnalytics Engineer with 6+ years’ experience bridging data engineering and BI. Designed SQL + dbt models, KPI frameworks, and automated ETL pipelines powering Power BI/Tableau dashboards. Skilled in Databricks, Python, and governance frameworks to deliver accurate, business-ready insights.\n\n---",
    "metadata": {
      "persona": "de",
      "file_path": "data_eng/data enginner.md",
      "file_name": "data enginner.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_0",
    "text": "# Comprehensive Data Engineering Q&A - 120 Questions\n\n## Section A: Core PySpark + Databricks (20 Questions)\n\n### Q1. What is your experience with PySpark in your current project?\n**A:** In my Walgreens project, I built end-to-end PySpark pipelines in Databricks that processed over 10TB monthly. I handled ingestion of raw data into Bronze, applied transformations and schema validation in Silver, and modeled fact/dimension tables in Gold for reporting. I frequently used PySpark DataFrame APIs, window functions, UDFs, and joins. For example, I implemented deduplication logic using `dropDuplicates()` and window functions, ensuring the most recent transaction record is retained.\n\n### Q2. How do you handle large datasets in PySpark efficiently?\n**A:** First, I design partitioning strategies at ingestion (date-based partitions). Second, I tune Spark configurations like `spark.sql.shuffle.partitions` and executor memory. Third, I use optimizations such as broadcast joins for small lookup tables and salting for skewed keys. For large jobs, I enable Adaptive Query Execution. In one case, these changes reduced a pipeline runtime from 2+ hours to 40 minutes.\n\n### Q3. How do you debug PySpark jobs when they fail?\n**A:** I start with Spark UI to analyze DAGs, stages, and tasks. I check for skew (few tasks taking much longer) or memory errors. Then I check logs in Databricks for stack traces. I usually replay the job with a smaller dataset to isolate the failing transformation. For example, when a job failed due to a corrupt record, I added `_corrupt_record` handling and moved bad rows into a quarantine table.\n\n### Q4. How do you handle nested JSON in PySpark?\n**A:** I define a StructType schema, use `from_json` to parse the string column, and `explode` for arrays. Then I flatten with `withColumn` to extract nested attributes. For deeply nested JSON, I use recursive functions to normalize. Finally, I store it as a clean Delta table in Silver for downstream consumption.\n\n### Q5. How do you handle duplicates in PySpark?\n**A:** For simple duplicates, I use `dropDuplicates()`. For business-specific deduplication, I use window functions with `row_number()` ordered by timestamp, keeping only the latest row. In Walgreens, this was used for handling multiple prescription updates from pharmacy systems.\n\n### Q6. How do you handle schema drift in PySpark pipelines?\n**A:** In Bronze, I ingest with permissive mode to avoid job failure. In Silver, I enforce strict schema. If new columns appear, I add them with defaults in Silver after business validation. For Delta tables, I use `mergeSchema` in controlled deployments, never blindly. This allows flexibility but avoids breaking downstream queries.\n\n### Q7. How do you design incremental loads in PySpark?\n**A:** I use watermarking (modified_date column) or surrogate keys. ADF passes parameters to notebooks for last processed date. In PySpark, I filter only incremental rows and apply Delta `merge` to update/inserts. This reduced daily runs from processing 10TB full to ~300GB delta, saving cost and runtime.\n\n### Q8. Can you explain time travel in Delta Lake? How have you used it?\n**A:** Time travel lets me query data at a specific version or timestamp. At Walgreens, one job overwrote 2 days of data. Using `VERSION AS OF` in Delta, I restored the table to its previous state in minutes without reloading raw files.\n\n### Q9. How do you handle slowly changing dimensions (SCD) in Databricks?\n**A:** I used Delta `merge` for Type 2. Old record is closed with an end_date, new record inserted with active_flag = 1. This keeps historical changes. For example, when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use `PERMISSIVE` mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_1",
    "text": ", when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use `PERMISSIVE` mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.\n\n### Q13. How do you test PySpark pipelines?\n**A:** Unit tests validate transformations with small sample data. Row count reconciliation checks ingestion completeness. Schema validation checks enforce consistency. We automated these checks in CI/CD pipelines.\n\n### Q14. How do you manage dependencies across notebooks in Databricks?\n**A:** I modularize common logic (like validations, schema enforcement) in utility notebooks or .py files stored in repos. Then I import them into main notebooks. This avoids code duplication and keeps pipelines maintainable.\n\n### Q15. How do you handle late arriving data?\n**A:** I use watermarking in Delta tables, so late data is still merged if within X days. If outside retention, we load them manually after business approval.\n\n### Q16. How do you handle large joins across multiple datasets?\n**A:** First, partition both datasets on the join key. If one is small, broadcast it. If skew occurs, apply salting. If still heavy, break into smaller joins and cache intermediate results.\n\n### Q17. How do you manage PySpark code for reusability?\n**A:** I follow modular design: separate ingestion, transformation, validation, and load functions. I store configs in parameter files, not hardcoded. Reusable frameworks allowed offshore to easily plug in new sources with minimal code.\n\n### Q18. How do you optimize PySpark DataFrame transformations?\n**A:** Avoid wide transformations until necessary, use `select` instead of `*`, cache intermediate results when reused, and avoid UDFs unless unavoidable. Vectorized operations (pandas UDFs) are faster than row-wise ones.\n\n### Q19. How do you manage error handling in PySpark?\n**A:** I wrap critical transformations with try/except. Failures are logged into error tables. In ADF, we configure retries and failure alerts. This ensures job doesn't fail silently.\n\n### Q20. What's the biggest challenge you solved with PySpark at Walgreens?\n**A:** Optimizing a 10TB sales fact pipeline that originally took 2+ hours. By tuning partitioning, salting skewed joins, and compacting files, I reduced runtime to 40 minutes. This improved SLA compliance and cut costs by 30%.\n\n## Section B: Azure Data Factory (ADF) & Orchestration (20 Questions)\n\n### Q21. How did you use ADF in your Walgreens project?\n**A:** I used ADF mainly for orchestration and scheduling of Databricks notebooks. ADF pipelines triggered ingestion of raw files into ADLS, parameterized notebook runs with date filters, and coordinated dependencies across multiple jobs. For example, when pharmacy and sales data needed to be processed together, ADF ensured ingestion → transformation → validation were executed in sequence with retries on failure.\n\n### Q22. How do you parameterize ADF pipelines?\n**A:** I use ADF global parameters and dataset parameters to make pipelines dynamic. For example, the source file path is parameterized with date and source name, so the same pipeline ingests multiple sources. Databricks notebook activities accept parameters for start_date and end_date, allowing incremental processing without hardcoding.\n\n### Q23. How do you handle scheduling in ADF?\n**A:** I use triggers — primarily tumbling window triggers for periodic jobs like daily loads, and event triggers for file arrival. For critical pipelines, we used tumbling window with retry policies. Business-critical reports were scheduled nightly at 2am, ensuring that Silver/Gold tables were refreshed before business started.\n\n### Q24. How do you handle failure in ADF pipelines?\n**A:** I configure retry policies on activities and failure paths to send alerts. Failed activities log errors in monitoring tables. For example, if a Databricks notebook failed due to schema mismatch, ADF retried once, and on repeated failure, triggered a Logic App alert to teams.\n\n### Q25. How do you design",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_2",
    "text": ", the source file path is parameterized with date and source name, so the same pipeline ingests multiple sources. Databricks notebook activities accept parameters for start_date and end_date, allowing incremental processing without hardcoding.\n\n### Q23. How do you handle scheduling in ADF?\n**A:** I use triggers — primarily tumbling window triggers for periodic jobs like daily loads, and event triggers for file arrival. For critical pipelines, we used tumbling window with retry policies. Business-critical reports were scheduled nightly at 2am, ensuring that Silver/Gold tables were refreshed before business started.\n\n### Q24. How do you handle failure in ADF pipelines?\n**A:** I configure retry policies on activities and failure paths to send alerts. Failed activities log errors in monitoring tables. For example, if a Databricks notebook failed due to schema mismatch, ADF retried once, and on repeated failure, triggered a Logic App alert to teams.\n\n### Q25. How do you design ADF pipelines for scalability?\n**A:** I avoid building one massive pipeline. Instead, I create modular pipelines — ingestion, transformations, validations — and link them with pipeline chaining. I also parameterize everything, so the same pipeline works across multiple sources. This reduced maintenance and improved reusability for offshore team.\n\n### Q26. How do you handle dependencies between ADF pipelines?\n**A:** I use dependency triggers and pipeline chaining. For example, Silver pipeline starts only after Bronze ingestion completes successfully. If multiple datasets must be ready before Gold processing, I use \"Wait on Activity\" or success/failure dependencies.\n\n### Q27. How do you integrate ADF with Databricks?\n**A:** ADF has a \"Databricks Notebook\" activity. I configured linked services with Key Vault for secrets, so credentials aren't hardcoded. Each notebook call passes parameters like file_date, source system, or load_type. This ensured pipelines remained dynamic and secure.\n\n### Q28. How do you handle incremental data loads with ADF?\n**A:** I store last run watermark in a metadata table. ADF fetches this value, passes it to Databricks as parameter. Databricks then queries only new/updated data and merges it into Delta. After successful load, ADF updates the watermark. This ensured daily loads were efficient.\n\n### Q29. How do you monitor ADF pipelines?\n**A:** I use ADF monitoring dashboard for real-time status. In addition, I log metadata like run_id, row counts, and errors into custom Delta tables. Failures are reported via email alerts. Power BI dashboards show historical success/failure trends, which helped management track SLA adherence.\n\n### Q30. How do you secure ADF pipelines?\n**A:** Secrets like database passwords and storage keys are stored in Azure Key Vault. ADF uses managed identities to connect to ADLS and Databricks. This eliminated hardcoding sensitive info.\n\n### Q31. How do you design ADF for reusability?\n**A:** I use parameterized datasets and pipeline templates. For example, one generic ingestion pipeline handled all CSV sources by passing schema, file_path, and delimiter as parameters. Offshore team just configured metadata, no code changes.\n\n### Q32. How do you handle event-driven ingestion in ADF?\n**A:** For real-time scenarios, I set up Event Grid triggers so ADF pipelines started as soon as new files landed in ADLS. This reduced latency from hours to minutes for near-real-time data availability.\n\n### Q33. How do you integrate ADF with CI/CD?\n**A:** ADF JSON definition files are stored in Git. Azure DevOps pipelines deploy these JSONs across dev, test, and prod environments. Parameters like connection strings are environment-specific and replaced during deployment using ARM templates.\n\n### Q34. How do you deal with long-running pipelines?\n**A:** For long pipelines, I break them into smaller pipelines with checkpoints. This ensures partial success is saved and we don't restart everything on failure. For example, ingestion pipeline completed successfully even if transformation pipeline failed, allowing us to restart only transformations.\n\n### Q35. How do you manage data validation with ADF?\n**A:** After ingestion, I run validation notebooks triggered by ADF. These check row counts, null ratios, and duplicates. ADF logs validation status into Delta tables. Alerts are raised if thresholds are violated.\n\n### Q36. How do you manage metadata in ADF pipelines?\n**A:** I store pipeline configs (file paths, schema, business rules) in metadata tables. ADF reads metadata at runtime and applies ingestion accordingly. This approach made pipelines completely dynamic — new sources onboarded without code changes.\n\n### Q37. How do you optimize ADF performance?\n**A:** I configure parallel copy in copy activities, use staging in ADLS/Blob, and partition large files. For example, one large file was split into",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_3",
    "text": "with checkpoints. This ensures partial success is saved and we don't restart everything on failure. For example, ingestion pipeline completed successfully even if transformation pipeline failed, allowing us to restart only transformations.\n\n### Q35. How do you manage data validation with ADF?\n**A:** After ingestion, I run validation notebooks triggered by ADF. These check row counts, null ratios, and duplicates. ADF logs validation status into Delta tables. Alerts are raised if thresholds are violated.\n\n### Q36. How do you manage metadata in ADF pipelines?\n**A:** I store pipeline configs (file paths, schema, business rules) in metadata tables. ADF reads metadata at runtime and applies ingestion accordingly. This approach made pipelines completely dynamic — new sources onboarded without code changes.\n\n### Q37. How do you optimize ADF performance?\n**A:** I configure parallel copy in copy activities, use staging in ADLS/Blob, and partition large files. For example, one large file was split into multiple blocks and ingested in parallel, reducing ingestion time from 40 minutes to under 10.\n\n### Q38. How do you orchestrate multiple technologies with ADF?\n**A:** ADF orchestrated ADLS, Databricks, Synapse, and Snowflake in my project. For example, ingestion from APIs landed in ADLS, processed in Databricks, exported to Snowflake, and then visualized in Power BI. ADF coordinated the entire workflow end-to-end.\n\n### Q39. How do you handle SLA in ADF pipelines?\n**A:** I tracked expected runtime and row counts. If pipelines exceeded thresholds, alerts triggered. For pharmacy data, SLA was 6am reporting availability — ADF was scheduled at 2am with monitoring, so if job failed, support team had time to re-run before SLA breach.\n\n### Q40. What challenges did you face with ADF and how did you solve them?\n**A:** One challenge was schema drift causing failures in ingestion. I solved it by using schema drift-tolerant ingestion at Bronze and enforcing strict schema at Silver. Another was long ingestion times, solved with parallel copy and partitioning. I also improved security by integrating Key Vault and managed identities, removing all hardcoded secrets.\n\n## Section C: Delta Lake, Schema, Data Modeling (20 Questions)\n\n### Q41. How did you design the medallion architecture in Walgreens?\n**A:** I followed the medallion architecture — Bronze, Silver, and Gold layers. Bronze captured raw ingested data exactly as it arrived from sources, tolerant to schema drift. Silver applied schema enforcement, deduplication, and standardization, making the data clean and query-ready. Gold was modeled into fact and dimension tables optimized for reporting. This layered approach made pipelines robust, reusable, and easy for business teams to consume.\n\n### Q42. How do you enforce schema in Delta Lake?\n**A:** I applied explicit schemas during ingestion, not relying on inference in production. For schema evolution, I used `mergeSchema` in controlled updates. For example, when a new column was introduced in sales data, I validated it in dev, updated downstream logic, and then enabled schema merge. This prevented silent failures.\n\n### Q43. How do you handle schema drift?\n**A:** Bronze is flexible — it accepts extra columns and quarantines bad rows. Silver enforces schema strictly. Any new column is validated in dev first. If valid, I add it with defaults and update documentation. This two-layer enforcement avoids unexpected breakages.\n\n### Q44. How do you design fact and dimension tables in Gold?\n**A:** I model fact tables to capture transactional data like sales, and dimension tables to capture master data like product, store, and customer. Facts include foreign keys to dimensions. I also denormalize selectively for performance. KPIs like revenue per store were modeled in Gold for Power BI dashboards.\n\n### Q45. How do you design SCD (Slowly Changing Dimensions)?\n**A:** I implemented Type 2 SCD with Delta merge. When a dimension attribute changes, I close the old record with an end_date and insert a new record with active_flag = 1. This preserved historical accuracy. Example: product price changes required Type 2 so reports showed past sales at old prices.\n\n### Q46. How do you manage historical versions of data?\n**A:** Delta Lake's time travel feature allowed querying older versions of tables. For example, when an accidental overwrite occurred, I restored previous version using `VERSION AS OF`. This was faster than reprocessing from raw files.\n\n### Q47. How do you deal with null values in schema enforcement?\n**A:** I use PySpark `fillna` or business rules to assign defaults. In Silver, nulls in critical columns are flagged in validation tables. Business-approved defaults like \"Unknown\" for missing store_id ensure pipelines don't break.\n\n### Q48",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_4",
    "text": "How do you design SCD (Slowly Changing Dimensions)?\n**A:** I implemented Type 2 SCD with Delta merge. When a dimension attribute changes, I close the old record with an end_date and insert a new record with active_flag = 1. This preserved historical accuracy. Example: product price changes required Type 2 so reports showed past sales at old prices.\n\n### Q46. How do you manage historical versions of data?\n**A:** Delta Lake's time travel feature allowed querying older versions of tables. For example, when an accidental overwrite occurred, I restored previous version using `VERSION AS OF`. This was faster than reprocessing from raw files.\n\n### Q47. How do you deal with null values in schema enforcement?\n**A:** I use PySpark `fillna` or business rules to assign defaults. In Silver, nulls in critical columns are flagged in validation tables. Business-approved defaults like \"Unknown\" for missing store_id ensure pipelines don't break.\n\n### Q48. How do you validate transformations across layers?\n**A:** I reconcile row counts and key distributions from Bronze → Silver → Gold. I log validation results into Delta monitoring tables. For example, I checked that deduplication didn't drop more records than expected.\n\n### Q49. How do you design partitioning in Delta tables?\n**A:** I usually partition large datasets by date, as most queries are time-based. For multi-dimensional queries, I use ZORDER indexing. Example: sales fact was partitioned by transaction_date, with ZORDER on store_id and product_id for efficient pruning.\n\n### Q50. How do you manage small file problems in Delta?\n**A:** I use `OPTIMIZE` in Databricks to compact small Parquet files into larger ones. I also control batch size during ingestion in ADF to avoid excessive tiny files. For Gold, I scheduled weekly compaction jobs to keep query performance high.\n\n### Q51. How do you design data models for reporting?\n**A:** I followed Kimball principles: fact tables for metrics, dimension tables for descriptive attributes, and star schema design. I ensured measures like sales_amount were additive and dimensions like date, product, and store enabled slice-and-dice in Power BI.\n\n### Q52. How do you manage data lineage?\n**A:** Unity Catalog and Purview tracked lineage across layers. For example, lineage view showed pharmacy source files → Silver clean table → Gold fact → Power BI dashboard. This transparency helped in audits and troubleshooting.\n\n### Q53. How do you handle late-arriving facts in modeling?\n**A:** I used Delta merge to insert or update late-arriving facts. If fact arrived after dimension change, I ensured it still mapped correctly using surrogate keys. For example, late prescription transactions still mapped to the correct product dimension.\n\n### Q54. How do you manage surrogate keys in dimensions?\n**A:** I generated surrogate keys in Silver using hash functions on natural keys. These surrogate keys became dimension table primary keys. Fact tables stored foreign keys referencing them. This approach ensured consistency even if natural keys changed.\n\n### Q55. How do you validate fact/dimension consistency?\n**A:** I ran referential integrity checks — ensuring every fact foreign key matched a valid dimension key. Invalid records were flagged in a quarantine table for review.\n\n### Q56. How do you manage incremental loads into Gold models?\n**A:** I used Delta `merge` for upserts. Facts were loaded incrementally based on modified_date. Dimensions were updated using SCD logic. This kept models fresh without reprocessing full history.\n\n### Q57. How do you optimize Gold layer models for BI?\n**A:** I pre-aggregated summary tables for common KPIs, reduced joins by denormalizing small dimensions, and compacted files. This ensured Power BI dashboards loaded in seconds instead of minutes.\n\n### Q58. How do you handle multi-source integration in modeling?\n**A:** I standardized schemas across sources in Silver. Then I conformed them into unified dimensions. Example: pharmacy and sales sources had different store codes. I standardized codes and built a single store dimension in Gold.\n\n### Q59. How do you document data models?\n**A:** I maintained data dictionaries in Confluence, showing column definitions, lineage, and business rules. Unity Catalog also stored schema metadata. This documentation helped both developers and business users.\n\n### Q60. What challenges did you face in data modeling and how did you solve them?\n**A:** One challenge was aligning different source systems with inconsistent schemas. I solved it by applying conformance rules in Silver and building standardized dimensions. Another challenge was query slowness in Power BI; I solved it by pre-aggregating summary tables and optimizing partitioning.\n\n## Section D: Optimization, Performance & Troubleshooting (20 Questions)\n\n### Q61. How did you optimize Spark jobs for large data volumes?\n**A:** First, I ensured partitioning",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_5",
    "text": "schemas across sources in Silver. Then I conformed them into unified dimensions. Example: pharmacy and sales sources had different store codes. I standardized codes and built a single store dimension in Gold.\n\n### Q59. How do you document data models?\n**A:** I maintained data dictionaries in Confluence, showing column definitions, lineage, and business rules. Unity Catalog also stored schema metadata. This documentation helped both developers and business users.\n\n### Q60. What challenges did you face in data modeling and how did you solve them?\n**A:** One challenge was aligning different source systems with inconsistent schemas. I solved it by applying conformance rules in Silver and building standardized dimensions. Another challenge was query slowness in Power BI; I solved it by pre-aggregating summary tables and optimizing partitioning.\n\n## Section D: Optimization, Performance & Troubleshooting (20 Questions)\n\n### Q61. How did you optimize Spark jobs for large data volumes?\n**A:** First, I ensured partitioning strategy matched query patterns, usually date-based. Then, I tuned shuffle partitions (`spark.sql.shuffle.partitions`) based on cluster size, avoiding both too few (skew) and too many (overhead). For joins, I used broadcast for small tables, salting for skew, and AQE (Adaptive Query Execution) to rebalance tasks. I also compacted small files using `OPTIMIZE`. One sales pipeline dropped from 2+ hours to 40 mins with these steps.\n\n### Q62. How do you identify bottlenecks in a Spark job?\n**A:** I use Spark UI to analyze DAG stages, tasks, and shuffle read/write sizes. If some tasks run much longer, it usually signals skew. If GC overhead is high, executors need memory tuning. If there's high shuffle volume, I check if joins/aggregations are causing unnecessary repartitions.\n\n### Q63. How do you resolve skew in Spark joins?\n**A:** If skew is caused by a few heavy keys, I apply salting — appending a random suffix to distribute skewed keys across partitions. For small lookup joins, I use `broadcast()`. AQE also automatically splits skewed partitions at runtime in Databricks.\n\n### Q64. How do you optimize Delta Lake performance?\n**A:** I regularly run `OPTIMIZE` with ZORDER on frequently filtered columns. I compact small files into larger Parquet files, improving metadata handling. I also vacuum old versions to reduce storage overhead. For query pruning, I carefully partition on high-cardinality columns like date, not on low-cardinality ones.\n\n### Q65. How do you handle small file problems?\n**A:** I control ingestion batch size in ADF to avoid generating thousands of tiny files. After ingestion, I use `OPTIMIZE` in Delta to compact them. For streaming, I use auto-compaction. This reduces metadata load and speeds up queries.\n\n### Q66. How do you handle long-running jobs?\n**A:** First, I profile the job in Spark UI to identify slow stages. Then, I repartition or broadcast joins as needed. If the job processes full data daily, I redesign it to incremental load. For one job that ran for 5+ hours, converting to incremental reduced it to under 1 hour.\n\n### Q67. How do you tune Spark cluster configurations?\n**A:** I tune driver/executor memory based on data size. I increase executor cores for parallelism but avoid too many to prevent GC pressure. I use autoscaling for heavy workloads, but for cost control, I size clusters to match partitioning. I also enable cache for reused datasets.\n\n### Q68. How do you optimize joins in PySpark?\n**A:** I decide based on size: Small table + large table → broadcast join; Large tables with skew → repartition + salting; Balanced large tables → hash partition on join keys. I always avoid Cartesian joins and prune unnecessary columns before joining.\n\n### Q69. How do you handle out-of-memory issues in Spark jobs?\n**A:** I check if wide transformations (e.g., groupBy) are blowing up. Then, I increase executor memory or repartition data to spread load. I persist intermediate results on disk instead of memory if needed. In Walgreens, this fixed a memory issue with 1B+ row aggregation.\n\n### Q70. How do you optimize aggregations in PySpark?\n**A:** I partition data on aggregation keys, cache intermediate results if reused, and pre-filter unnecessary rows early. For distinct counts, I used approx algorithms like HyperLogLog when exact wasn't needed, saving resources.\n\n### Q71. How do you debug frequent job failures?\n**A:** I check Databricks logs for stack traces, isolate failing transformation, and test with sample data. If schema mismatch, I enforce schema in Bronze. If bad records, I redirect to quarantine. If infrastructure, I scale cluster or",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_6",
    "text": "joining.\n\n### Q69. How do you handle out-of-memory issues in Spark jobs?\n**A:** I check if wide transformations (e.g., groupBy) are blowing up. Then, I increase executor memory or repartition data to spread load. I persist intermediate results on disk instead of memory if needed. In Walgreens, this fixed a memory issue with 1B+ row aggregation.\n\n### Q70. How do you optimize aggregations in PySpark?\n**A:** I partition data on aggregation keys, cache intermediate results if reused, and pre-filter unnecessary rows early. For distinct counts, I used approx algorithms like HyperLogLog when exact wasn't needed, saving resources.\n\n### Q71. How do you debug frequent job failures?\n**A:** I check Databricks logs for stack traces, isolate failing transformation, and test with sample data. If schema mismatch, I enforce schema in Bronze. If bad records, I redirect to quarantine. If infrastructure, I scale cluster or tune configs.\n\n### Q72. How do you tune pipeline latency?\n**A:** I parallelize independent tasks in ADF, use event triggers for real-time ingestion, and optimize Spark transformations for early filtering. For dashboards, I pre-aggregate Gold tables so BI loads in seconds.\n\n### Q73. How do you troubleshoot data quality issues raised by business?\n**A:** First, I trace lineage in Unity Catalog or Purview to identify which layer/data caused it. Then, I check validations in Silver logs. If caused by schema drift, I fix mapping and reprocess. Communication is key — I keep business updated on issue status and fix ETA.\n\n### Q74. How do you deal with high shuffle volume in Spark jobs?\n**A:** I reduce unnecessary shuffles by avoiding multiple repartitions, pruning columns early, and reusing partitioning. For unavoidable large shuffles, I increase shuffle partitions and enable AQE.\n\n### Q75. How do you handle slow dashboards due to data issues?\n**A:** I optimize Gold tables by compacting files, pre-aggregating metrics, and using summary tables. I also partition models so Power BI queries can prune efficiently. One KPI dashboard load time went from 90s to 15s after introducing summary tables.\n\n### Q76. How do you debug performance issues across layers (Bronze → Silver → Gold)?\n**A:** I compare row counts and timings logged at each layer. If Bronze is fine but Silver is slow, I check schema enforcement and dedup logic. If Gold is slow, I check joins and aggregations. Logging at each step helps isolate bottlenecks.\n\n### Q77. How do you optimize pipelines for cost?\n**A:** I use job clusters with auto-termination instead of always-on clusters. I right-size clusters based on workload. I compact files to reduce storage and metadata cost. For non-critical jobs, I use spot instances.\n\n### Q78. How do you approach troubleshooting late data arrival?\n**A:** I check if ingestion trigger failed in ADF. If source delayed, I escalate to source team. If files arrived but schema mismatched, I fix schema mapping and re-run partial load. I log SLA misses to ensure business visibility.\n\n### Q79. How do you ensure optimized queries in BI tools?\n**A:** I pre-aggregate Gold data, reduce table joins by denormalizing small dimensions, and ensure partition pruning. I also monitor Power BI query logs to tune backend models accordingly.\n\n### Q80. What's the toughest optimization challenge you solved?\n**A:** A sales fact pipeline processing 10TB+ daily was breaching SLA. Spark jobs had heavy shuffles and skewed joins. I applied salting, ZORDER, and partition tuning, and compacted files weekly. This reduced runtime from 2 hours to 40 minutes, restored SLA compliance, and saved 25% compute cost.\n\n## Section E: Governance, CI/CD, Validation, Offshore & Stakeholder Collaboration (20 Questions)\n\n### Q81. How did you implement data governance in Walgreens?\n**A:** We used Unity Catalog as the central governance layer. It controlled table- and column-level access, enforced policies like masking PII, and provided lineage. For example, DOB and SSN columns were masked for analysts while full access was limited to compliance teams. Purview was also integrated to show lineage across ADF, Databricks, and Power BI.\n\n### Q82. How do you protect PII data in Databricks?\n**A:** I masked PII columns in Unity Catalog, enforced row-level security policies, and ensured encryption at rest in ADLS. Keys and secrets were stored in Key Vault, never in code. For reporting, Power BI consumed masked views, ensuring compliance while still supporting analysis.\n\n### Q83. How do you ensure role-based access?\n**A:** Unity Catalog roles were mapped to business roles.",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_7",
    "text": "/CD, Validation, Offshore & Stakeholder Collaboration (20 Questions)\n\n### Q81. How did you implement data governance in Walgreens?\n**A:** We used Unity Catalog as the central governance layer. It controlled table- and column-level access, enforced policies like masking PII, and provided lineage. For example, DOB and SSN columns were masked for analysts while full access was limited to compliance teams. Purview was also integrated to show lineage across ADF, Databricks, and Power BI.\n\n### Q82. How do you protect PII data in Databricks?\n**A:** I masked PII columns in Unity Catalog, enforced row-level security policies, and ensured encryption at rest in ADLS. Keys and secrets were stored in Key Vault, never in code. For reporting, Power BI consumed masked views, ensuring compliance while still supporting analysis.\n\n### Q83. How do you ensure role-based access?\n**A:** Unity Catalog roles were mapped to business roles. Developers had write access in dev, read-only in test/prod. Analysts had read-only access to Gold tables only. Access was granted at schema/table/column levels, following the principle of least privilege.\n\n### Q84. How do you manage data lineage?\n**A:** Unity Catalog and Purview automatically captured lineage from ingestion → transformation → Gold → dashboards. This made it easy to trace an issue in a dashboard back to the source system. Business teams used lineage views during audits to verify compliance.\n\n### Q85. How do you integrate Databricks with Azure DevOps CI/CD?\n**A:** We stored all notebooks and ADF pipelines in Git repos. Azure DevOps pipelines deployed them to different environments. Databricks CLI automated notebook deployment, and ADF JSONs were deployed with ARM templates. Environment variables replaced connection strings dynamically.\n\n### Q86. How do you test pipelines before deployment?\n**A:** I implemented unit tests on sample datasets, row count reconciliations, and schema validations in lower environments. Each PR triggered DevOps tests to ensure correctness before merging to main. Only validated pipelines were deployed to higher environments.\n\n### Q87. How do you handle rollback in CI/CD if deployment fails?\n**A:** Each deployment version was tagged in Git. If a pipeline failed in prod, I rolled back to the last stable version quickly using Git tags and redeployment. Delta Lake time travel also supported rolling back data changes.\n\n### Q88. How do you validate data quality automatically?\n**A:** I built a PySpark validation framework that ran checks like duplicates, nulls, referential integrity, and business rules. Results were logged into validation Delta tables. Any violations triggered ADF failure path alerts and were visible in Power BI dashboards.\n\n### Q89. How do you report data quality to business?\n**A:** Business users accessed a Power BI dashboard that showed row counts, duplicates, null % by table, and rule violations. This gave real-time transparency into data health. Business could drill down to see which rules failed.\n\n### Q90. How do you monitor SLA compliance?\n**A:** I logged pipeline run durations and compared them to SLA thresholds. If a pipeline breached SLA, ADF triggered alerts. Weekly SLA adherence reports were shared with stakeholders to ensure trust in data delivery.\n\n### Q91. How do you collaborate with offshore teams?\n**A:** I led daily standups with offshore, reviewing backlog and helping unblock them. I also created reusable PySpark frameworks (ingestion, validation) so offshore could onboard new sources with minimal coding. Code reviews and knowledge-sharing sessions ensured they ramped up quickly.\n\n### Q92. How do you mentor offshore developers?\n**A:** I reviewed their PySpark scripts, explained optimization techniques like partitioning and salting, and walked them through Spark UI. I also created runbooks for common errors so they could resolve issues without escalation.\n\n### Q93. How do you handle production incidents with offshore?\n**A:** Offshore raised tickets during their shift. I joined morning calls, reviewed logs, and guided them in root cause analysis. If it was schema drift, I advised schema mapping. If infrastructure, I helped with cluster tuning. Communication with business ensured transparency on resolution ETA.\n\n### Q94. How do you communicate with stakeholders?\n**A:** I tailored communication: technical details for developers, impact and ETA for business stakeholders. For example, when a pipeline failed due to schema drift, I told business: \"Data will be delayed by 1 hour while we patch schema. No data loss.\" This maintained confidence.\n\n### Q95. How do you balance technical delivery with business priorities?\n**A:** I aligned backlog with business SLA commitments. Critical sales reports were prioritized for early-morning refresh. Less critical pipelines (like historical reloads) were scheduled during off-peak. This kept business impact minimal.\n\n### Q96. How do you approach handling unexpected requests?",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_8",
    "text": "incidents with offshore?\n**A:** Offshore raised tickets during their shift. I joined morning calls, reviewed logs, and guided them in root cause analysis. If it was schema drift, I advised schema mapping. If infrastructure, I helped with cluster tuning. Communication with business ensured transparency on resolution ETA.\n\n### Q94. How do you communicate with stakeholders?\n**A:** I tailored communication: technical details for developers, impact and ETA for business stakeholders. For example, when a pipeline failed due to schema drift, I told business: \"Data will be delayed by 1 hour while we patch schema. No data loss.\" This maintained confidence.\n\n### Q95. How do you balance technical delivery with business priorities?\n**A:** I aligned backlog with business SLA commitments. Critical sales reports were prioritized for early-morning refresh. Less critical pipelines (like historical reloads) were scheduled during off-peak. This kept business impact minimal.\n\n### Q96. How do you approach handling unexpected requests?\n**A:** I first clarify urgency with the business, then estimate technical impact. If it's quick (like adding a column), I handle same day. If bigger (like new dataset), I put it in sprint backlog. For example, adding a new KPI to Power BI was prioritized within 24h because executives needed it.\n\n### Q97. How do you ensure compliance in pipelines?\n**A:** I ensured encryption at rest (ADLS), masking in Unity Catalog, and logging of all access requests. Data quality dashboards ensured transparency. For audits, Purview lineage reports showed full data flows, proving compliance.\n\n### Q98. How do you track pipeline metrics over time?\n**A:** I logged row counts, runtime, and errors into Delta monitoring tables. A Power BI dashboard visualized historical pipeline performance, showing trends in failures or runtimes. This helped proactively optimize before SLAs broke.\n\n### Q99. How do you ensure knowledge transfer across teams?\n**A:** I maintained detailed documentation in Confluence, covering pipeline design, schema definitions, and troubleshooting steps. I also held KT sessions with offshore, walking through real examples in Databricks notebooks.\n\n### Q100. What's your biggest governance or collaboration achievement?\n**A:** My biggest achievement was implementing Unity Catalog + validation framework end-to-end. It ensured PII was masked, data lineage was transparent, and business saw data quality dashboards. Combined with mentoring offshore, it built trust in the system. Stakeholders appreciated that pipelines were compliant, optimized, and reliable — all while I led a distributed team.\n\n## Section F: Scenario-Based Questions (20 Questions)\n\n### S1. Scenario: A downstream Power BI dashboard shows wrong sales numbers. How do you handle it?\n**A:** First, I'd trace lineage in Unity Catalog to identify which Gold table feeds that report. Then, I'd check Silver → Gold transformations for logic errors. I'd validate row counts and business rules in validation logs. If the issue came from source schema drift (like a new column added in pharmacy data), I'd patch mapping in Silver, reprocess affected partitions, and communicate with stakeholders immediately, explaining ETA for fix. This keeps business confidence while resolving the root cause.\n\n### S2. Scenario: Your pipeline starts failing at 2am due to corrupt input files. What's your approach?\n**A:** I'd configure ingestion with `PERMISSIVE` mode so corrupt rows are flagged into `_corrupt_record`. These rows would be redirected into a quarantine Delta table. The main pipeline would continue for good rows, avoiding SLA breach. Later, I'd review the bad records, escalate to source teams, and patch schema validation if needed. This way, business dashboards still refresh on time.\n\n### S3. Scenario: You need to migrate Synapse notebooks into Databricks. How would you do it?\n**A:** Synapse transformations are SQL-based. I'd first review existing T-SQL scripts, then rewrite them into PySpark DataFrame transformations in Databricks. I'd leverage Delta Lake features like ACID and time travel for consistency. To validate migration, I'd reconcile row counts, run sample KPI checks, and parallel-run old Synapse vs new Databricks for a cycle before cutover.\n\n### S4. Scenario: A job that usually runs in 30 minutes suddenly takes 2 hours. How do you troubleshoot?\n**A:** I'd check Spark UI to see if shuffle partitions increased, or if skew developed. I'd check recent data volume spikes. If caused by skew, I'd apply salting or broadcast joins. If due to small file explosion, I'd run compaction (`OPTIMIZE`). If it's cluster issue, I'd tune executors or restart with right sizing. Documentation of findings ensures root cause is understood.\n\n### S5. Scenario: Two sets of users need different partitioning (state-based vs product-based). How do you solve",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_9",
    "text": ", then rewrite them into PySpark DataFrame transformations in Databricks. I'd leverage Delta Lake features like ACID and time travel for consistency. To validate migration, I'd reconcile row counts, run sample KPI checks, and parallel-run old Synapse vs new Databricks for a cycle before cutover.\n\n### S4. Scenario: A job that usually runs in 30 minutes suddenly takes 2 hours. How do you troubleshoot?\n**A:** I'd check Spark UI to see if shuffle partitions increased, or if skew developed. I'd check recent data volume spikes. If caused by skew, I'd apply salting or broadcast joins. If due to small file explosion, I'd run compaction (`OPTIMIZE`). If it's cluster issue, I'd tune executors or restart with right sizing. Documentation of findings ensures root cause is understood.\n\n### S5. Scenario: Two sets of users need different partitioning (state-based vs product-based). How do you solve it?\n**A:** Instead of duplicating datasets, I'd partition primarily on date (common for both) and apply ZORDER indexing on state and product columns. This ensures pruning for both user groups. If certain queries are very heavy, I'd create summary tables (by state or product) in Gold for faster BI performance.\n\n### S6. Scenario: A new data source is onboarded. How do you make your pipeline flexible?\n**A:** I'd store ingestion configs in metadata tables (path, schema, delimiter, rules). ADF would read configs and trigger the generic ingestion pipeline. In Silver, reusable PySpark validation functions enforce schema and rules. Offshore team only needs to add metadata entries, no code changes. This accelerates onboarding.\n\n### S7. Scenario: Business complains KPIs don't match finance reports. What's your action plan?\n**A:** First, I'd meet with finance team to understand calculation logic. Then, I'd trace current KPI logic in Gold models. If discrepancy is due to business rule misalignment (e.g., revenue net of returns), I'd adjust logic and document it. If due to data lag, I'd reschedule refresh. Clear documentation + governance ensures future alignment.\n\n### S8. Scenario: Pipeline fails due to schema drift — new column added in source. What's your fix?\n**A:** Bronze ingests flexibly. In Silver, I'd add the new column with default or nulls, update schema enforcement, and test in dev. After validating downstream transformations, I'd push change to prod. I'd also update schema documentation and inform business about the new attribute.\n\n### S9. Scenario: Offshore reports jobs failed overnight, but you're onsite. How do you handle?\n**A:** I'd quickly check logs to identify root cause. If it's schema mismatch, I'd patch mapping. If volume spike, I'd rescale cluster. I'd communicate to business with ETA (\"Data delayed by 1 hour, fix in progress\"). Meanwhile, I'd guide offshore via call so they learn the resolution. This builds trust and team capability.\n\n### S10. Scenario: Data load is complete but dashboards are very slow. What's your fix?\n**A:** I'd review Gold models. If queries scan too many rows, I'd pre-aggregate summary tables. I'd compact files (`OPTIMIZE`) and ensure partitioning matches query filters. For Power BI, I'd use DirectQuery with RLS and reduce joins by denormalizing smaller dimensions.\n\n### S11. Scenario: You need to handle late-arriving sales transactions. How do you design this?\n**A:** I'd use Delta `merge` with watermarking. Late rows within X days are merged automatically. If outside retention, I'd reprocess specific partitions after business approval. For analytics, SCD ensures late facts still map to correct dimension versions.\n\n### S12. Scenario: Business requests PII masking for compliance. What's your approach?\n**A:** I'd configure Unity Catalog to mask sensitive columns like SSN or DOB. Only compliance teams would see full values. Analysts would see masked/nulls. Access logs would track queries. This balanced compliance and usability.\n\n### S13. Scenario: API source throttles requests, but you must ingest daily. How do you manage?\n**A:** In ADF, I'd configure pagination and retries. In Databricks, I'd implement rate-limiting logic with exponential backoff. If needed, I'd parallelize calls with controlled concurrency. Failures are logged and retried separately, so ingestion completes within SLA without breaching API limits.\n\n### S14. Scenario: You find too many small files in ADLS. How do you handle this?\n**A:** I'd batch ingestion in ADF to reduce small files. In Delta, I'd run weekly compaction jobs (`OPTIMIZE`) and ZORDER for query columns",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_10",
    "text": "masking for compliance. What's your approach?\n**A:** I'd configure Unity Catalog to mask sensitive columns like SSN or DOB. Only compliance teams would see full values. Analysts would see masked/nulls. Access logs would track queries. This balanced compliance and usability.\n\n### S13. Scenario: API source throttles requests, but you must ingest daily. How do you manage?\n**A:** In ADF, I'd configure pagination and retries. In Databricks, I'd implement rate-limiting logic with exponential backoff. If needed, I'd parallelize calls with controlled concurrency. Failures are logged and retried separately, so ingestion completes within SLA without breaching API limits.\n\n### S14. Scenario: You find too many small files in ADLS. How do you handle this?\n**A:** I'd batch ingestion in ADF to reduce small files. In Delta, I'd run weekly compaction jobs (`OPTIMIZE`) and ZORDER for query columns. This reduced file count, improved performance, and lowered metadata overhead.\n\n### S15. Scenario: A regulatory audit requires proof of lineage. How do you provide it?\n**A:** I'd use Purview/Unity Catalog lineage reports to show flow from source → Bronze → Silver → Gold → dashboard. This visual lineage, plus validation logs, provided end-to-end transparency. During audit, we demonstrated compliance with role-based access and PII masking.\n\n### S16. Scenario: Pipeline processing jumps from 1TB/day to 5TB/day. How do you scale?\n**A:** I'd scale clusters with more executors temporarily. Then, I'd review partitioning to ensure balanced distribution. I'd switch from full loads to incremental using modified_date. I'd also optimize joins and ZORDER on high-cardinality columns. This allowed scaling without uncontrolled cost.\n\n### S17. Scenario: Business wants new KPI \"avg sales per customer\" in dashboards. How do you deliver?\n**A:** I'd update Gold model by joining sales fact with customer dimension, compute metric in PySpark, and store in summary table. I'd validate numbers with business team, deploy changes via CI/CD, and update Power BI to expose the new KPI.\n\n### S18. Scenario: Data quality check flags 5% null store_id in Silver. What's your action?\n**A:** I'd first confirm if nulls are due to source issues. If yes, escalate to source team. Meanwhile, I'd assign default store \"Unknown\" for analysis continuity. I'd document rule and flag these records in data quality dashboard so business is aware.\n\n### S19. Scenario: Offshore wants to add a new pipeline but lacks guidance. How do you help?\n**A:** I'd ask them to define source and target configs in metadata. Then, I'd guide them on plugging configs into our reusable ingestion framework. I'd review their PySpark script and provide optimization tips. Over time, this made them independent and efficient.\n\n### S20. Scenario: Customer interview asks: 'What's your biggest achievement?'\n**A:** I'd highlight optimizing a 10TB sales pipeline from 2+ hours to 40 mins using partition tuning, salting, ZORDER, and compaction. This improvement ensured SLA compliance and cut costs by 25%. I'd also mention building validation dashboards for data quality, which gave business confidence in our platform.\n\n---\n\n## Key Takeaways\n\n- **Performance**: Proper partitioning, broadcast joins, and adaptive query execution are crucial\n- **Data Quality**: Implement comprehensive error handling and data validation\n- **Monitoring**: Log metrics and create dashboards for pipeline health\n- **Testing**: Unit tests and automated validation ensure pipeline reliability\n- **Code Organization**: Modular design improves maintainability and reusability\n- **Delta Lake**: Leverage time travel and merge capabilities for data management\n- **Governance**: Unity Catalog and Purview provide comprehensive data governance\n- **Collaboration**: Effective offshore team management and stakeholder communication",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "interview_examples_0",
    "text": "# Interview Response Examples\n\n## Example 1: Partitioning Strategy\n**Q: How do you pick partitioning for a 200M-row Delta table used by state and product queries?**\n\nI start from access patterns and row distribution. For mixed state/product queries, I usually partition by `txn_date` (ingest/write efficiency + pruning) and use **Z-ORDER on state, product_id** for read locality. If state is very uneven, I avoid partitioning by it to prevent small/huge-file imbalance; I rely on **Z-ORDER + AQE** and sometimes **salting** if a few states dominate joins. On the write path I **OPTIMIZE** weekly to compact files and keep pruning effective. I validate with Spark UI (shuffle read, tasks skew) and query explain to confirm pruning stats.\n\n## Example 2: Nested JSON Processing\n**Q: You get nested JSON from an API; how do you land it in Silver?**\n\nI infer or define a **StructType**, then normalize with `from_json`/`explode` into a flat schema we control. I keep raw JSON in Bronze for replay, and in Silver I enforce types, handle null defaults, and dedupe on `(business_key, event_ts)`. For evolution, I allow **additive** fields only and route breaking changes to a quarantine stream. Finally, I write Delta with merge-on keys and track schema drift in a small audit table.\n\n## Example 3: ADF Orchestration\n**Q: Orchestrating ADF → Databricks for incremental upserts—what's the flow?**\n\nADF pipeline passes a **watermark** (last modified) to a parameterized notebook. The notebook reads only changed rows, applies business rules, then runs a **Delta MERGE** keyed by the business ID plus `effective_ts`. I checkpoint the watermark in a control table, and failures roll back by not advancing it. I add a light set of row-count, duplicate, and null checks and publish metrics to a monitor table that feeds a Power BI ops dashboard.\n\n## Example 4: Performance Troubleshooting\n**Q: A long-running job regressed from 35 to 90 minutes—what's your first move?**\n\nI open the **Spark UI** and compare stage DAGs between good vs bad runs to locate the new hot stage. If it's a join, I check skew (shuffle read variance) and either **broadcast** the small side, **repartition** by the join key, or **salt** the heavy key. If it's file I/O, I check small-file counts and run **OPTIMIZE**/`ZORDER`; if input grew, I bump `spark.sql.shuffle.partitions` or enable **AQE**. I also diff config and datasource versions and add a regression alert on duration.\n\n## Example 5: Schema Evolution\n**Q: How do you handle schema changes in production pipelines?**\n\nI use a two-layer approach: Bronze accepts schema drift with permissive mode, Silver enforces strict schema. When new columns arrive, I validate them in dev first, then add with defaults in Silver after business approval. For Delta tables, I use `mergeSchema` in controlled deployments, never blindly. I also log schema changes to an audit table and update documentation. This keeps pipelines flexible but prevents downstream breakages.\n\n## Example 6: Data Quality Framework\n**Q: How do you implement data quality checks across your pipeline?**\n\nI built a reusable PySpark validation framework that runs checks like row counts, null ratios, duplicates, and business rules. Results go into a monitoring Delta table, and violations trigger ADF alerts. I surface this in Power BI dashboards so business can see data health in real-time. For critical tables, I also do referential integrity checks and flag orphaned records in a quarantine table for manual review.\n\n## Example 7: Cost Optimization\n**Q: How do you optimize Databricks costs while maintaining performance?**\n\nI use job clusters with auto-termination instead of always-on, right-size based on workload patterns, and enable auto-scaling. I compact small files weekly with `OPTIMIZE` and use ZORDER for query performance. For non-critical jobs, I use spot instances. I also monitor cluster utilization and tune executor memory to avoid over-provisioning. These changes typically cut costs by 25-30% while maintaining SLA performance.\n\n## Example 8: Multi-Source Integration\n**Q: How do you integrate data from different source systems with inconsistent schemas?**\n\nI standardize schemas in the Silver layer using conformance rules. I map different source column names to standard names, handle data type differences with explicit casting, and apply business rules for missing or invalid values. I build unified dimensions in Gold that combine data from multiple sources. For example, I created a single store dimension that maps different store codes from pharmacy",
    "metadata": {
      "tags": [
        "interview",
        "examples",
        "scenarios",
        "responses"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_examples.md",
      "file_name": "interview_examples.md"
    }
  },
  {
    "id": "interview_examples_1",
    "text": "Q: How do you optimize Databricks costs while maintaining performance?**\n\nI use job clusters with auto-termination instead of always-on, right-size based on workload patterns, and enable auto-scaling. I compact small files weekly with `OPTIMIZE` and use ZORDER for query performance. For non-critical jobs, I use spot instances. I also monitor cluster utilization and tune executor memory to avoid over-provisioning. These changes typically cut costs by 25-30% while maintaining SLA performance.\n\n## Example 8: Multi-Source Integration\n**Q: How do you integrate data from different source systems with inconsistent schemas?**\n\nI standardize schemas in the Silver layer using conformance rules. I map different source column names to standard names, handle data type differences with explicit casting, and apply business rules for missing or invalid values. I build unified dimensions in Gold that combine data from multiple sources. For example, I created a single store dimension that maps different store codes from pharmacy and sales systems to a standard format.",
    "metadata": {
      "tags": [
        "interview",
        "examples",
        "scenarios",
        "responses"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_examples.md",
      "file_name": "interview_examples.md"
    }
  }
]