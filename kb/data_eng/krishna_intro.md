---
tags: [krishna, walgreens, data-engineer, azure, databricks, pyspark, medallion-architecture, delta-lake, azure-data-factory, unity-catalog, performance-optimization]
---

# Krishna - Data Engineer Profile & Experience

## Professional Overview

Krishna is a Data Engineer with extensive experience at Walgreens, specializing in Azure and Databricks technologies. He has built and optimized large-scale data pipelines processing over 10TB of data monthly across multiple business domains.

## Current Role & Responsibilities

### **Primary Focus Areas**
- **Large-Scale Data Processing**: Handling 10TB+ monthly data volume
- **Modern Data Architecture**: Implementing medallion architecture (Bronze, Silver, Gold)
- **Cross-Domain Expertise**: Sales, customer loyalty, pharmacy, and supply chain data
- **Legacy Migration**: Transitioning from traditional systems to cloud-native solutions

### **Technical Leadership**
- **Subject Matter Expert**: Databricks and PySpark specialist
- **Team Mentorship**: Guiding offshore developers on best practices
- **Code Review**: Ensuring quality and performance standards
- **Best Practices**: Driving data engineering standards across teams

## Technical Expertise

### **Core Technologies**
- **Azure Data Factory (ADF)**: Orchestration and workflow management
- **Databricks**: Notebook development and cluster management
- **PySpark**: Large-scale data transformations and processing
- **Delta Lake**: ACID transactions, schema enforcement, time travel
- **Unity Catalog**: Data governance and access control
- **Azure DevOps**: CI/CD pipeline implementation

### **Data Architecture Patterns**
- **Medallion Architecture**: Bronze (raw), Silver (standardized), Gold (modeled)
- **Delta Lake Features**: Schema evolution, ACID compliance, time travel
- **Partitioning Strategies**: Date-based, state-based, product-based
- **Data Quality Frameworks**: Validation, monitoring, and alerting

## Project Experience

### **Walgreens Data Platform Migration**
**Scope**: End-to-end data engineering for enterprise migration
**Scale**: 10TB+ monthly processing across multiple business domains
**Architecture**: Modern medallion architecture implementation

#### **Bronze Layer (Raw Data Ingestion)**
- Ingested raw data from multiple sources
- Implemented schema-on-read for flexibility
- Handled various data formats and structures
- Set up automated ingestion pipelines

#### **Silver Layer (Standardized Data)**
- Applied schema enforcement and validation
- Implemented deduplication logic
- Data cleansing and standardization
- Quality checks and error handling

#### **Gold Layer (Business Models)**
- Built fact and dimension tables
- Created KPI and reporting models
- Optimized for business consumption
- Integrated with Power BI dashboards

## Performance Optimization Achievements

### **Significant Performance Improvements**
- **Runtime Reduction**: Cut job execution time from 2+ hours to under 40 minutes
- **Cost Optimization**: 30% reduction in processing costs
- **SLA Compliance**: Improved pipeline reliability and performance

### **Optimization Techniques**
- **Partitioning**: Strategic partitioning by date, state, and product
- **Skew Resolution**: Implemented salting for skewed data joins
- **Join Optimization**: Broadcast joins for small lookup tables
- **File Compaction**: Optimized Delta file sizes for better performance
- **Resource Tuning**: Executor memory and CPU optimization

## Data Governance & Security

### **Unity Catalog Implementation**
- **Role-Based Access Control**: Granular permissions management
- **PII Protection**: Data masking and tokenization
- **Lineage Tracking**: Complete data lineage for audits
- **Compliance**: Meeting regulatory requirements

### **Security Measures**
- **Data Masking**: Sensitive field protection (emails, customer IDs)
- **Tokenization**: Secure handling of PII data
- **Access Controls**: Business teams access only anonymized data
- **Audit Trails**: Complete tracking of data access and changes

## CI/CD & DevOps

### **Automated Deployment Pipeline**
- **Version Control**: Git-based notebook and pipeline management
- **Environment Management**: Automated deployments across dev, test, prod
- **Error Reduction**: Eliminated manual deployment errors
- **Repeatability**: Consistent deployment processes

### **Quality Assurance**
- **Automated Testing**: CI/CD integration with validation checks
- **Code Review**: Systematic review processes
- **Deployment Validation**: Automated testing in each environment

## Collaboration & Mentoring

### **Team Leadership**
- **Offshore Team Mentorship**: Guided international developers
- **Technical Training**: PySpark and ADF best practices
- **Code Reviews**: Ensuring quality and performance standards
- **Knowledge Sharing**: Documented processes and procedures

### **Stakeholder Engagement**
- **Business Translation**: Converting KPI requirements to data models
- **Cross-Team Collaboration**: Working with business and IT teams
- **Requirements Gathering**: Understanding business needs and priorities

## Monitoring & Data Quality

### **Data Quality Framework**
- **Validation Checks**: Automated data quality monitoring
- **Error Logging**: Comprehensive error tracking and reporting
- **Monitoring Tables**: Centralized quality metrics storage
- **Real-Time Dashboards**: Power BI integration for business visibility

### **Operational Excellence**
- **Pipeline Health**: Real-time monitoring of data pipeline status
- **Business Visibility**: Dashboards for business stakeholders
- **Proactive Alerting**: Early detection of data quality issues
- **Performance Tracking**: Continuous optimization monitoring

## Key Achievements & Impact

### **Technical Achievements**
- **Scale**: Successfully handling 10TB+ monthly data processing
- **Performance**: 75% reduction in job runtime (2+ hours to 40 minutes)
- **Cost**: 30% reduction in processing costs through optimization
- **Reliability**: Improved SLA compliance and system stability

### **Business Impact**
- **Modernization**: Successfully migrated legacy systems
- **Governance**: Implemented enterprise-grade data governance
- **Collaboration**: Enhanced cross-team productivity
- **Quality**: Improved data quality and business confidence

## Skills Summary

### **Technical Skills**
- **Big Data**: PySpark, Databricks, Delta Lake
- **Cloud**: Azure Data Factory, Azure DevOps
- **Governance**: Unity Catalog, data lineage, security
- **Performance**: Optimization, tuning, monitoring
- **DevOps**: CI/CD, version control, automation

### **Soft Skills**
- **Leadership**: Team mentoring and technical guidance
- **Communication**: Cross-functional collaboration
- **Problem Solving**: Complex technical challenges
- **Project Management**: End-to-end delivery responsibility

## Professional Philosophy

Krishna believes in building reliable, scalable, and maintainable data solutions that serve both technical and business needs. His approach combines hands-on technical expertise with strong collaboration skills to drive successful data platform implementations.

---

*This profile represents Krishna's comprehensive experience in modern data engineering, combining technical depth with business impact and team leadership.*
