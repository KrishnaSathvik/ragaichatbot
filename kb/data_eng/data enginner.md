# ðŸ“„ OG Resume â€“ Krishna Sathvik (v2)

---

## Summary

Data Engineer with 6+ yearsâ€™ experience evolving from backend development into advanced data engineering. Skilled in Databricks, PySpark, SQL, and Azure, with proven success delivering 10TB+ scale pipelines, medallion architectures, and governance frameworks. Recognized for performance tuning, cost optimization, and enabling analytics that improve decision-making across healthcare, retail, and finance domains.

---

## Experience

**Data Engineer | TCS (Walgreens, USA) | Feb 2022 â€“ Present**

Engineered Delta Lakehouse pipelines in Databricks + PySpark, processing 10TB+ monthly data across sales, pharmacy, and supply chain, enabling real-time analytics and reducing ad-hoc reporting by 32%

Designed medallion architecture flows (Bronze, Silver, Gold) with schema enforcement, deduplication, and fact/dimension modeling, improving KPI reporting accuracy 30%

Automated orchestration with ADF triggering Databricks notebooks, implementing dynamic parameterization and CI/CD via Azure DevOps to achieve zero-downtime deployments

Implemented Unity Catalog for role-based access, PII masking, and lineage tracking, ensuring compliance and secure access for regulated data and anonymized reporting

Optimized partitioning strategies, resolved skew with salting and broadcast joins, and compacted small Delta filesâ€”reducing critical job runtime from 2+ hours to under 40 minutes

**Analytics Engineer | CVS Health (USA) | Jan 2021 â€“ Jan 2022**

Built ingestion pipelines in ADF + Databricks integrating supply chain and sales data, producing audit-ready datasets powering KPI dashboards across 200+ retail sites

Modeled fact/dimension schemas in SQL + dbt, improving reconciliation agility and accuracy 20% while supporting finance and operations decision-making

Collaborated with stakeholders to define business rules for replenishment and billing, surfacing 20+ KPIs leveraged by leadership for operational strategy

Optimized Databricks jobs with incremental models and clustering, cutting compute costs 18% and improving pipeline performance 25%

**Data Science Intern | McKesson (USA) | May 2020 â€“ Dec 2020**

Automated ETL scripts in Python + SQL reducing ingestion latency 50%, accelerating delivery of compliance dashboards for executive monitoring

Built forecasting models aligning patient demand with supply capacity, preventing mismatches and driving \$20M+ savings in procurement

Produced insights from claims and sales data supporting compliance reviews and informing leadershipâ€™s cost-recovery initiatives

**Software Developer | Inditek Pioneer Solutions (India) | 2017 â€“ 2019**

Developed backend APIs and optimized SQL queries for ERP modules, improving system response 35% and strengthening transactional accuracy in client billing

Designed reporting modules surfacing missed payments and contract discrepancies, cutting manual reconciliation and improving transparency in logistics workflows

---

## Skills

- **Data Engineering & ELT:** Databricks, PySpark, dbt, Airflow, ADF
- **Cloud Platforms:** Azure (Synapse, Data Factory, DevOps), Snowflake, AWS (Glue, Lambda)
- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle
- **Analytics & BI:** Power BI, Tableau, KPI Dashboards
- **DevOps & Orchestration:** GitHub Actions, Azure DevOps, Docker, Kubernetes
- **Governance & Security:** Data Quality Validation, PII Masking, Unity Catalog

---

# ðŸ“‘ Resume Optimization Framework â€“ v2

## 1. Structure & Bullet Rules

**5-4-3-2 Rule**

- Walgreens â†’ 5 bullets | Present tense | Outcome + metrics | Core tech (Databricks, PySpark, dbt, Azure)
- CVS â†’ 4 bullets | Past tense | Analytics enablement focus + optimization | ADF + dbt + SQL
- McKesson â†’ 3 bullets | Past tense | Forecasting + audit outcomes | Python + SQL + models
- Inditek â†’ 2 bullets | Past tense | ERP/API outcomes | SQL + APIs | Efficiency gains

**Character Constraint Rule**

- Each bullet = **220â€“240 characters**
- Never <215, never >240

---

## 2. Domain Adaptation Layer

Adapt **vocabulary + emphasis** per JD:

- Healthcare/Compliance â†’ audit-ready datasets, regulatory workflows, compliance monitoring
- Retail/Audit â†’ financial integrity, recovery audits, supplier agreements, reconciliation
- Tech/Data Platforms â†’ streaming pipelines, real-time analytics, Kafka/Event Hubs, ML integration
- Finance/AI/ML â†’ PySpark models, AWS Glue, risk compliance, forecasting, ML deployment

---

## 3. Skills Optimization Layer

**Always keep 6 OG categories:**

1. Data Engineering & ELT
2. Cloud Platforms
3. Databases & Storage
4. Analytics & BI
5. DevOps & Orchestration
6. Governance & Security

**JD Alignment Examples:**

- **Azure-Heavy JD**

  - Data Engineering & ELT: Databricks, PySpark, ADF, dbt, Airflow
  - Cloud Platforms: Azure (Synapse, Data Factory, DevOps), Snowflake
  - Databases & Storage: SQL Server, Delta Lake, PostgreSQL
  - Analytics & BI: Power BI, Tableau
  - DevOps & Orchestration: Azure DevOps, GitHub Actions, Docker
  - Governance & Security: Unity Catalog, PII Masking, Data Quality Validation

- **AWS-Heavy JD**

  - Data Engineering & ELT: PySpark, dbt, Airflow, AWS Glue
  - Cloud Platforms: AWS (Glue, Lambda, MSK/Kafka, S3), Snowflake
  - Databases & Storage: PostgreSQL, SQL Server, Redshift
  - Analytics & BI: Tableau, Power BI
  - DevOps & Orchestration: GitHub Actions, Docker, Kubernetes
  - Governance & Security: Data Lineage, PII Masking, Compliance Frameworks

- **BI/Analytics-Heavy JD**

  - Data Engineering & ELT: SQL, dbt, ADF, Databricks (light emphasis)
  - Cloud Platforms: Azure (Synapse, Data Factory), AWS (basic S3/Glue)
  - Databases & Storage: SQL Server, Delta Lake
  - Analytics & BI: Power BI, Tableau, KPI Dashboards
  - DevOps & Orchestration: Azure DevOps, GitHub Actions
  - Governance & Security: Data Quality Validation, Anonymization, Lineage Tracking

---

## 4. Verb Rotation Rule

Rotate verbs to avoid repetition: **Engineer, Optimize, Automate, Deploy, Collaborate, Design, Implement, Develop, Create, Scale, Orchestrate, Modernize**

---

## 5. Metrics Bank Rule

Every bullet ties to outcome + metric:

- % improvements (22% cost reduction, 30% reliability boost)
- Scale (10TB+, 1.2B rows)
- Time savings (30+ hours weekly)
- Financial impact (\$20M+ savings)
- Adoption (200+ sites, 50+ pipelines)

---

# ðŸ“‹ JD â†’ Resume Adaptation Checklist

1. **Read JD closely** â†’ Highlight cloud platform (AWS vs Azure), pipeline type (batch vs streaming), domain (healthcare, retail, finance).
2. **Apply Domain Layer** â†’ Swap vocabulary to mirror JD (audit, compliance, ML, streaming, etc).
3. **Reorder Skills** â†’ Keep 6 categories but move JD-priority tools to the front.
4. **Rewrite Bullets** â†’ Use 5-4-3-2 rule, adjust tech names, keep impact + metrics fixed.
5. **Rotate Verbs** â†’ Swap starting verbs per bullet to keep flow strong.
6. **Check Character Count** â†’ Ensure 220â€“240 characters per bullet.
7. **Final Scan** â†’ ATS keywords match JD, measurable outcomes present, tense alignment correct.

---

# ðŸŽ¯ JD-Specific Summary Templates

**Azure-Heavy JD (Walgreens, CVS, Microsoft ecosystem)**\
Data Engineer with 6+ yearsâ€™ experience designing and scaling pipelines on Azure and Databricks. Skilled in PySpark, SQL, ADF, and medallion architecture with proven success optimizing costs, enforcing governance, and enabling real-time analytics at enterprise scale.

**AWS-Heavy JD (Moodyâ€™s, Possible Finance, startups)**\
Data Engineer with 6+ yearsâ€™ experience building ELT pipelines on AWS using Glue, Lambda, MSK/Kafka, and S3. Expertise in PySpark, dbt, and SQL with a strong record of optimizing performance, reducing costs, and delivering audit-ready data platforms across finance and retail.

**BI/Analytics-Heavy JD (Analytics Engineer / BI Dev focus)**\
Analytics Engineer with 6+ yearsâ€™ experience bridging data engineering and BI. Designed SQL + dbt models, KPI frameworks, and automated ETL pipelines powering Power BI/Tableau dashboards. Skilled in Databricks, Python, and governance frameworks to deliver accurate, business-ready insights.

---

