---
tags: [tejuu, experience, resume, analytics-engineer, data-engineer, azure, power-bi]
persona: analytics
---

# Tejuu's Analytics Engineer Experience & Background

## Professional Summary

Senior Analytics Engineer with 6+ years of experience building enterprise-scale data platforms and analytics solutions. Expert in Azure (Synapse, Data Factory, Databricks), PySpark, data migration, ETL/ELT pipelines, and medallion architecture. Successfully processed 15M+ transactions monthly, reduced pipeline runtime by 45%, and unlocked $2M+ in revenue opportunities. Passionate about building robust data systems and enabling data-driven decision making across global organizations.

## Professional Experience

### Senior Analytics Engineer — Central Bank of Missouri
**Duration:** Dec 2024 – Present

**Key Responsibilities and Achievements:**

At Central Bank of Missouri, I'm leading enterprise data modernization initiatives across banking operations. It's a regional bank with complex data requirements spanning customer data, transactions, risk management, and regulatory reporting. I'm responsible for building and optimizing data pipelines that process financial, customer, and operational data while ensuring compliance and data quality.

**What I've accomplished:**
- Migrated 12M+ legacy records from Excel/Access to Azure SQL/Power BI
- Improved reporting efficiency by 25% through automation
- Reduced pipeline runtime by 45% through optimization
- Uncovered $2M+ in revenue opportunities through data insights
- Supporting 1K+ global users with self-service analytics

**Data Migration & Modernization:**
So I led this massive data migration project where we had 12M+ records scattered across Excel spreadsheets, Access databases, and legacy systems. The challenge was that different departments were using different formats and there was no single source of truth. What I did was design a comprehensive migration strategy using Azure Data Factory and Power BI. I built automated pipelines that could handle different data formats, validate data quality, and transform everything into a unified Azure SQL database. This was a game-changer - we went from having data silos to having a centralized data warehouse.

**ETL/ELT Pipeline Development:**
I built these end-to-end data pipelines using Azure Data Factory + Azure Synapse, processing customer transactions, loan data, and risk metrics. What I did was implement a dimensional modeling approach with proper fact and dimension tables. This really helped us organize the data flow and ensure consistency across all our reporting. The business users were really happy because they could finally get consistent data across different reports.

**Performance Optimization:**
One of my biggest wins was optimizing these legacy reports that were taking 4+ hours to refresh. The stakeholders were getting frustrated because they couldn't get their daily reports on time. I rebuilt the entire data model with proper indexing, partitioning, and query optimization. The result was amazing - report refresh time reduced from 4 hours to 45 minutes. The executives were really happy with the improvement and could now make decisions faster.

**Self-Service Analytics Platform:**
I designed and implemented a self-service analytics platform using Power BI that allows business users to create their own reports and dashboards. This was really important because it reduced the dependency on IT for every small report request. I created standardized data models, trained users on Power BI, and built a governance framework. The result was that we reduced report development time by 60% and increased user satisfaction significantly.

**Data Quality & Governance:**
I implemented comprehensive data quality frameworks with automated validation checks, monitoring, and alerting. This was super important because in banking, data quality issues can lead to regulatory problems. I added data lineage tracking, automated data profiling, and exception reporting. This helped us catch data issues before they impact business decisions and regulatory reporting.

**Technologies Used:**
- Azure Synapse, Azure Data Factory, Databricks
- Power BI, DAX, Power Query
- SQL, Python, PySpark
- Azure SQL, Delta Lake
- Git for version control

### Data Engineer — Stryker
**Duration:** Jan 2022 – Dec 2024

**Key Responsibilities and Achievements:**

At Stryker, I led enterprise-scale data engineering initiatives across global operations, building and optimizing data platforms that processed 15M+ transactions monthly. I designed and implemented comprehensive data architectures using Azure Synapse, Data Factory, and Databricks to support 1,000+ users across multiple regions.

**Major Data Engineering Projects:**

**Enterprise Data Lake Migration:**
I directed the massive migration of global sales and service data from Oracle and Excel into Azure Synapse star schemas. I built end-to-end ETL pipelines using Azure Data Factory and Databricks, processing over 15 million transactions with comprehensive data validation. This enabled unified reporting across all regions and was a game-changer for the company's data strategy.

**PySpark & Databricks Data Processing:**
I developed sophisticated PySpark-based data processing pipelines in Databricks that handled complex transformations and data quality validation. I implemented advanced DataFrame operations, window functions, UDFs, and automated data quality checks. The pipelines included duplicate detection, business rule validation, and real-time data processing capabilities using Python and PySpark.

**Medallion Architecture Implementation:**
I engineered partitioned ELT pipelines with optimized orchestration using Databricks job clusters and Azure Data Factory. I implemented medallion architecture (Bronze/Silver/Gold layers) with Delta Lake format for ACID transactions and time travel capabilities. This architecture cut refresh runtimes by 45% and significantly lowered compute costs while scaling secure data access to over 1,000 field reps, executives, and business stakeholders worldwide.

**Data Quality & Governance:**
I developed comprehensive data lineage documentation and KPI guardrails that lowered discrepancies in quarterly executive scorecards and accelerated month-end close by days. I built automated monitoring and alerting systems that improved trust in CFO reporting packages globally.

**Business Impact & Collaboration:**
I collaborated with clinical and product teams to translate operational questions into measurable KPIs and drill-through dashboards. This work uncovered process inefficiencies and unlocked over $2 million in actionable revenue opportunities. I also mentored junior data engineers on PySpark best practices, data modeling, and performance optimization.

**Technologies Used:**
- **Cloud Platforms:** Azure Synapse, Azure Data Factory, Databricks, Azure SQL
- **Big Data:** PySpark, Delta Lake, Spark SQL, distributed computing
- **Programming:** Python, SQL, Scala (basic)
- **Data Engineering:** ETL/ELT pipelines, data lakes, data warehousing, medallion architecture
- **Databases:** Oracle, SQL Server, Azure SQL Database
- **DevOps:** Azure DevOps, Git, CI/CD, ARM templates

### Product Data Analyst — CVS Health
**Duration:** May 2020 – Jan 2022

**Key Responsibilities and Achievements:**

At CVS Health, I migrated pharmacy and inventory reporting from Access and Excel into Azure SQL with Power BI pipelines. I designed Python and SQL reconciliation scripts that validated over 10 million records and reduced manual reconciliation tasks by 40% consistently.

I also worked with PySpark in Databricks for processing large-scale pharmacy data and building data quality validation frameworks. I developed automated data profiling scripts using PySpark DataFrames and implemented incremental data processing patterns for real-time inventory tracking. This enabled faster data processing and improved data accuracy across nationwide pharmacy operations.

I tuned complex SQL transformations and Power Query refreshes to stabilize recurring dashboards. This saved 25-30 analyst hours monthly and improved the accuracy and reliability of operational reporting across nationwide teams.

I partnered with engineers to define semantic layer fields, KPI acceptance contracts, and validation scripts. This ensured smooth release cycles and reduced risks of downstream disruptions during production deployments.

I also piloted A/B reporting tied to operational initiatives and staffing levers. I translated findings into measurable throughput improvements that directly influenced roadmap prioritization and leadership decision making.

**Technologies Used:**
- Azure SQL, Power BI, Databricks
- Python, SQL, PySpark
- Power Query, DAX
- Access database migration
- Excel automation

### Data Analyst — Colruyt IT Group
**Duration:** May 2018 – Dec 2019

**Key Responsibilities and Achievements:**

At Colruyt IT Group, I consolidated fragmented Excel and Access reporting processes into standardized Power BI dashboards with reusable DAX templates. This reduced duplication and increased trust in weekly financial and merchandising performance metrics.

I designed SQL dimensional models to support finance and merchandising analyses. This accelerated month-end close cycles, provided interactive drilldowns, and replaced time-consuming manual spreadsheet-based efforts.

I captured requirements across departments, authored detailed user stories, and developed UAT scripts with standardized KPI definitions. This ensured successful deliveries aligned with governance rules and stakeholder expectations.

**Technologies Used:**
- Power BI, DAX, Power Query
- SQL Server
- Excel, Access
- Requirements gathering tools
- UAT documentation

## Technical Skills

### Programming & Data
- **SQL**: Expert level - Complex queries, window functions, CTEs, performance tuning
- **Python**: Proficient - Pandas, NumPy, data validation scripts, automation
- **PySpark**: Proficient - DataFrame operations, window functions, UDFs, data processing
- **R**: Intermediate - Statistical analysis, data visualization
- **Git**: Version control for code and documentation

### Analytics & BI Tools
- **Power BI**: Expert - DAX, Power Query, data modeling, RLS, performance optimization
- **Tableau**: Proficient - Dashboard design, calculated fields, parameters
- **Excel**: Advanced - Power Pivot, Power Query, VBA, complex formulas

### Cloud & Data Platforms
- **Azure**: Synapse, Data Factory, Databricks, Azure SQL, Power BI
- **Databricks**: PySpark, Delta Lake, job clusters, notebook development
- **AWS**: S3, Redshift, Glue, Athena, Lambda
- **Snowflake**: Data warehousing, query optimization
- **dbt**: Data transformation, testing, documentation

### Data & Business Skills
- **Data Engineering**: ETL/ELT pipelines, data migration, data warehousing
- **Data Architecture**: Medallion architecture, dimensional modeling, data lakes
- **Data Quality**: Validation frameworks, monitoring, governance
- **Performance Optimization**: Query tuning, indexing, partitioning
- **Cloud Platforms**: Azure, AWS, multi-cloud strategies
- **Analytics**: KPI definition, self-service analytics, reporting
- **Stakeholder Management**: Cross-functional collaboration, requirements gathering
- **Project Management**: Migration planning, risk management, delivery

## Education

**MS Data Science**
University of North Texas, Denton, TX

**B.Tech Information Technology**
JNTU, Hyderabad, India

## Certifications

- **Microsoft** — Azure Data Engineer Associate
- **Google** — Business Intelligence Professional
- **Scrum Alliance** — Certified Scrum Product Owner

## Key Achievements & Metrics

### Efficiency & Savings
- Saved 25-30 hours monthly through automation
- Cut manual reconciliation by 40%
- Reduced refresh runtime by 45%
- Reduced compute spend by 22%

### Scale & Volume
- Reconciled 12M+ legacy rows
- Built pipelines for 15M+ transactions
- Supported 1K+ field users
- Processed 10+ TB monthly

### Impact & Adoption
- Improved reporting efficiency by 25%
- Reduced ad-hoc requests by 30%
- Boosted dashboard adoption across regions
- Consolidated 30% of manual spreadsheets

### Financial Impact
- Uncovered $2M+ revenue opportunity
- Cut ad-hoc requests by 32%
- Improved quarterly close by multiple days
- Reduced operational costs significantly

### Accuracy & Quality
- Improved metric accuracy by 20%
- Reduced discrepancies by 30%
- Improved trust in KPIs across organization

## Interview Talking Points

### Problem-Solving Examples

**Challenge: Enterprise Data Lake Migration**
At Stryker, we had this massive challenge where global sales and service data was scattered across Oracle databases and Excel files across different regions. The data was inconsistent, reports were breaking, and we couldn't get a unified view of our business. What I did was design a comprehensive migration strategy to move everything to Azure Synapse with a medallion architecture. I built PySpark pipelines in Databricks to process over 15 million transactions, implemented data quality validation, and created automated reconciliation scripts. The result was that we enabled unified reporting across all regions and unlocked over $2 million in revenue opportunities.

**Challenge: Performance Optimization with PySpark**
At Stryker, we had these ELT pipelines that were taking 2-3 hours to process our global data, which was causing delays in reporting and frustrating users worldwide. I analyzed the execution plans, identified bottlenecks, and implemented partitioning strategies in Databricks. I also optimized the PySpark code using window functions, UDFs, and advanced DataFrame operations. I redesigned the orchestration in Azure Data Factory with job clusters. The result was that we cut the runtime by 45% - from 2-3 hours down to about an hour. This also reduced our compute costs significantly while supporting 1,000+ users globally.

**Challenge: Medallion Architecture Implementation**
At Stryker, we needed to implement a scalable data architecture that could handle our growing data volumes and support real-time analytics. I designed and implemented a medallion architecture (Bronze/Silver/Gold layers) using Delta Lake format for ACID transactions and time travel capabilities. I built PySpark pipelines that processed data through each layer with proper data quality checks and business rule validation. The result was that we could handle 10x more data volume, enabled real-time analytics, and improved data trust across the organization.

**Challenge: Stakeholder Alignment**
One challenge I faced at Central Bank was getting buy-in from different departments for the migration project. Finance wanted things one way, audit had different requirements, and risk had their own needs. What I did was organize workshops with each group, documented their requirements, and found common ground. I created a standardized KPI framework that met everyone's needs. The result was successful adoption across all departments and a 30% reduction in ad-hoc reporting requests.

### Technical Strengths

**Data Engineering & PySpark**
I'm really strong in data engineering with extensive PySpark and Databricks experience. At Stryker, I built enterprise-scale data processing pipelines that handled 15M+ transactions monthly. I know how to use advanced DataFrame operations, window functions, UDFs, and implement medallion architecture with Delta Lake. I've optimized pipelines that reduced runtime by 45% while supporting 1,000+ users globally.

**Cloud Data Platforms**
I'm expert in Azure data services - Synapse, Data Factory, Databricks, and Azure SQL. I know how to design scalable data architectures, implement ETL/ELT pipelines, and optimize for performance and cost. I've also worked with AWS services like S3, Redshift, and Glue for multi-cloud strategies.

**SQL & Data Modeling**
SQL is my bread and butter. I write complex queries daily, including window functions, CTEs, and performance-tuned joins. I'm experienced in dimensional modeling - both star and snowflake schemas. I know how to design data warehouses that are optimized for analytics and can handle enterprise-scale data volumes.

**Data Migration & Architecture**
I've led multiple large-scale data migration projects, including migrating 15M+ records from Oracle to Azure Synapse. I know how to plan migrations, validate data, handle edge cases, and ensure business continuity. I always write reconciliation scripts and implement proper data quality frameworks to make sure no data is lost and everything matches.

**Stakeholder Management**
I'm good at working with non-technical stakeholders. I can translate business requirements into technical specifications and explain technical concepts in business terms. I've worked with executives, finance teams, operations, and IT across multiple companies.

## Work Style & Approach

### My Philosophy
I believe in building things that last. When I design a dashboard or data model, I think about maintainability, scalability, and documentation. I don't just deliver a solution - I make sure the team can maintain it after I'm done.

### Collaboration
I work well in cross-functional teams. I've collaborated with data engineers, software developers, business analysts, and executives. I believe in clear communication, documentation, and knowledge sharing.

### Continuous Learning
I'm always learning new tools and techniques. I recently learned dbt and have been exploring more advanced Azure services. I believe in staying current with industry trends and best practices.

### Problem-Solving Approach
When I face a problem, I start by understanding the root cause. I gather requirements, analyze the data, and design a solution. I believe in iterative development - start with an MVP, get feedback, and improve. I also believe in testing thoroughly before production deployment.
