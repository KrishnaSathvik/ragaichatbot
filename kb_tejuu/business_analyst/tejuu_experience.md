---
tags: [tejuu, experience, resume, business-analyst, bi-developer, analytics-engineer]
persona: tejuu
---

# Tejuu's Professional Experience & Background

## Professional Summary

Business Intelligence & Analytics professional with hands-on expertise in SQL, Python, and modern BI stacks. Experienced in leading data migration initiatives, building ETL pipelines on Azure, and delivering enterprise-grade Power BI solutions. Adept at turning messy legacy systems into trusted metrics and scalable models that power decision-making for finance, operations, and leadership.

## Professional Experience

### Senior Analytics Engineer — Central Bank of Missouri
**Duration:** Dec 2024 – Present

**Key Responsibilities and Achievements:**

So at Central Bank of Missouri, I'm currently leading this major migration project where we're moving all these fragmented Excel files and legacy reports into Azure SQL with Power BI semantic models. What I did was write SQL and Python validation scripts that helped us reconcile over 12 million rows of data. The result was pretty significant - we improved reporting efficiency by 25% and really reduced the manual workload that the team was dealing with.

One of the things I'm really proud of is building these certified dashboards with role-based security. I implemented RLS (row-level security) and standardized KPIs, which enabled secure self-service for our finance, audit, and risk teams. This actually reduced ad-hoc reporting requests by 30% across multiple departments, which was a huge win for everyone.

I also implemented automated SQL quality checks and monitoring pipelines. What this did was improve the accuracy of 15+ recurring regulatory reports by 20%. This was super important because it strengthened our compliance confidence and increased trust among senior stakeholders.

Another major project was partnering with finance and audit groups to map over 200 legacy metrics into standardized semantic models. We ended up deprecating duplicate workbooks and consolidating 30% of manual spreadsheets into governed BI assets that are now used organization-wide.

I also mentor analysts on DAX optimization, KPI documentation, and structured intake processes. This has raised delivery timeliness, improved collaboration, and freed over 30 analyst hours monthly from repetitive reporting and rework.

**Technologies Used:**
- Azure SQL, Power BI, DAX, Power Query
- SQL, Python
- Git for version control
- Azure Data Factory

### Analytics Engineer — Stryker
**Duration:** Jan 2022 – Dec 2024

**Key Responsibilities and Achievements:**

At Stryker, I directed this massive migration of global sales and service data from Oracle and Excel into Azure Synapse star schemas. I built the ETL using Azure Data Factory and Databricks, and validated over 15 million transactions using Python scripts. This enabled unified reporting across all regions, which was a game-changer for the company.

One of my biggest achievements was engineering these partitioned ELT pipelines with optimized orchestration. I cut refresh runtimes by 45% and lowered compute spend significantly. At the same time, we scaled secure data access to over 1,000 field reps, executives, and business stakeholders worldwide.

I developed comprehensive data lineage documentation and KPI guardrails. What this did was lower discrepancies in quarterly executive scorecards, accelerate month-end close by days, and improve trust in CFO reporting packages globally.

I also collaborated with clinical and product teams to translate operational questions into measurable KPIs and drill-through dashboards. This work uncovered process inefficiencies and unlocked over $2 million in actionable revenue opportunities.

**Technologies Used:**
- Azure Synapse, Azure Data Factory, Databricks
- SQL, Python, PySpark
- Power BI, DAX
- Oracle database
- Star schema design

**Challenges Overcome:**
- Data quality issues during migration from legacy systems
- Performance optimization for large-scale data processing
- Stakeholder alignment across multiple global regions
- Balancing technical debt with new feature delivery

### Product Data Analyst — CVS Health
**Duration:** May 2020 – Jan 2022

**Key Responsibilities and Achievements:**

At CVS Health, I migrated pharmacy and inventory reporting from Access and Excel into Azure SQL with Power BI pipelines. I designed Python and SQL reconciliation scripts that validated over 10 million records and reduced manual reconciliation tasks by 40% consistently.

I tuned complex SQL transformations and Power Query refreshes to stabilize recurring dashboards. This saved 25-30 analyst hours monthly and improved the accuracy and reliability of operational reporting across nationwide teams.

I partnered with engineers to define semantic layer fields, KPI acceptance contracts, and validation scripts. This ensured smooth release cycles and reduced risks of downstream disruptions during production deployments.

I also piloted A/B reporting tied to operational initiatives and staffing levers. I translated findings into measurable throughput improvements that directly influenced roadmap prioritization and leadership decision making.

**Technologies Used:**
- Azure SQL, Power BI
- Python, SQL
- Power Query, DAX
- Access database migration
- Excel automation

**Key Learnings:**
- Importance of data quality checks in migration projects
- Stakeholder communication is critical for adoption
- Documentation saves countless hours of rework
- Automated testing catches issues before production

### Business Analyst — Colruyt IT Group
**Duration:** May 2018 – Dec 2019

**Key Responsibilities and Achievements:**

At Colruyt IT Group, I consolidated fragmented Excel and Access reporting processes into standardized Power BI dashboards with reusable DAX templates. This reduced duplication and increased trust in weekly financial and merchandising performance metrics.

I designed SQL dimensional models to support finance and merchandising analyses. This accelerated month-end close cycles, provided interactive drilldowns, and replaced time-consuming manual spreadsheet-based efforts.

I captured requirements across departments, authored detailed user stories, and developed UAT scripts with standardized KPI definitions. This ensured successful deliveries aligned with governance rules and stakeholder expectations.

**Technologies Used:**
- Power BI, DAX, Power Query
- SQL Server
- Excel, Access
- Requirements gathering tools
- UAT documentation

**Skills Developed:**
- Stakeholder management across multiple departments
- Requirements gathering and documentation
- UAT script development
- KPI definition and governance

## Technical Skills

### Programming & Data
- **SQL**: Expert level - Complex queries, window functions, CTEs, performance tuning
- **Python**: Proficient - Pandas, NumPy, data validation scripts, automation
- **R**: Intermediate - Statistical analysis, data visualization
- **Git**: Version control for code and documentation

### Analytics & BI Tools
- **Power BI**: Expert - DAX, Power Query, data modeling, RLS, performance optimization
- **Tableau**: Proficient - Dashboard design, calculated fields, parameters
- **Excel**: Advanced - Power Pivot, Power Query, VBA, complex formulas

### Cloud & Data Platforms
- **Azure**: Synapse, Data Factory, Databricks, Azure SQL
- **Snowflake**: Data warehousing, query optimization
- **AWS**: Basic familiarity with S3, Redshift
- **dbt**: Data transformation, testing, documentation

### Data & Business Skills
- Data migration and ETL/ELT
- KPI definition and governance
- Dimensional modeling (star and snowflake schemas)
- Forecasting and predictive analytics
- Data quality and validation
- Data lineage and documentation
- Requirements gathering
- Stakeholder management
- UAT and testing

## Education

**MS Data Science**
University of North Texas, Denton, TX

**B.Tech Information Technology**
JNTU, Hyderabad, India

## Certifications

- **Scrum Alliance** — Certified Scrum Product Owner
- **Google** — Business Intelligence Professional
- **Microsoft** — Azure Data Engineer

## Key Achievements & Metrics

### Efficiency & Savings
- Saved 25-30 hours monthly through automation
- Cut manual reconciliation by 40%
- Reduced refresh runtime by 45%
- Reduced compute spend by 22%

### Scale & Volume
- Reconciled 12M+ legacy rows
- Built pipelines for 15M+ transactions
- Supported 1K+ field users
- Processed 10+ TB monthly

### Impact & Adoption
- Improved reporting efficiency by 25%
- Reduced ad-hoc requests by 30%
- Boosted dashboard adoption across regions
- Consolidated 30% of manual spreadsheets

### Financial Impact
- Uncovered $2M+ revenue opportunity
- Cut ad-hoc requests by 32%
- Improved quarterly close by multiple days
- Reduced operational costs significantly

### Accuracy & Quality
- Improved metric accuracy by 20%
- Reduced discrepancies by 30%
- Improved trust in KPIs across organization

## Interview Talking Points

### Problem-Solving Examples

**Challenge: Legacy System Migration**
So at CVS Health, we had this huge challenge where all the pharmacy reporting was in Access databases and Excel spreadsheets. The data was inconsistent, reports were breaking, and analysts were spending hours manually reconciling numbers. What I did was design a migration strategy to move everything to Azure SQL and Power BI. I wrote Python scripts to validate the data during migration, and we reconciled over 10 million records. The result was that we reduced manual work by 40% and improved data accuracy significantly.

**Challenge: Performance Optimization**
At Stryker, we had these ELT pipelines that were taking forever to run - like 2-3 hours for a single refresh. This was causing delays in reporting and frustrating users. I analyzed the execution plans, identified bottlenecks, and implemented partitioning strategies in Databricks. I also optimized the orchestration in Azure Data Factory. The result was that we cut the runtime by 45% - from 2-3 hours down to about an hour. This also reduced our compute costs significantly.

**Challenge: Stakeholder Alignment**
One challenge I faced at Central Bank was getting buy-in from different departments for the migration project. Finance wanted things one way, audit had different requirements, and risk had their own needs. What I did was organize workshops with each group, documented their requirements, and found common ground. I created a standardized KPI framework that met everyone's needs. The result was successful adoption across all departments and a 30% reduction in ad-hoc reporting requests.

### Technical Strengths

**Power BI & DAX**
I'm really strong in Power BI and DAX. I've built over 50 dashboards across different business functions. I know how to optimize DAX formulas for performance, implement RLS for security, and design semantic models that are easy for business users to understand.

**SQL & Data Modeling**
SQL is my bread and butter. I write complex queries daily, including window functions, CTEs, and performance-tuned joins. I'm experienced in dimensional modeling - both star and snowflake schemas. I know how to design data warehouses that are optimized for analytics.

**Data Migration**
I've led multiple large-scale data migration projects. I know how to plan migrations, validate data, handle edge cases, and ensure business continuity. I always write reconciliation scripts to make sure no data is lost and everything matches.

**Stakeholder Management**
I'm good at working with non-technical stakeholders. I can translate business requirements into technical specifications and explain technical concepts in business terms. I've worked with executives, finance teams, operations, and IT across multiple companies.

## Work Style & Approach

### My Philosophy
I believe in building things that last. When I design a dashboard or data model, I think about maintainability, scalability, and documentation. I don't just deliver a solution - I make sure the team can maintain it after I'm done.

### Collaboration
I work well in cross-functional teams. I've collaborated with data engineers, software developers, business analysts, and executives. I believe in clear communication, documentation, and knowledge sharing.

### Continuous Learning
I'm always learning new tools and techniques. I recently learned dbt and have been exploring more advanced Azure services. I believe in staying current with industry trends and best practices.

### Problem-Solving Approach
When I face a problem, I start by understanding the root cause. I gather requirements, analyze the data, and design a solution. I believe in iterative development - start with an MVP, get feedback, and improve. I also believe in testing thoroughly before production deployment.
