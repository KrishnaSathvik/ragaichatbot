# ðŸ“„ Data Engineer Resume â€“ Sai Tejaswini Thumpala (Tejuu)

---

## Summary

Data Engineer with 6+ years' experience building enterprise-scale data pipelines and cloud-based data systems. Skilled in Python, PySpark, cloud platforms (Azure, AWS), and orchestration tools with proven success processing 15M+ transactions monthly and leading data engineering projects. Recognized for automation expertise, system integration, and establishing best practices for data teams.

---

## Experience

**Data Engineer | Central Bank of Missouri | Dec 2024 â€“ Present**
Built enterprise data pipelines using Python and PySpark, processing 12M+ legacy records and establishing comprehensive data processing framework with 25% efficiency improvement
Engineered cloud-based data warehouse using Azure Data Factory and Python automation, supporting 1K+ users and reducing data processing time by 60%
Optimized data pipeline performance by 60% through strategic partitioning and Python-based automation, cutting data refresh time from 4 hours to 45 minutes
Implemented comprehensive data governance framework with automated monitoring and Python scripts, achieving 99.8% data accuracy and ensuring regulatory compliance
Developed real-time ETL workflows using Azure Data Factory and Python automation, reducing manual data processing by 50% and enabling real-time data analytics

**Data Engineer | Stryker | Jan 2022 â€“ Dec 2024**
Built enterprise-scale PySpark pipelines processing 15M+ transactions monthly using Python and medallion architecture, enabling unified data processing across global operations
Migrated complex data systems from Oracle/Excel to Azure Synapse using Python automation and ADF orchestration, unlocking $2M+ in commercial value opportunities
Engineered optimized data pipelines with Python-based automation and job orchestration, supporting 1K+ field users while achieving 35% cost reduction
Developed comprehensive data lineage documentation and Python-based monitoring systems, accelerating month-end close and improving data team efficiency

**Product Data Analyst | CVS Health | May 2020 â€“ Jan 2022**
Built data processing systems using Python and PySpark for large-scale pharmacy data, implementing incremental processing patterns and improving data accuracy across nationwide operations
Developed ETL frameworks using Python automation and SQL for data warehouse solutions, migrating 10M+ pharmacy records and reducing manual reconciliation by 40%
Collaborated with cross-functional teams to translate business requirements into Python-based data solutions, saving 25-30 analyst hours monthly

**Data Analyst | Colruyt IT Group | May 2018 â€“ Dec 2019**
Designed SQL data models and Python automation scripts supporting finance and merchandising data processes, accelerating month-end close cycles
Consolidated fragmented reporting into standardized data warehouse solutions using Python automation, reducing duplication and increasing data processing efficiency

---

## Skills

Data Engineering & ETL: Python; PySpark; Data Pipelines; ETL Pipelines; Azure Data Factory; dbt
Cloud Platforms: Azure (Synapse, Data Factory, DevOps); AWS (S3, Glue, Lambda); Microsoft Fabric
Databases & Storage: SQL Server; Azure SQL; Delta Lake; Oracle; PostgreSQL
Programming & Automation: Python; SQL; PySpark; Data Processing; API Integration
DevOps & Orchestration: Azure DevOps; GitHub Actions; CI/CD; Data Lineage; Automation
Data Architecture: Data Warehousing; Data Modeling; System Integration; Best Practices

---

## Education

**MS Data Science**
University of North Texas, Denton, TX

**B.Tech Information Technology**
JNTU, Hyderabad, India

---

## Key Achievements

- **Data Processing Scale:** Processed 15M+ transactions monthly using Python and PySpark
- **Performance Optimization:** Reduced pipeline runtime by 45% through Python-based automation
- **Cost Efficiency:** Achieved 35% cost reduction through optimized data processing systems
- **Team Leadership:** Led data engineering projects and established best practices for data teams
- **System Integration:** Successfully integrated multiple data systems using Python and cloud platforms

---

# ðŸ“‘ JD Adaptation Notes

## Applied Framework Adaptations:

**Domain Layer Applied:**
- Emphasized "data pipelines," "automation," and "system integration"
- Used "Python and PySpark" as primary technologies
- Highlighted "cloud platforms" and "orchestration tools"
- Focused on "best practices" and "team leadership"

**Skills Reordered for Robert Half JD:**
- Moved Python, PySpark, Data Pipelines to front
- Emphasized cloud platforms and automation
- Added API Integration and System Integration
- Highlighted programming and automation skills

**Summary Template Used:**
- Technical-focused summary emphasizing Python, PySpark, and cloud platforms
- Highlighted automation expertise and team leadership

**Key JD Requirements Addressed:**
- âœ… Python and PySpark expertise (extensive experience)
- âœ… Data pipelines and data processing (core experience)
- âœ… Cloud experience (Azure, AWS)
- âœ… Orchestration tools (Azure Data Factory, automation)
- âœ… Project leadership and best practices (team leadership experience)
- âœ… Long-term employment (6+ years with consistent growth)

**ATS Keywords Included:**
- Data Engineering, data pipelines, Python, PySpark
- Cloud platforms, automation, orchestration
- System integration, API integration, best practices
- Data warehousing, ETL pipelines, data processing
