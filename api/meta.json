[
  {
    "text": "---\ntags: [data-engineer, data-warehousing, snowflake, dimensional-modeling, star-schema, scd]\npersona: de\n---\n\n# Data Warehousing - Krishna's Expertise\n\n## Introduction\n**Krishna's Warehousing Background:**\nI've designed and implemented data warehouse solutions at Walgreens and previous companies. From dimensional modeling to Snowflake optimization, here's what I've learned about building enterprise warehouses.\n\n## Dimensional Modeling\n\n### Star Schema Design\n**Krishna's Approach:**\nStar schema is my go-to for most analytics use cases - it's simple for users and performs well.\n\n**Example: Sales Data Warehouse**\n```sql\n-- Fact Table: fct_sales\nCREATE TABLE fct_sales (\n    sale_id BIGINT PRIMARY KEY,\n    date_key INT NOT NULL,              -- FK to dim_date\n    customer_key BIGINT NOT NULL,       -- FK to dim_customer\n    product_key BIGINT NOT NULL,        -- FK to dim_product\n    store_key BIGINT NOT NULL,          -- FK to dim_store\n    \n    -- Measures (additive)\n    quantity INT,\n ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 0
    },
    "id": "de_de_data_warehousing_0"
  },
  {
    "text": "       -- FK to dim_customer\n    product_key BIGINT NOT NULL,        -- FK to dim_product\n    store_key BIGINT NOT NULL,          -- FK to dim_store\n    \n    -- Measures (additive)\n    quantity INT,\n    unit_price DECIMAL(18,2),\n    discount_amount DECIMAL(18,2),\n    tax_amount DECIMAL(18,2),\n    total_amount DECIMAL(18,2),\n    cost_amount DECIMAL(18,2),\n    profit_amount DECIMAL(18,2),\n    \n    -- Audit columns\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Dimension Table: dim_customer\nCREATE TABLE dim_customer (\n    customer_key BIGINT PRIMARY KEY,    -- Surrogate key\n    customer_id VARCHAR(50) NOT NULL,   -- Natural key\n    \n    -- Attributes\n    customer_name VARCHAR(200),\n    customer_email VARCHAR(200),\n    customer_segment VARCHAR(50),\n    customer_tier VARCHAR(20),\n    \n    -- Address\n    street VARCHAR(200),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    zip_code VARCHAR(20),\n    country VARCHAR(50),\n    \n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 1
    },
    "id": "de_de_data_warehousing_1"
  },
  {
    "text": "ment VARCHAR(50),\n    customer_tier VARCHAR(20),\n    \n    -- Address\n    street VARCHAR(200),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    zip_code VARCHAR(20),\n    country VARCHAR(50),\n    \n    -- SCD Type 2 fields\n    effective_date DATE NOT NULL,\n    end_date DATE,\n    is_current BOOLEAN DEFAULT TRUE,\n    \n    -- Audit\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Dimension Table: dim_date\nCREATE TABLE dim_date (\n    date_key INT PRIMARY KEY,           -- YYYYMMDD format\n    date DATE NOT NULL,\n    \n    -- Day attributes\n    day_of_week INT,\n    day_name VARCHAR(10),\n    day_of_month INT,\n    day_of_year INT,\n    \n    -- Week attributes\n    week_of_year INT,\n    week_start_date DATE,\n    week_end_date DATE,\n    \n    -- Month attributes\n    month_number INT,\n    month_name VARCHAR(10),\n    month_abbr VARCHAR(3),\n    quarter INT,\n    \n    -- Year attributes\n    year INT,\n    fiscal_year INT,\n    fiscal_quarter INT,\n ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 2
    },
    "id": "de_de_data_warehousing_2"
  },
  {
    "text": " Month attributes\n    month_number INT,\n    month_name VARCHAR(10),\n    month_abbr VARCHAR(3),\n    quarter INT,\n    \n    -- Year attributes\n    year INT,\n    fiscal_year INT,\n    fiscal_quarter INT,\n    \n    -- Flags\n    is_weekend BOOLEAN,\n    is_holiday BOOLEAN,\n    holiday_name VARCHAR(100),\n    is_business_day BOOLEAN\n);\n```\n\n### Slowly Changing Dimensions (SCD)\n\n**SCD Type 1 (Overwrite):**\n```sql\n-- Update customer segment (no history)\nUPDATE dim_customer\nSET customer_segment = 'Gold',\n    updated_at = CURRENT_TIMESTAMP\nWHERE customer_id = 'CUST001';\n```\n\n**SCD Type 2 (Historical Tracking):**\n```python\n# Krishna's SCD Type 2 Implementation\ndef apply_scd_type2(new_df, target_table, natural_key, change_columns):\n    \"\"\"\n    Implement SCD Type 2 for dimension table\n    \"\"\"\n    from pyspark.sql import functions as F\n    from delta.tables import DeltaTable\n    \n    # Read current dimension\n    current_df = spark.table(target_table).filter(\"is_current = true\")\n    \n    # Find changed re",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 3
    },
    "id": "de_de_data_warehousing_3"
  },
  {
    "text": "rk.sql import functions as F\n    from delta.tables import DeltaTable\n    \n    # Read current dimension\n    current_df = spark.table(target_table).filter(\"is_current = true\")\n    \n    # Find changed records\n    changed_df = (\n        new_df.alias(\"new\")\n        .join(\n            current_df.alias(\"current\"),\n            natural_key,\n            \"inner\"\n        )\n        .where(\n            # Check if any change column differs\n            \" OR \".join([f\"new.{col} != current.{col}\" for col in change_columns])\n        )\n        .select(\"new.*\")\n    )\n    \n    # Step 1: Expire old records\n    delta_table = DeltaTable.forName(spark, target_table)\n    \n    (\n        delta_table.alias(\"target\")\n        .merge(\n            changed_df.alias(\"updates\"),\n            f\"target.{natural_key} = updates.{natural_key} AND target.is_current = true\"\n        )\n        .whenMatchedUpdate(\n            set={\n                \"end_date\": F.current_date(),\n                \"is_current\": F.lit(False),\n            ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 4
    },
    "id": "de_de_data_warehousing_4"
  },
  {
    "text": "atural_key} AND target.is_current = true\"\n        )\n        .whenMatchedUpdate(\n            set={\n                \"end_date\": F.current_date(),\n                \"is_current\": F.lit(False),\n                \"updated_at\": F.current_timestamp()\n            }\n        )\n        .execute()\n    )\n    \n    # Step 2: Insert new versions\n    new_versions = (\n        changed_df\n        .withColumn(\"effective_date\", F.current_date())\n        .withColumn(\"end_date\", F.lit(None).cast(\"date\"))\n        .withColumn(\"is_current\", F.lit(True))\n        .withColumn(\"created_at\", F.current_timestamp())\n        .withColumn(\"updated_at\", F.current_timestamp())\n    )\n    \n    new_versions.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n    \n    # Step 3: Insert truly new customers\n    new_customers = (\n        new_df.alias(\"new\")\n        .join(\n            current_df.alias(\"current\"),\n            natural_key,\n            \"left_anti\"\n        )\n        .withColumn(\"effective_date\", F.current_date())",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 5
    },
    "id": "de_de_data_warehousing_5"
  },
  {
    "text": "(\n        new_df.alias(\"new\")\n        .join(\n            current_df.alias(\"current\"),\n            natural_key,\n            \"left_anti\"\n        )\n        .withColumn(\"effective_date\", F.current_date())\n        .withColumn(\"end_date\", F.lit(None).cast(\"date\"))\n        .withColumn(\"is_current\", F.lit(True))\n    )\n    \n    new_customers.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n\n# Usage\napply_scd_type2(\n    new_df=df_new_customers,\n    target_table=\"dim_customer\",\n    natural_key=\"customer_id\",\n    change_columns=[\"customer_segment\", \"customer_tier\", \"city\", \"state\"]\n)\n```\n\n**SCD Type 3 (Limited History):**\n```sql\n-- Track previous value only\nALTER TABLE dim_customer\nADD COLUMN previous_segment VARCHAR(50),\nADD COLUMN segment_change_date DATE;\n\nUPDATE dim_customer\nSET previous_segment = customer_segment,\n    segment_change_date = CURRENT_DATE,\n    customer_segment = 'Platinum'\nWHERE customer_id = 'CUST001';\n```\n\n## Snowflake Platform\n\n### Snowflake Architecture\n**Krish",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 6
    },
    "id": "de_de_data_warehousing_6"
  },
  {
    "text": "ious_segment = customer_segment,\n    segment_change_date = CURRENT_DATE,\n    customer_segment = 'Platinum'\nWHERE customer_id = 'CUST001';\n```\n\n## Snowflake Platform\n\n### Snowflake Architecture\n**Krishna's Snowflake Setup:**\n\n**1. Virtual Warehouse Configuration:**\n```sql\n-- Create warehouses for different workloads\nCREATE WAREHOUSE etl_warehouse\n    WAREHOUSE_SIZE = 'LARGE'\n    AUTO_SUSPEND = 300          -- 5 minutes\n    AUTO_RESUME = TRUE\n    INITIALLY_SUSPENDED = TRUE\n    COMMENT = 'ETL and data loading';\n\nCREATE WAREHOUSE analytics_warehouse\n    WAREHOUSE_SIZE = 'MEDIUM'\n    AUTO_SUSPEND = 120          -- 2 minutes\n    AUTO_RESUME = TRUE\n    MIN_CLUSTER_COUNT = 1\n    MAX_CLUSTER_COUNT = 3\n    SCALING_POLICY = 'STANDARD'\n    COMMENT = 'Ad-hoc analytics queries';\n\nCREATE WAREHOUSE reporting_warehouse\n    WAREHOUSE_SIZE = 'SMALL'\n    AUTO_SUSPEND = 60\n    AUTO_RESUME = TRUE\n    COMMENT = 'Scheduled reports and dashboards';\n```\n\n**2. Database & Schema Structure:**\n```sql\n-- Create data",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 7
    },
    "id": "de_de_data_warehousing_7"
  },
  {
    "text": "ing_warehouse\n    WAREHOUSE_SIZE = 'SMALL'\n    AUTO_SUSPEND = 60\n    AUTO_RESUME = TRUE\n    COMMENT = 'Scheduled reports and dashboards';\n```\n\n**2. Database & Schema Structure:**\n```sql\n-- Create database hierarchy\nCREATE DATABASE walgreens_dwh;\n\nUSE DATABASE walgreens_dwh;\n\nCREATE SCHEMA raw COMMENT = 'Raw landing data';\nCREATE SCHEMA staging COMMENT = 'Staging transformations';\nCREATE SCHEMA dwh COMMENT = 'Production data warehouse';\nCREATE SCHEMA analytics COMMENT = 'Analytics views and aggregations';\n```\n\n### Snowflake Load Patterns\n\n**1. COPY INTO (Bulk Load):**\n```sql\n-- Load from S3/Azure Blob\nCOPY INTO raw.orders\nFROM @raw_stage/orders/2024-01-15/\nFILE_FORMAT = (\n    TYPE = 'CSV'\n    FIELD_DELIMITER = ','\n    SKIP_HEADER = 1\n    NULL_IF = ('NULL', 'null', '')\n    EMPTY_FIELD_AS_NULL = TRUE\n)\nON_ERROR = 'CONTINUE'\nPURGE = TRUE;  -- Delete files after successful load\n```\n\n**2. Snowpipe (Continuous Loading):**\n```sql\n-- Create pipe for auto-ingestion\nCREATE PIPE raw.orders_pipe\n  ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 8
    },
    "id": "de_de_data_warehousing_8"
  },
  {
    "text": "L = TRUE\n)\nON_ERROR = 'CONTINUE'\nPURGE = TRUE;  -- Delete files after successful load\n```\n\n**2. Snowpipe (Continuous Loading):**\n```sql\n-- Create pipe for auto-ingestion\nCREATE PIPE raw.orders_pipe\n    AUTO_INGEST = TRUE\n    AS\n    COPY INTO raw.orders\n    FROM @raw_stage/orders/\n    FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1);\n\n-- Show pipe status\nSELECT SYSTEM$PIPE_STATUS('raw.orders_pipe');\n```\n\n**3. External Tables:**\n```sql\n-- Query data without loading\nCREATE EXTERNAL TABLE raw.ext_orders\nWITH LOCATION = @raw_stage/orders/\nFILE_FORMAT = (TYPE = 'PARQUET')\nAUTO_REFRESH = TRUE;\n\n-- Query like a normal table\nSELECT * FROM raw.ext_orders WHERE order_date = '2024-01-15';\n```\n\n### Snowflake Performance Optimization\n\n**1. Clustering Keys:**\n```sql\n-- Add clustering for frequently filtered columns\nALTER TABLE fct_sales\nCLUSTER BY (order_date, customer_id);\n\n-- Check clustering quality\nSELECT SYSTEM$CLUSTERING_INFORMATION('fct_sales');\n\n-- Reclustering (automatic by default)\nALTER TABLE ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 9
    },
    "id": "de_de_data_warehousing_9"
  },
  {
    "text": "columns\nALTER TABLE fct_sales\nCLUSTER BY (order_date, customer_id);\n\n-- Check clustering quality\nSELECT SYSTEM$CLUSTERING_INFORMATION('fct_sales');\n\n-- Reclustering (automatic by default)\nALTER TABLE fct_sales RESUME RECLUSTER;\n```\n\n**2. Materialized Views:**\n```sql\n-- Pre-aggregate for faster queries\nCREATE MATERIALIZED VIEW mv_daily_sales AS\nSELECT\n    order_date,\n    customer_segment,\n    product_category,\n    COUNT(*) as order_count,\n    SUM(total_amount) as total_sales,\n    AVG(total_amount) as avg_order_value\nFROM fct_sales s\nJOIN dim_customer c ON s.customer_key = c.customer_key\nJOIN dim_product p ON s.product_key = p.product_key\nWHERE c.is_current = TRUE\nGROUP BY order_date, customer_segment, product_category;\n\n-- Queries use materialized view automatically\n```\n\n**3. Search Optimization:**\n```sql\n-- Optimize for point lookups\nALTER TABLE dim_customer\nADD SEARCH OPTIMIZATION ON EQUALITY(customer_id, customer_email);\n\n-- Check optimization status\nSELECT * FROM TABLE(SYSTEM$SEARCH",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 10
    },
    "id": "de_de_data_warehousing_10"
  },
  {
    "text": "ation:**\n```sql\n-- Optimize for point lookups\nALTER TABLE dim_customer\nADD SEARCH OPTIMIZATION ON EQUALITY(customer_id, customer_email);\n\n-- Check optimization status\nSELECT * FROM TABLE(SYSTEM$SEARCH_OPTIMIZATION_HISTORY('dim_customer'));\n```\n\n### Snowflake Cost Optimization\n\n**Krishna's Cost-Saving Strategies:**\n\n**1. Warehouse Auto-Suspend:**\n```sql\n-- Aggressive auto-suspend for development\nALTER WAREHOUSE dev_warehouse SET AUTO_SUSPEND = 60;  -- 1 minute\n\n-- Moderate for production\nALTER WAREHOUSE prod_warehouse SET AUTO_SUSPEND = 300;  -- 5 minutes\n```\n\n**2. Result Caching:**\n```sql\n-- Queries return cached results (24 hours) - FREE!\n-- No code change needed, automatic\n\n-- Check if query used cache\nSELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY())\nWHERE QUERY_ID = 'xxx'\n  AND BYTES_SCANNED = 0;  -- Indicates cache hit\n```\n\n**3. Query Optimization:**\n```sql\n-- BAD: Full table scan\nSELECT * FROM fct_sales\nWHERE YEAR(order_date) = 2024;\n\n-- GOOD: Uses clustering/partitioning\nS",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 11
    },
    "id": "de_de_data_warehousing_11"
  },
  {
    "text": "YTES_SCANNED = 0;  -- Indicates cache hit\n```\n\n**3. Query Optimization:**\n```sql\n-- BAD: Full table scan\nSELECT * FROM fct_sales\nWHERE YEAR(order_date) = 2024;\n\n-- GOOD: Uses clustering/partitioning\nSELECT * FROM fct_sales\nWHERE order_date >= '2024-01-01'\n  AND order_date < '2025-01-01';\n\n-- BETTER: Limit columns\nSELECT order_id, customer_id, total_amount\nFROM fct_sales\nWHERE order_date >= '2024-01-01'\n  AND order_date < '2025-01-01';\n```\n\n**4. Table Type Selection:**\n```sql\n-- Transient tables (no fail-safe, cheaper)\nCREATE TRANSIENT TABLE staging.temp_data AS\nSELECT * FROM raw.orders;\n\n-- Temporary tables (session-scoped, cheapest)\nCREATE TEMPORARY TABLE temp_calc AS\nSELECT customer_id, SUM(total_amount) as total\nFROM fct_sales\nGROUP BY customer_id;\n```\n\n## Data Warehouse Best Practices\n\n### Surrogate Keys\n**Krishna's Key Generation:**\n```sql\n-- Generate surrogate keys with sequence\nCREATE SEQUENCE customer_key_seq START = 1 INCREMENT = 1;\n\nINSERT INTO dim_customer (\n    customer_key",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 12
    },
    "id": "de_de_data_warehousing_12"
  },
  {
    "text": "\n### Surrogate Keys\n**Krishna's Key Generation:**\n```sql\n-- Generate surrogate keys with sequence\nCREATE SEQUENCE customer_key_seq START = 1 INCREMENT = 1;\n\nINSERT INTO dim_customer (\n    customer_key,\n    customer_id,\n    customer_name,\n    effective_date,\n    is_current\n)\nSELECT\n    customer_key_seq.NEXTVAL,\n    customer_id,\n    customer_name,\n    CURRENT_DATE,\n    TRUE\nFROM staging.stg_customers;\n```\n\n### Fact Table Granularity\n**Krishna's Granularity Decisions:**\n\n**Transaction Grain (Most Detailed):**\n```sql\n-- One row per order line item\nCREATE TABLE fct_order_lines (\n    order_line_id BIGINT PRIMARY KEY,\n    order_id VARCHAR(50),\n    date_key INT,\n    customer_key BIGINT,\n    product_key BIGINT,\n    quantity INT,\n    line_amount DECIMAL(18,2)\n);\n```\n\n**Daily Grain (Aggregated):**\n```sql\n-- One row per customer per product per day\nCREATE TABLE fct_daily_sales (\n    date_key INT,\n    customer_key BIGINT,\n    product_key BIGINT,\n    order_count INT,\n    total_quantity INT,\n    tota",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 13
    },
    "id": "de_de_data_warehousing_13"
  },
  {
    "text": "`sql\n-- One row per customer per product per day\nCREATE TABLE fct_daily_sales (\n    date_key INT,\n    customer_key BIGINT,\n    product_key BIGINT,\n    order_count INT,\n    total_quantity INT,\n    total_amount DECIMAL(18,2),\n    PRIMARY KEY (date_key, customer_key, product_key)\n);\n```\n\n### Conformed Dimensions\n**Krishna's Shared Dimensions:**\n```sql\n-- dim_date used by ALL fact tables\n-- dim_customer used by sales, service, marketing\n-- dim_product used by sales, inventory, procurement\n\n-- This enables cross-functional analysis:\nSELECT\n    d.month_name,\n    SUM(s.total_amount) as sales_revenue,\n    SUM(i.inventory_value) as avg_inventory,\n    SUM(s.total_amount) / NULLIF(SUM(i.inventory_value), 0) as inventory_turnover\nFROM fct_sales s\nFULL OUTER JOIN fct_inventory i \n    ON s.date_key = i.date_key \n    AND s.product_key = i.product_key\nJOIN dim_date d ON COALESCE(s.date_key, i.date_key) = d.date_key\nWHERE d.year = 2024\nGROUP BY d.month_name;\n```\n\n## ETL to Data Warehouse\n\n### Increment",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 14
    },
    "id": "de_de_data_warehousing_14"
  },
  {
    "text": "date_key \n    AND s.product_key = i.product_key\nJOIN dim_date d ON COALESCE(s.date_key, i.date_key) = d.date_key\nWHERE d.year = 2024\nGROUP BY d.month_name;\n```\n\n## ETL to Data Warehouse\n\n### Incremental Load Pattern\n**Krishna's Production Pattern:**\n```python\ndef incremental_load_dimension(source_table, target_table, natural_key):\n    \"\"\"\n    Incremental load with upsert\n    \"\"\"\n    from pyspark.sql import functions as F\n    \n    # Read incremental data\n    df_new = spark.table(f\"staging.{source_table}\")\n    \n    # Read current dimension\n    df_current = spark.table(f\"dwh.{target_table}\").filter(\"is_current = true\")\n    \n    # Identify new, changed, and unchanged records\n    df_joined = df_new.alias(\"new\").join(\n        df_current.alias(\"current\"),\n        natural_key,\n        \"left\"\n    )\n    \n    # New records (insert)\n    df_inserts = (\n        df_joined\n        .where(\"current.customer_key IS NULL\")\n        .select(\"new.*\")\n        .withColumn(\"customer_key\", F.expr(\"uuid()\"))\n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 15
    },
    "id": "de_de_data_warehousing_15"
  },
  {
    "text": "\n    \n    # New records (insert)\n    df_inserts = (\n        df_joined\n        .where(\"current.customer_key IS NULL\")\n        .select(\"new.*\")\n        .withColumn(\"customer_key\", F.expr(\"uuid()\"))\n        .withColumn(\"effective_date\", F.current_date())\n        .withColumn(\"is_current\", F.lit(True))\n    )\n    \n    # Changed records (SCD Type 2)\n    df_updates = (\n        df_joined\n        .where(\n            \"current.customer_key IS NOT NULL AND \"\n            \"(new.customer_segment != current.customer_segment OR \"\n            \" new.city != current.city)\"\n        )\n        .select(\"new.*\", \"current.customer_key\")\n    )\n    \n    # Apply updates\n    apply_scd_type2(df_updates, target_table, natural_key, [\"customer_segment\", \"city\"])\n    \n    # Insert new records\n    df_inserts.write.mode(\"append\").saveAsTable(f\"dwh.{target_table}\")\n```\n\n### Data Quality in DWH\n**Krishna's Quality Framework:**\n```sql\n-- Quality checks before loading to warehouse\nCREATE OR REPLACE PROCEDURE validate_data_qual",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 16
    },
    "id": "de_de_data_warehousing_16"
  },
  {
    "text": "d\").saveAsTable(f\"dwh.{target_table}\")\n```\n\n### Data Quality in DWH\n**Krishna's Quality Framework:**\n```sql\n-- Quality checks before loading to warehouse\nCREATE OR REPLACE PROCEDURE validate_data_quality(table_name VARCHAR)\nRETURNS VARCHAR\nLANGUAGE SQL\nAS\n$$\nBEGIN\n    -- Check 1: No duplicates on natural key\n    LET dup_count INT := (\n        SELECT COUNT(*) FROM (\n            SELECT customer_id, COUNT(*)\n            FROM staging.stg_customers\n            GROUP BY customer_id\n            HAVING COUNT(*) > 1\n        )\n    );\n    \n    IF (dup_count > 0) THEN\n        RETURN 'FAILED: Found ' || dup_count || ' duplicate customer_ids';\n    END IF;\n    \n    -- Check 2: Required fields not null\n    LET null_count INT := (\n        SELECT COUNT(*)\n        FROM staging.stg_customers\n        WHERE customer_id IS NULL OR customer_name IS NULL\n    );\n    \n    IF (null_count > 0) THEN\n        RETURN 'FAILED: Found ' || null_count || ' records with null required fields';\n    END IF;\n    \n    -- Check ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 17
    },
    "id": "de_de_data_warehousing_17"
  },
  {
    "text": "tomer_id IS NULL OR customer_name IS NULL\n    );\n    \n    IF (null_count > 0) THEN\n        RETURN 'FAILED: Found ' || null_count || ' records with null required fields';\n    END IF;\n    \n    -- Check 3: Valid values\n    LET invalid_segment INT := (\n        SELECT COUNT(*)\n        FROM staging.stg_customers\n        WHERE customer_segment NOT IN ('Bronze', 'Silver', 'Gold', 'Platinum')\n    );\n    \n    IF (invalid_segment > 0) THEN\n        RETURN 'FAILED: Found ' || invalid_segment || ' invalid customer segments';\n    END IF;\n    \n    RETURN 'PASSED';\nEND;\n$$;\n\n-- Run quality checks\nCALL validate_data_quality('stg_customers');\n```\n\nThese warehousing patterns ensure our analytics are reliable, performant, and scalable at Walgreens!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_warehousing.md",
      "file_name": "de_data_warehousing.md",
      "chunk_index": 18
    },
    "id": "de_de_data_warehousing_18"
  },
  {
    "text": "---\ntags: [data-engineer, interview, questions, answers, behavioral, technical, star-method, pyspark, databricks, azure, comprehensive]\npersona: de\n---\n\n# Comprehensive Data Engineering Interview Guide - Krishna's 120+ Q&A\n\n## Introduction\n**Krishna's Interview Preparation:**\nThis comprehensive guide contains 120+ real interview questions covering PySpark, Databricks, Azure Data Factory, Delta Lake, SQL, Python, data governance, and behavioral questions. All answers are based on my real experience at Walgreens with specific examples, metrics, and STAR method format.\n\n\n## Section A: Core PySpark + Databricks (20 Questions)\n\n### Q1. What is your experience with PySpark in your current project?\n**A:** In my Walgreens project, I built end-to-end PySpark pipelines in Databricks that processed over 10TB monthly. I handled ingestion of raw data into Bronze, applied transformations and schema validation in Silver, and modeled fact/dimension tables in Gold for reporting. I frequently used PySpa",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 19
    },
    "id": "de_de_interview_guide_19"
  },
  {
    "text": "ver 10TB monthly. I handled ingestion of raw data into Bronze, applied transformations and schema validation in Silver, and modeled fact/dimension tables in Gold for reporting. I frequently used PySpark DataFrame APIs, window functions, UDFs, and joins. For example, I implemented deduplication logic using `dropDuplicates()` and window functions, ensuring the most recent transaction record is retained.\n\n### Q2. How do you handle large datasets in PySpark efficiently?\n**A:** First, I design partitioning strategies at ingestion (date-based partitions). Second, I tune Spark configurations like `spark.sql.shuffle.partitions` and executor memory. Third, I use optimizations such as broadcast joins for small lookup tables and salting for skewed keys. For large jobs, I enable Adaptive Query Execution. In one case, these changes reduced a pipeline runtime from 2+ hours to 40 minutes.\n\n### Q3. How do you debug PySpark jobs when they fail?\n**A:** I start with Spark UI to analyze DAGs, stages, and ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 20
    },
    "id": "de_de_interview_guide_20"
  },
  {
    "text": "on. In one case, these changes reduced a pipeline runtime from 2+ hours to 40 minutes.\n\n### Q3. How do you debug PySpark jobs when they fail?\n**A:** I start with Spark UI to analyze DAGs, stages, and tasks. I check for skew (few tasks taking much longer) or memory errors. Then I check logs in Databricks for stack traces. I usually replay the job with a smaller dataset to isolate the failing transformation. For example, when a job failed due to a corrupt record, I added `_corrupt_record` handling and moved bad rows into a quarantine table.\n\n### Q4. How do you handle nested JSON in PySpark?\n**A:** I define a StructType schema, use `from_json` to parse the string column, and `explode` for arrays. Then I flatten with `withColumn` to extract nested attributes. For deeply nested JSON, I use recursive functions to normalize. Finally, I store it as a clean Delta table in Silver for downstream consumption.\n\n### Q5. How do you handle duplicates in PySpark?\n**A:** For simple duplicates, I use `dr",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 21
    },
    "id": "de_de_interview_guide_21"
  },
  {
    "text": "rsive functions to normalize. Finally, I store it as a clean Delta table in Silver for downstream consumption.\n\n### Q5. How do you handle duplicates in PySpark?\n**A:** For simple duplicates, I use `dropDuplicates()`. For business-specific deduplication, I use window functions with `row_number()` ordered by timestamp, keeping only the latest row. In Walgreens, this was used for handling multiple prescription updates from pharmacy systems.\n\n### Q6. How do you handle schema drift in PySpark pipelines?\n**A:** In Bronze, I ingest with permissive mode to avoid job failure. In Silver, I enforce strict schema. If new columns appear, I add them with defaults in Silver after business validation. For Delta tables, I use `mergeSchema` in controlled deployments, never blindly. This allows flexibility but avoids breaking downstream queries.\n\n### Q7. How do you design incremental loads in PySpark?\n**A:** I use watermarking (modified_date column) or surrogate keys. ADF passes parameters to notebooks f",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 22
    },
    "id": "de_de_interview_guide_22"
  },
  {
    "text": "ut avoids breaking downstream queries.\n\n### Q7. How do you design incremental loads in PySpark?\n**A:** I use watermarking (modified_date column) or surrogate keys. ADF passes parameters to notebooks for last processed date. In PySpark, I filter only incremental rows and apply Delta `merge` to update/inserts. This reduced daily runs from processing 10TB full to ~300GB delta, saving cost and runtime.\n\n### Q8. Can you explain time travel in Delta Lake? How have you used it?\n**A:** Time travel lets me query data at a specific version or timestamp. At Walgreens, one job overwrote 2 days of data. Using `VERSION AS OF` in Delta, I restored the table to its previous state in minutes without reloading raw files.\n\n### Q9. How do you handle slowly changing dimensions (SCD) in Databricks?\n**A:** I used Delta `merge` for Type 2. Old record is closed with an end_date, new record inserted with active_flag = 1. This keeps historical changes. For example, when product pricing changed, our dimension tab",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 23
    },
    "id": "de_de_interview_guide_23"
  },
  {
    "text": "d Delta `merge` for Type 2. Old record is closed with an end_date, new record inserted with active_flag = 1. This keeps historical changes. For example, when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use `PERMISSIVE` mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for man",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 24
    },
    "id": "de_de_interview_guide_24"
  },
  {
    "text": "12. How do you handle corrupt records in ingestion?\n**A:** I use `PERMISSIVE` mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.\n\n### Q13. How do you test PySpark pipelines?\n**A:** Unit tests validate transformations with small sample data. Row count reconciliation checks ingestion completeness. Schema validation checks enforce consistency. We automated these checks in CI/CD pipelines.\n\n### Q14. How do you manage dependencies across notebooks in Databricks?\n**A:** I modularize common logic (like validations, schema enforcement) in utility notebooks or .py files stored in repos. Then I import them into main notebooks. This avoids code duplication and keeps pipelines maintainable.\n\n### Q15. How do you handle late arriving data?\n**A:** I use watermarking in Delta tables, so late data is still merged if within X days. If outside retention, we load them manually after bu",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 25
    },
    "id": "de_de_interview_guide_25"
  },
  {
    "text": "tainable.\n\n### Q15. How do you handle late arriving data?\n**A:** I use watermarking in Delta tables, so late data is still merged if within X days. If outside retention, we load them manually after business approval.\n\n### Q16. How do you handle large joins across multiple datasets?\n**A:** First, partition both datasets on the join key. If one is small, broadcast it. If skew occurs, apply salting. If still heavy, break into smaller joins and cache intermediate results.\n\n### Q17. How do you manage PySpark code for reusability?\n**A:** I follow modular design: separate ingestion, transformation, validation, and load functions. I store configs in parameter files, not hardcoded. Reusable frameworks allowed offshore to easily plug in new sources with minimal code.\n\n### Q18. How do you optimize PySpark DataFrame transformations?\n**A:** Avoid wide transformations until necessary, use `select` instead of `*`, cache intermediate results when reused, and avoid UDFs unless unavoidable. Vectorized o",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 26
    },
    "id": "de_de_interview_guide_26"
  },
  {
    "text": "Spark DataFrame transformations?\n**A:** Avoid wide transformations until necessary, use `select` instead of `*`, cache intermediate results when reused, and avoid UDFs unless unavoidable. Vectorized operations (pandas UDFs) are faster than row-wise ones.\n\n### Q19. How do you manage error handling in PySpark?\n**A:** I wrap critical transformations with try/except. Failures are logged into error tables. In ADF, we configure retries and failure alerts. This ensures job doesn't fail silently.\n\n### Q20. What's the biggest challenge you solved with PySpark at Walgreens?\n**A:** Optimizing a 10TB sales fact pipeline that originally took 2+ hours. By tuning partitioning, salting skewed joins, and compacting files, I reduced runtime to 40 minutes. This improved SLA compliance and cut costs by 30%.\n\n## Section B: Azure Data Factory (ADF) & Orchestration (20 Questions)\n\n### Q21. How did you use ADF in your Walgreens project?\n**A:** I used ADF mainly for orchestration and scheduling of Databricks n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 27
    },
    "id": "de_de_interview_guide_27"
  },
  {
    "text": "## Section B: Azure Data Factory (ADF) & Orchestration (20 Questions)\n\n### Q21. How did you use ADF in your Walgreens project?\n**A:** I used ADF mainly for orchestration and scheduling of Databricks notebooks. ADF pipelines triggered ingestion of raw files into ADLS, parameterized notebook runs with date filters, and coordinated dependencies across multiple jobs. For example, when pharmacy and sales data needed to be processed together, ADF ensured ingestion → transformation → validation were executed in sequence with retries on failure.\n\n### Q22. How do you parameterize ADF pipelines?\n**A:** I use ADF global parameters and dataset parameters to make pipelines dynamic. For example, the source file path is parameterized with date and source name, so the same pipeline ingests multiple sources. Databricks notebook activities accept parameters for start_date and end_date, allowing incremental processing without hardcoding.\n\n### Q23. How do you handle scheduling in ADF?\n**A:** I use trigger",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 28
    },
    "id": "de_de_interview_guide_28"
  },
  {
    "text": "s. Databricks notebook activities accept parameters for start_date and end_date, allowing incremental processing without hardcoding.\n\n### Q23. How do you handle scheduling in ADF?\n**A:** I use triggers — primarily tumbling window triggers for periodic jobs like daily loads, and event triggers for file arrival. For critical pipelines, we used tumbling window with retry policies. Business-critical reports were scheduled nightly at 2am, ensuring that Silver/Gold tables were refreshed before business started.\n\n### Q24. How do you handle failure in ADF pipelines?\n**A:** I configure retry policies on activities and failure paths to send alerts. Failed activities log errors in monitoring tables. For example, if a Databricks notebook failed due to schema mismatch, ADF retried once, and on repeated failure, triggered a Logic App alert to teams.\n\n### Q25. How do you design ADF pipelines for scalability?\n**A:** I avoid building one massive pipeline. Instead, I create modular pipelines — ingestion",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 29
    },
    "id": "de_de_interview_guide_29"
  },
  {
    "text": " failure, triggered a Logic App alert to teams.\n\n### Q25. How do you design ADF pipelines for scalability?\n**A:** I avoid building one massive pipeline. Instead, I create modular pipelines — ingestion, transformations, validations — and link them with pipeline chaining. I also parameterize everything, so the same pipeline works across multiple sources. This reduced maintenance and improved reusability for offshore team.\n\n### Q26. How do you handle dependencies between ADF pipelines?\n**A:** I use dependency triggers and pipeline chaining. For example, Silver pipeline starts only after Bronze ingestion completes successfully. If multiple datasets must be ready before Gold processing, I use \"Wait on Activity\" or success/failure dependencies.\n\n### Q27. How do you integrate ADF with Databricks?\n**A:** ADF has a \"Databricks Notebook\" activity. I configured linked services with Key Vault for secrets, so credentials aren't hardcoded. Each notebook call passes parameters like file_date, source ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 30
    },
    "id": "de_de_interview_guide_30"
  },
  {
    "text": "\n**A:** ADF has a \"Databricks Notebook\" activity. I configured linked services with Key Vault for secrets, so credentials aren't hardcoded. Each notebook call passes parameters like file_date, source system, or load_type. This ensured pipelines remained dynamic and secure.\n\n### Q28. How do you handle incremental data loads with ADF?\n**A:** I store last run watermark in a metadata table. ADF fetches this value, passes it to Databricks as parameter. Databricks then queries only new/updated data and merges it into Delta. After successful load, ADF updates the watermark. This ensured daily loads were efficient.\n\n### Q29. How do you monitor ADF pipelines?\n**A:** I use ADF monitoring dashboard for real-time status. In addition, I log metadata like run_id, row counts, and errors into custom Delta tables. Failures are reported via email alerts. Power BI dashboards show historical success/failure trends, which helped management track SLA adherence.\n\n### Q30. How do you secure ADF pipelines?\n**A",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 31
    },
    "id": "de_de_interview_guide_31"
  },
  {
    "text": " tables. Failures are reported via email alerts. Power BI dashboards show historical success/failure trends, which helped management track SLA adherence.\n\n### Q30. How do you secure ADF pipelines?\n**A:** Secrets like database passwords and storage keys are stored in Azure Key Vault. ADF uses managed identities to connect to ADLS and Databricks. This eliminated hardcoding sensitive info.\n\n### Q31. How do you design ADF for reusability?\n**A:** I use parameterized datasets and pipeline templates. For example, one generic ingestion pipeline handled all CSV sources by passing schema, file_path, and delimiter as parameters. Offshore team just configured metadata, no code changes.\n\n### Q32. How do you handle event-driven ingestion in ADF?\n**A:** For real-time scenarios, I set up Event Grid triggers so ADF pipelines started as soon as new files landed in ADLS. This reduced latency from hours to minutes for near-real-time data availability.\n\n### Q33. How do you integrate ADF with CI/CD?\n**A:** ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 32
    },
    "id": "de_de_interview_guide_32"
  },
  {
    "text": "rs so ADF pipelines started as soon as new files landed in ADLS. This reduced latency from hours to minutes for near-real-time data availability.\n\n### Q33. How do you integrate ADF with CI/CD?\n**A:** ADF JSON definition files are stored in Git. Azure DevOps pipelines deploy these JSONs across dev, test, and prod environments. Parameters like connection strings are environment-specific and replaced during deployment using ARM templates.\n\n### Q34. How do you deal with long-running pipelines?\n**A:** For long pipelines, I break them into smaller pipelines with checkpoints. This ensures partial success is saved and we don't restart everything on failure. For example, ingestion pipeline completed successfully even if transformation pipeline failed, allowing us to restart only transformations.\n\n### Q35. How do you manage data validation with ADF?\n**A:** After ingestion, I run validation notebooks triggered by ADF. These check row counts, null ratios, and duplicates. ADF logs validation status",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 33
    },
    "id": "de_de_interview_guide_33"
  },
  {
    "text": "## Q35. How do you manage data validation with ADF?\n**A:** After ingestion, I run validation notebooks triggered by ADF. These check row counts, null ratios, and duplicates. ADF logs validation status into Delta tables. Alerts are raised if thresholds are violated.\n\n### Q36. How do you manage metadata in ADF pipelines?\n**A:** I store pipeline configs (file paths, schema, business rules) in metadata tables. ADF reads metadata at runtime and applies ingestion accordingly. This approach made pipelines completely dynamic — new sources onboarded without code changes.\n\n### Q37. How do you optimize ADF performance?\n**A:** I configure parallel copy in copy activities, use staging in ADLS/Blob, and partition large files. For example, one large file was split into multiple blocks and ingested in parallel, reducing ingestion time from 40 minutes to under 10.\n\n### Q38. How do you orchestrate multiple technologies with ADF?\n**A:** ADF orchestrated ADLS, Databricks, Synapse, and Snowflake in my proj",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 34
    },
    "id": "de_de_interview_guide_34"
  },
  {
    "text": "allel, reducing ingestion time from 40 minutes to under 10.\n\n### Q38. How do you orchestrate multiple technologies with ADF?\n**A:** ADF orchestrated ADLS, Databricks, Synapse, and Snowflake in my project. For example, ingestion from APIs landed in ADLS, processed in Databricks, exported to Snowflake, and then visualized in Power BI. ADF coordinated the entire workflow end-to-end.\n\n### Q39. How do you handle SLA in ADF pipelines?\n**A:** I tracked expected runtime and row counts. If pipelines exceeded thresholds, alerts triggered. For pharmacy data, SLA was 6am reporting availability — ADF was scheduled at 2am with monitoring, so if job failed, support team had time to re-run before SLA breach.\n\n### Q40. What challenges did you face with ADF and how did you solve them?\n**A:** One challenge was schema drift causing failures in ingestion. I solved it by using schema drift-tolerant ingestion at Bronze and enforcing strict schema at Silver. Another was long ingestion times, solved with paral",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 35
    },
    "id": "de_de_interview_guide_35"
  },
  {
    "text": "as schema drift causing failures in ingestion. I solved it by using schema drift-tolerant ingestion at Bronze and enforcing strict schema at Silver. Another was long ingestion times, solved with parallel copy and partitioning. I also improved security by integrating Key Vault and managed identities, removing all hardcoded secrets.\n\n## Section C: Delta Lake, Schema, Data Modeling (20 Questions)\n\n### Q41. How did you design the medallion architecture in Walgreens?\n**A:** I followed the medallion architecture — Bronze, Silver, and Gold layers. Bronze captured raw ingested data exactly as it arrived from sources, tolerant to schema drift. Silver applied schema enforcement, deduplication, and standardization, making the data clean and query-ready. Gold was modeled into fact and dimension tables optimized for reporting. This layered approach made pipelines robust, reusable, and easy for business teams to consume.\n\n### Q42. How do you enforce schema in Delta Lake?\n**A:** I applied explicit sc",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 36
    },
    "id": "de_de_interview_guide_36"
  },
  {
    "text": " optimized for reporting. This layered approach made pipelines robust, reusable, and easy for business teams to consume.\n\n### Q42. How do you enforce schema in Delta Lake?\n**A:** I applied explicit schemas during ingestion, not relying on inference in production. For schema evolution, I used `mergeSchema` in controlled updates. For example, when a new column was introduced in sales data, I validated it in dev, updated downstream logic, and then enabled schema merge. This prevented silent failures.\n\n### Q43. How do you handle schema drift?\n**A:** Bronze is flexible — it accepts extra columns and quarantines bad rows. Silver enforces schema strictly. Any new column is validated in dev first. If valid, I add it with defaults and update documentation. This two-layer enforcement avoids unexpected breakages.\n\n### Q44. How do you design fact and dimension tables in Gold?\n**A:** I model fact tables to capture transactional data like sales, and dimension tables to capture master data like produ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 37
    },
    "id": "de_de_interview_guide_37"
  },
  {
    "text": "ed breakages.\n\n### Q44. How do you design fact and dimension tables in Gold?\n**A:** I model fact tables to capture transactional data like sales, and dimension tables to capture master data like product, store, and customer. Facts include foreign keys to dimensions. I also denormalize selectively for performance. KPIs like revenue per store were modeled in Gold for Power BI dashboards.\n\n### Q45. How do you design SCD (Slowly Changing Dimensions)?\n**A:** I implemented Type 2 SCD with Delta merge. When a dimension attribute changes, I close the old record with an end_date and insert a new record with active_flag = 1. This preserved historical accuracy. Example: product price changes required Type 2 so reports showed past sales at old prices.\n\n### Q46. How do you manage historical versions of data?\n**A:** Delta Lake's time travel feature allowed querying older versions of tables. For example, when an accidental overwrite occurred, I restored previous version using `VERSION AS OF`. This wa",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 38
    },
    "id": "de_de_interview_guide_38"
  },
  {
    "text": " data?\n**A:** Delta Lake's time travel feature allowed querying older versions of tables. For example, when an accidental overwrite occurred, I restored previous version using `VERSION AS OF`. This was faster than reprocessing from raw files.\n\n### Q47. How do you deal with null values in schema enforcement?\n**A:** I use PySpark `fillna` or business rules to assign defaults. In Silver, nulls in critical columns are flagged in validation tables. Business-approved defaults like \"Unknown\" for missing store_id ensure pipelines don't break.\n\n### Q48. How do you validate transformations across layers?\n**A:** I reconcile row counts and key distributions from Bronze → Silver → Gold. I log validation results into Delta monitoring tables. For example, I checked that deduplication didn't drop more records than expected.\n\n### Q49. How do you design partitioning in Delta tables?\n**A:** I usually partition large datasets by date, as most queries are time-based. For multi-dimensional queries, I use ZO",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 39
    },
    "id": "de_de_interview_guide_39"
  },
  {
    "text": "ords than expected.\n\n### Q49. How do you design partitioning in Delta tables?\n**A:** I usually partition large datasets by date, as most queries are time-based. For multi-dimensional queries, I use ZORDER indexing. Example: sales fact was partitioned by transaction_date, with ZORDER on store_id and product_id for efficient pruning.\n\n### Q50. How do you manage small file problems in Delta?\n**A:** I use `OPTIMIZE` in Databricks to compact small Parquet files into larger ones. I also control batch size during ingestion in ADF to avoid excessive tiny files. For Gold, I scheduled weekly compaction jobs to keep query performance high.\n\n### Q51. How do you design data models for reporting?\n**A:** I followed Kimball principles: fact tables for metrics, dimension tables for descriptive attributes, and star schema design. I ensured measures like sales_amount were additive and dimensions like date, product, and store enabled slice-and-dice in Power BI.\n\n### Q52. How do you manage data lineage?\n**",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 40
    },
    "id": "de_de_interview_guide_40"
  },
  {
    "text": "and star schema design. I ensured measures like sales_amount were additive and dimensions like date, product, and store enabled slice-and-dice in Power BI.\n\n### Q52. How do you manage data lineage?\n**A:** Unity Catalog and Purview tracked lineage across layers. For example, lineage view showed pharmacy source files → Silver clean table → Gold fact → Power BI dashboard. This transparency helped in audits and troubleshooting.\n\n### Q53. How do you handle late-arriving facts in modeling?\n**A:** I used Delta merge to insert or update late-arriving facts. If fact arrived after dimension change, I ensured it still mapped correctly using surrogate keys. For example, late prescription transactions still mapped to the correct product dimension.\n\n### Q54. How do you manage surrogate keys in dimensions?\n**A:** I generated surrogate keys in Silver using hash functions on natural keys. These surrogate keys became dimension table primary keys. Fact tables stored foreign keys referencing them. This ap",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 41
    },
    "id": "de_de_interview_guide_41"
  },
  {
    "text": "s?\n**A:** I generated surrogate keys in Silver using hash functions on natural keys. These surrogate keys became dimension table primary keys. Fact tables stored foreign keys referencing them. This approach ensured consistency even if natural keys changed.\n\n### Q55. How do you validate fact/dimension consistency?\n**A:** I ran referential integrity checks — ensuring every fact foreign key matched a valid dimension key. Invalid records were flagged in a quarantine table for review.\n\n### Q56. How do you manage incremental loads into Gold models?\n**A:** I used Delta `merge` for upserts. Facts were loaded incrementally based on modified_date. Dimensions were updated using SCD logic. This kept models fresh without reprocessing full history.\n\n### Q57. How do you optimize Gold layer models for BI?\n**A:** I pre-aggregated summary tables for common KPIs, reduced joins by denormalizing small dimensions, and compacted files. This ensured Power BI dashboards loaded in seconds instead of minutes.\n\n#",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 42
    },
    "id": "de_de_interview_guide_42"
  },
  {
    "text": "\n**A:** I pre-aggregated summary tables for common KPIs, reduced joins by denormalizing small dimensions, and compacted files. This ensured Power BI dashboards loaded in seconds instead of minutes.\n\n### Q58. How do you handle multi-source integration in modeling?\n**A:** I standardized schemas across sources in Silver. Then I conformed them into unified dimensions. Example: pharmacy and sales sources had different store codes. I standardized codes and built a single store dimension in Gold.\n\n### Q59. How do you document data models?\n**A:** I maintained data dictionaries in Confluence, showing column definitions, lineage, and business rules. Unity Catalog also stored schema metadata. This documentation helped both developers and business users.\n\n### Q60. What challenges did you face in data modeling and how did you solve them?\n**A:** One challenge was aligning different source systems with inconsistent schemas. I solved it by applying conformance rules in Silver and building standardized",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 43
    },
    "id": "de_de_interview_guide_43"
  },
  {
    "text": "modeling and how did you solve them?\n**A:** One challenge was aligning different source systems with inconsistent schemas. I solved it by applying conformance rules in Silver and building standardized dimensions. Another challenge was query slowness in Power BI; I solved it by pre-aggregating summary tables and optimizing partitioning.\n\n## Section D: Optimization, Performance & Troubleshooting (20 Questions)\n\n### Q61. How did you optimize Spark jobs for large data volumes?\n**A:** First, I ensured partitioning strategy matched query patterns, usually date-based. Then, I tuned shuffle partitions (`spark.sql.shuffle.partitions`) based on cluster size, avoiding both too few (skew) and too many (overhead). For joins, I used broadcast for small tables, salting for skew, and AQE (Adaptive Query Execution) to rebalance tasks. I also compacted small files using `OPTIMIZE`. One sales pipeline dropped from 2+ hours to 40 mins with these steps.\n\n### Q62. How do you identify bottlenecks in a Spark ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 44
    },
    "id": "de_de_interview_guide_44"
  },
  {
    "text": "xecution) to rebalance tasks. I also compacted small files using `OPTIMIZE`. One sales pipeline dropped from 2+ hours to 40 mins with these steps.\n\n### Q62. How do you identify bottlenecks in a Spark job?\n**A:** I use Spark UI to analyze DAG stages, tasks, and shuffle read/write sizes. If some tasks run much longer, it usually signals skew. If GC overhead is high, executors need memory tuning. If there's high shuffle volume, I check if joins/aggregations are causing unnecessary repartitions.\n\n### Q63. How do you resolve skew in Spark joins?\n**A:** If skew is caused by a few heavy keys, I apply salting — appending a random suffix to distribute skewed keys across partitions. For small lookup joins, I use `broadcast()`. AQE also automatically splits skewed partitions at runtime in Databricks.\n\n### Q64. How do you optimize Delta Lake performance?\n**A:** I regularly run `OPTIMIZE` with ZORDER on frequently filtered columns. I compact small files into larger Parquet files, improving metadata",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 45
    },
    "id": "de_de_interview_guide_45"
  },
  {
    "text": "\n\n### Q64. How do you optimize Delta Lake performance?\n**A:** I regularly run `OPTIMIZE` with ZORDER on frequently filtered columns. I compact small files into larger Parquet files, improving metadata handling. I also vacuum old versions to reduce storage overhead. For query pruning, I carefully partition on high-cardinality columns like date, not on low-cardinality ones.\n\n### Q65. How do you handle small file problems?\n**A:** I control ingestion batch size in ADF to avoid generating thousands of tiny files. After ingestion, I use `OPTIMIZE` in Delta to compact them. For streaming, I use auto-compaction. This reduces metadata load and speeds up queries.\n\n### Q66. How do you handle long-running jobs?\n**A:** First, I profile the job in Spark UI to identify slow stages. Then, I repartition or broadcast joins as needed. If the job processes full data daily, I redesign it to incremental load. For one job that ran for 5+ hours, converting to incremental reduced it to under 1 hour.\n\n### Q67. ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 46
    },
    "id": "de_de_interview_guide_46"
  },
  {
    "text": " broadcast joins as needed. If the job processes full data daily, I redesign it to incremental load. For one job that ran for 5+ hours, converting to incremental reduced it to under 1 hour.\n\n### Q67. How do you tune Spark cluster configurations?\n**A:** I tune driver/executor memory based on data size. I increase executor cores for parallelism but avoid too many to prevent GC pressure. I use autoscaling for heavy workloads, but for cost control, I size clusters to match partitioning. I also enable cache for reused datasets.\n\n### Q68. How do you optimize joins in PySpark?\n**A:** I decide based on size: Small table + large table → broadcast join; Large tables with skew → repartition + salting; Balanced large tables → hash partition on join keys. I always avoid Cartesian joins and prune unnecessary columns before joining.\n\n### Q69. How do you handle out-of-memory issues in Spark jobs?\n**A:** I check if wide transformations (e.g., groupBy) are blowing up. Then, I increase executor memory or",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 47
    },
    "id": "de_de_interview_guide_47"
  },
  {
    "text": "ssary columns before joining.\n\n### Q69. How do you handle out-of-memory issues in Spark jobs?\n**A:** I check if wide transformations (e.g., groupBy) are blowing up. Then, I increase executor memory or repartition data to spread load. I persist intermediate results on disk instead of memory if needed. In Walgreens, this fixed a memory issue with 1B+ row aggregation.\n\n### Q70. How do you optimize aggregations in PySpark?\n**A:** I partition data on aggregation keys, cache intermediate results if reused, and pre-filter unnecessary rows early. For distinct counts, I used approx algorithms like HyperLogLog when exact wasn't needed, saving resources.\n\n### Q71. How do you debug frequent job failures?\n**A:** I check Databricks logs for stack traces, isolate failing transformation, and test with sample data. If schema mismatch, I enforce schema in Bronze. If bad records, I redirect to quarantine. If infrastructure, I scale cluster or tune configs.\n\n### Q72. How do you tune pipeline latency?\n**A:",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 48
    },
    "id": "de_de_interview_guide_48"
  },
  {
    "text": "ple data. If schema mismatch, I enforce schema in Bronze. If bad records, I redirect to quarantine. If infrastructure, I scale cluster or tune configs.\n\n### Q72. How do you tune pipeline latency?\n**A:** I parallelize independent tasks in ADF, use event triggers for real-time ingestion, and optimize Spark transformations for early filtering. For dashboards, I pre-aggregate Gold tables so BI loads in seconds.\n\n### Q73. How do you troubleshoot data quality issues raised by business?\n**A:** First, I trace lineage in Unity Catalog or Purview to identify which layer/data caused it. Then, I check validations in Silver logs. If caused by schema drift, I fix mapping and reprocess. Communication is key — I keep business updated on issue status and fix ETA.\n\n### Q74. How do you deal with high shuffle volume in Spark jobs?\n**A:** I reduce unnecessary shuffles by avoiding multiple repartitions, pruning columns early, and reusing partitioning. For unavoidable large shuffles, I increase shuffle parti",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 49
    },
    "id": "de_de_interview_guide_49"
  },
  {
    "text": " volume in Spark jobs?\n**A:** I reduce unnecessary shuffles by avoiding multiple repartitions, pruning columns early, and reusing partitioning. For unavoidable large shuffles, I increase shuffle partitions and enable AQE.\n\n### Q75. How do you handle slow dashboards due to data issues?\n**A:** I optimize Gold tables by compacting files, pre-aggregating metrics, and using summary tables. I also partition models so Power BI queries can prune efficiently. One KPI dashboard load time went from 90s to 15s after introducing summary tables.\n\n### Q76. How do you debug performance issues across layers (Bronze → Silver → Gold)?\n**A:** I compare row counts and timings logged at each layer. If Bronze is fine but Silver is slow, I check schema enforcement and dedup logic. If Gold is slow, I check joins and aggregations. Logging at each step helps isolate bottlenecks.\n\n### Q77. How do you optimize pipelines for cost?\n**A:** I use job clusters with auto-termination instead of always-on clusters. I righ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 50
    },
    "id": "de_de_interview_guide_50"
  },
  {
    "text": "nd aggregations. Logging at each step helps isolate bottlenecks.\n\n### Q77. How do you optimize pipelines for cost?\n**A:** I use job clusters with auto-termination instead of always-on clusters. I right-size clusters based on workload. I compact files to reduce storage and metadata cost. For non-critical jobs, I use spot instances.\n\n### Q78. How do you approach troubleshooting late data arrival?\n**A:** I check if ingestion trigger failed in ADF. If source delayed, I escalate to source team. If files arrived but schema mismatched, I fix schema mapping and re-run partial load. I log SLA misses to ensure business visibility.\n\n### Q79. How do you ensure optimized queries in BI tools?\n**A:** I pre-aggregate Gold data, reduce table joins by denormalizing small dimensions, and ensure partition pruning. I also monitor Power BI query logs to tune backend models accordingly.\n\n### Q80. What's the toughest optimization challenge you solved?\n**A:** A sales fact pipeline processing 10TB+ daily was br",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 51
    },
    "id": "de_de_interview_guide_51"
  },
  {
    "text": "ning. I also monitor Power BI query logs to tune backend models accordingly.\n\n### Q80. What's the toughest optimization challenge you solved?\n**A:** A sales fact pipeline processing 10TB+ daily was breaching SLA. Spark jobs had heavy shuffles and skewed joins. I applied salting, ZORDER, and partition tuning, and compacted files weekly. This reduced runtime from 2 hours to 40 minutes, restored SLA compliance, and saved 25% compute cost.\n\n## Section E: Governance, CI/CD, Validation, Offshore & Stakeholder Collaboration (20 Questions)\n\n### Q81. How did you implement data governance in Walgreens?\n**A:** We used Unity Catalog as the central governance layer. It controlled table- and column-level access, enforced policies like masking PII, and provided lineage. For example, DOB and SSN columns were masked for analysts while full access was limited to compliance teams. Purview was also integrated to show lineage across ADF, Databricks, and Power BI.\n\n### Q82. How do you protect PII data in Da",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 52
    },
    "id": "de_de_interview_guide_52"
  },
  {
    "text": "ere masked for analysts while full access was limited to compliance teams. Purview was also integrated to show lineage across ADF, Databricks, and Power BI.\n\n### Q82. How do you protect PII data in Databricks?\n**A:** I masked PII columns in Unity Catalog, enforced row-level security policies, and ensured encryption at rest in ADLS. Keys and secrets were stored in Key Vault, never in code. For reporting, Power BI consumed masked views, ensuring compliance while still supporting analysis.\n\n### Q83. How do you ensure role-based access?\n**A:** Unity Catalog roles were mapped to business roles. Developers had write access in dev, read-only in test/prod. Analysts had read-only access to Gold tables only. Access was granted at schema/table/column levels, following the principle of least privilege.\n\n### Q84. How do you manage data lineage?\n**A:** Unity Catalog and Purview automatically captured lineage from ingestion → transformation → Gold → dashboards. This made it easy to trace an issue in ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 53
    },
    "id": "de_de_interview_guide_53"
  },
  {
    "text": ".\n\n### Q84. How do you manage data lineage?\n**A:** Unity Catalog and Purview automatically captured lineage from ingestion → transformation → Gold → dashboards. This made it easy to trace an issue in a dashboard back to the source system. Business teams used lineage views during audits to verify compliance.\n\n### Q85. How do you integrate Databricks with Azure DevOps CI/CD?\n**A:** We stored all notebooks and ADF pipelines in Git repos. Azure DevOps pipelines deployed them to different environments. Databricks CLI automated notebook deployment, and ADF JSONs were deployed with ARM templates. Environment variables replaced connection strings dynamically.\n\n### Q86. How do you test pipelines before deployment?\n**A:** I implemented unit tests on sample datasets, row count reconciliations, and schema validations in lower environments. Each PR triggered DevOps tests to ensure correctness before merging to main. Only validated pipelines were deployed to higher environments.\n\n### Q87. How do you",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 54
    },
    "id": "de_de_interview_guide_54"
  },
  {
    "text": "hema validations in lower environments. Each PR triggered DevOps tests to ensure correctness before merging to main. Only validated pipelines were deployed to higher environments.\n\n### Q87. How do you handle rollback in CI/CD if deployment fails?\n**A:** Each deployment version was tagged in Git. If a pipeline failed in prod, I rolled back to the last stable version quickly using Git tags and redeployment. Delta Lake time travel also supported rolling back data changes.\n\n### Q88. How do you validate data quality automatically?\n**A:** I built a PySpark validation framework that ran checks like duplicates, nulls, referential integrity, and business rules. Results were logged into validation Delta tables. Any violations triggered ADF failure path alerts and were visible in Power BI dashboards.\n\n### Q89. How do you report data quality to business?\n**A:** Business users accessed a Power BI dashboard that showed row counts, duplicates, null % by table, and rule violations. This gave real-time",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 55
    },
    "id": "de_de_interview_guide_55"
  },
  {
    "text": "\n\n### Q89. How do you report data quality to business?\n**A:** Business users accessed a Power BI dashboard that showed row counts, duplicates, null % by table, and rule violations. This gave real-time transparency into data health. Business could drill down to see which rules failed.\n\n### Q90. How do you monitor SLA compliance?\n**A:** I logged pipeline run durations and compared them to SLA thresholds. If a pipeline breached SLA, ADF triggered alerts. Weekly SLA adherence reports were shared with stakeholders to ensure trust in data delivery.\n\n### Q91. How do you collaborate with offshore teams?\n**A:** I led daily standups with offshore, reviewing backlog and helping unblock them. I also created reusable PySpark frameworks (ingestion, validation) so offshore could onboard new sources with minimal coding. Code reviews and knowledge-sharing sessions ensured they ramped up quickly.\n\n### Q92. How do you mentor offshore developers?\n**A:** I reviewed their PySpark scripts, explained optimiza",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 56
    },
    "id": "de_de_interview_guide_56"
  },
  {
    "text": "minimal coding. Code reviews and knowledge-sharing sessions ensured they ramped up quickly.\n\n### Q92. How do you mentor offshore developers?\n**A:** I reviewed their PySpark scripts, explained optimization techniques like partitioning and salting, and walked them through Spark UI. I also created runbooks for common errors so they could resolve issues without escalation.\n\n### Q93. How do you handle production incidents with offshore?\n**A:** Offshore raised tickets during their shift. I joined morning calls, reviewed logs, and guided them in root cause analysis. If it was schema drift, I advised schema mapping. If infrastructure, I helped with cluster tuning. Communication with business ensured transparency on resolution ETA.\n\n### Q94. How do you communicate with stakeholders?\n**A:** I tailored communication: technical details for developers, impact and ETA for business stakeholders. For example, when a pipeline failed due to schema drift, I told business: \"Data will be delayed by 1 hour ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 57
    },
    "id": "de_de_interview_guide_57"
  },
  {
    "text": "ed communication: technical details for developers, impact and ETA for business stakeholders. For example, when a pipeline failed due to schema drift, I told business: \"Data will be delayed by 1 hour while we patch schema. No data loss.\" This maintained confidence.\n\n### Q95. How do you balance technical delivery with business priorities?\n**A:** I aligned backlog with business SLA commitments. Critical sales reports were prioritized for early-morning refresh. Less critical pipelines (like historical reloads) were scheduled during off-peak. This kept business impact minimal.\n\n### Q96. How do you approach handling unexpected requests?\n**A:** I first clarify urgency with the business, then estimate technical impact. If it's quick (like adding a column), I handle same day. If bigger (like new dataset), I put it in sprint backlog. For example, adding a new KPI to Power BI was prioritized within 24h because executives needed it.\n\n### Q97. How do you ensure compliance in pipelines?\n**A:** I en",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 58
    },
    "id": "de_de_interview_guide_58"
  },
  {
    "text": "ataset), I put it in sprint backlog. For example, adding a new KPI to Power BI was prioritized within 24h because executives needed it.\n\n### Q97. How do you ensure compliance in pipelines?\n**A:** I ensured encryption at rest (ADLS), masking in Unity Catalog, and logging of all access requests. Data quality dashboards ensured transparency. For audits, Purview lineage reports showed full data flows, proving compliance.\n\n### Q98. How do you track pipeline metrics over time?\n**A:** I logged row counts, runtime, and errors into Delta monitoring tables. A Power BI dashboard visualized historical pipeline performance, showing trends in failures or runtimes. This helped proactively optimize before SLAs broke.\n\n### Q99. How do you ensure knowledge transfer across teams?\n**A:** I maintained detailed documentation in Confluence, covering pipeline design, schema definitions, and troubleshooting steps. I also held KT sessions with offshore, walking through real examples in Databricks notebooks.\n\n##",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 59
    },
    "id": "de_de_interview_guide_59"
  },
  {
    "text": " documentation in Confluence, covering pipeline design, schema definitions, and troubleshooting steps. I also held KT sessions with offshore, walking through real examples in Databricks notebooks.\n\n### Q100. What's your biggest governance or collaboration achievement?\n**A:** My biggest achievement was implementing Unity Catalog + validation framework end-to-end. It ensured PII was masked, data lineage was transparent, and business saw data quality dashboards. Combined with mentoring offshore, it built trust in the system. Stakeholders appreciated that pipelines were compliant, optimized, and reliable — all while I led a distributed team.\n\n## Section F: Scenario-Based Questions (20 Questions)\n\n### S1. Scenario: A downstream Power BI dashboard shows wrong sales numbers. How do you handle it?\n**A:** First, I'd trace lineage in Unity Catalog to identify which Gold table feeds that report. Then, I'd check Silver → Gold transformations for logic errors. I'd validate row counts and business r",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 60
    },
    "id": "de_de_interview_guide_60"
  },
  {
    "text": "\n**A:** First, I'd trace lineage in Unity Catalog to identify which Gold table feeds that report. Then, I'd check Silver → Gold transformations for logic errors. I'd validate row counts and business rules in validation logs. If the issue came from source schema drift (like a new column added in pharmacy data), I'd patch mapping in Silver, reprocess affected partitions, and communicate with stakeholders immediately, explaining ETA for fix. This keeps business confidence while resolving the root cause.\n\n### S2. Scenario: Your pipeline starts failing at 2am due to corrupt input files. What's your approach?\n**A:** I'd configure ingestion with `PERMISSIVE` mode so corrupt rows are flagged into `_corrupt_record`. These rows would be redirected into a quarantine Delta table. The main pipeline would continue for good rows, avoiding SLA breach. Later, I'd review the bad records, escalate to source teams, and patch schema validation if needed. This way, business dashboards still refresh on time.",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 61
    },
    "id": "de_de_interview_guide_61"
  },
  {
    "text": "ld continue for good rows, avoiding SLA breach. Later, I'd review the bad records, escalate to source teams, and patch schema validation if needed. This way, business dashboards still refresh on time.\n\n### S3. Scenario: You need to migrate Synapse notebooks into Databricks. How would you do it?\n**A:** Synapse transformations are SQL-based. I'd first review existing T-SQL scripts, then rewrite them into PySpark DataFrame transformations in Databricks. I'd leverage Delta Lake features like ACID and time travel for consistency. To validate migration, I'd reconcile row counts, run sample KPI checks, and parallel-run old Synapse vs new Databricks for a cycle before cutover.\n\n### S4. Scenario: A job that usually runs in 30 minutes suddenly takes 2 hours. How do you troubleshoot?\n**A:** I'd check Spark UI to see if shuffle partitions increased, or if skew developed. I'd check recent data volume spikes. If caused by skew, I'd apply salting or broadcast joins. If due to small file explosion, I'",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 62
    },
    "id": "de_de_interview_guide_62"
  },
  {
    "text": " Spark UI to see if shuffle partitions increased, or if skew developed. I'd check recent data volume spikes. If caused by skew, I'd apply salting or broadcast joins. If due to small file explosion, I'd run compaction (`OPTIMIZE`). If it's cluster issue, I'd tune executors or restart with right sizing. Documentation of findings ensures root cause is understood.\n\n### S5. Scenario: Two sets of users need different partitioning (state-based vs product-based). How do you solve it?\n**A:** Instead of duplicating datasets, I'd partition primarily on date (common for both) and apply ZORDER indexing on state and product columns. This ensures pruning for both user groups. If certain queries are very heavy, I'd create summary tables (by state or product) in Gold for faster BI performance.\n\n### S6. Scenario: A new data source is onboarded. How do you make your pipeline flexible?\n**A:** I'd store ingestion configs in metadata tables (path, schema, delimiter, rules). ADF would read configs and trigge",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 63
    },
    "id": "de_de_interview_guide_63"
  },
  {
    "text": "nario: A new data source is onboarded. How do you make your pipeline flexible?\n**A:** I'd store ingestion configs in metadata tables (path, schema, delimiter, rules). ADF would read configs and trigger the generic ingestion pipeline. In Silver, reusable PySpark validation functions enforce schema and rules. Offshore team only needs to add metadata entries, no code changes. This accelerates onboarding.\n\n### S7. Scenario: Business complains KPIs don't match finance reports. What's your action plan?\n**A:** First, I'd meet with finance team to understand calculation logic. Then, I'd trace current KPI logic in Gold models. If discrepancy is due to business rule misalignment (e.g., revenue net of returns), I'd adjust logic and document it. If due to data lag, I'd reschedule refresh. Clear documentation + governance ensures future alignment.\n\n### S8. Scenario: Pipeline fails due to schema drift — new column added in source. What's your fix?\n**A:** Bronze ingests flexibly. In Silver, I'd add t",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 64
    },
    "id": "de_de_interview_guide_64"
  },
  {
    "text": "ntation + governance ensures future alignment.\n\n### S8. Scenario: Pipeline fails due to schema drift — new column added in source. What's your fix?\n**A:** Bronze ingests flexibly. In Silver, I'd add the new column with default or nulls, update schema enforcement, and test in dev. After validating downstream transformations, I'd push change to prod. I'd also update schema documentation and inform business about the new attribute.\n\n### S9. Scenario: Offshore reports jobs failed overnight, but you're onsite. How do you handle?\n**A:** I'd quickly check logs to identify root cause. If it's schema mismatch, I'd patch mapping. If volume spike, I'd rescale cluster. I'd communicate to business with ETA (\"Data delayed by 1 hour, fix in progress\"). Meanwhile, I'd guide offshore via call so they learn the resolution. This builds trust and team capability.\n\n### S10. Scenario: Data load is complete but dashboards are very slow. What's your fix?\n**A:** I'd review Gold models. If queries scan too many",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 65
    },
    "id": "de_de_interview_guide_65"
  },
  {
    "text": " the resolution. This builds trust and team capability.\n\n### S10. Scenario: Data load is complete but dashboards are very slow. What's your fix?\n**A:** I'd review Gold models. If queries scan too many rows, I'd pre-aggregate summary tables. I'd compact files (`OPTIMIZE`) and ensure partitioning matches query filters. For Power BI, I'd use DirectQuery with RLS and reduce joins by denormalizing smaller dimensions.\n\n### S11. Scenario: You need to handle late-arriving sales transactions. How do you design this?\n**A:** I'd use Delta `merge` with watermarking. Late rows within X days are merged automatically. If outside retention, I'd reprocess specific partitions after business approval. For analytics, SCD ensures late facts still map to correct dimension versions.\n\n### S12. Scenario: Business requests PII masking for compliance. What's your approach?\n**A:** I'd configure Unity Catalog to mask sensitive columns like SSN or DOB. Only compliance teams would see full values. Analysts would see",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 66
    },
    "id": "de_de_interview_guide_66"
  },
  {
    "text": "requests PII masking for compliance. What's your approach?\n**A:** I'd configure Unity Catalog to mask sensitive columns like SSN or DOB. Only compliance teams would see full values. Analysts would see masked/nulls. Access logs would track queries. This balanced compliance and usability.\n\n### S13. Scenario: API source throttles requests, but you must ingest daily. How do you manage?\n**A:** In ADF, I'd configure pagination and retries. In Databricks, I'd implement rate-limiting logic with exponential backoff. If needed, I'd parallelize calls with controlled concurrency. Failures are logged and retried separately, so ingestion completes within SLA without breaching API limits.\n\n### S14. Scenario: You find too many small files in ADLS. How do you handle this?\n**A:** I'd batch ingestion in ADF to reduce small files. In Delta, I'd run weekly compaction jobs (`OPTIMIZE`) and ZORDER for query columns. This reduced file count, improved performance, and lowered metadata overhead.\n\n### S15. Scena",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 67
    },
    "id": "de_de_interview_guide_67"
  },
  {
    "text": "to reduce small files. In Delta, I'd run weekly compaction jobs (`OPTIMIZE`) and ZORDER for query columns. This reduced file count, improved performance, and lowered metadata overhead.\n\n### S15. Scenario: A regulatory audit requires proof of lineage. How do you provide it?\n**A:** I'd use Purview/Unity Catalog lineage reports to show flow from source → Bronze → Silver → Gold → dashboard. This visual lineage, plus validation logs, provided end-to-end transparency. During audit, we demonstrated compliance with role-based access and PII masking.\n\n### S16. Scenario: Pipeline processing jumps from 1TB/day to 5TB/day. How do you scale?\n**A:** I'd scale clusters with more executors temporarily. Then, I'd review partitioning to ensure balanced distribution. I'd switch from full loads to incremental using modified_date. I'd also optimize joins and ZORDER on high-cardinality columns. This allowed scaling without uncontrolled cost.\n\n### S17. Scenario: Business wants new KPI \"avg sales per customer",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 68
    },
    "id": "de_de_interview_guide_68"
  },
  {
    "text": " using modified_date. I'd also optimize joins and ZORDER on high-cardinality columns. This allowed scaling without uncontrolled cost.\n\n### S17. Scenario: Business wants new KPI \"avg sales per customer\" in dashboards. How do you deliver?\n**A:** I'd update Gold model by joining sales fact with customer dimension, compute metric in PySpark, and store in summary table. I'd validate numbers with business team, deploy changes via CI/CD, and update Power BI to expose the new KPI.\n\n### S18. Scenario: Data quality check flags 5% null store_id in Silver. What's your action?\n**A:** I'd first confirm if nulls are due to source issues. If yes, escalate to source team. Meanwhile, I'd assign default store \"Unknown\" for analysis continuity. I'd document rule and flag these records in data quality dashboard so business is aware.\n\n### S19. Scenario: Offshore wants to add a new pipeline but lacks guidance. How do you help?\n**A:** I'd ask them to define source and target configs in metadata. Then, I'd gui",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 69
    },
    "id": "de_de_interview_guide_69"
  },
  {
    "text": "d so business is aware.\n\n### S19. Scenario: Offshore wants to add a new pipeline but lacks guidance. How do you help?\n**A:** I'd ask them to define source and target configs in metadata. Then, I'd guide them on plugging configs into our reusable ingestion framework. I'd review their PySpark script and provide optimization tips. Over time, this made them independent and efficient.\n\n### S20. Scenario: Customer interview asks: 'What's your biggest achievement?'\n**A:** I'd highlight optimizing a 10TB sales pipeline from 2+ hours to 40 mins using partition tuning, salting, ZORDER, and compaction. This improvement ensured SLA compliance and cut costs by 25%. I'd also mention building validation dashboards for data quality, which gave business confidence in our platform.\n\n---\n\n## Key Takeaways\n\n- **Performance**: Proper partitioning, broadcast joins, and adaptive query execution are crucial\n- **Data Quality**: Implement comprehensive error handling and data validation\n- **Monitoring**: Log me",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 70
    },
    "id": "de_de_interview_guide_70"
  },
  {
    "text": " **Performance**: Proper partitioning, broadcast joins, and adaptive query execution are crucial\n- **Data Quality**: Implement comprehensive error handling and data validation\n- **Monitoring**: Log metrics and create dashboards for pipeline health\n- **Testing**: Unit tests and automated validation ensure pipeline reliability\n- **Code Organization**: Modular design improves maintainability and reusability\n- **Delta Lake**: Leverage time travel and merge capabilities for data management\n- **Governance**: Unity Catalog and Purview provide comprehensive data governance\n- **Collaboration**: Effective offshore team management and stakeholder communication\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_interview_guide.md",
      "file_name": "de_interview_guide.md",
      "chunk_index": 71
    },
    "id": "de_de_interview_guide_71"
  },
  {
    "text": "---\ntags: [data-engineer, python, pandas, automation, data-quality, scripting]\npersona: de\n---\n\n# Python for Data Engineering - Krishna's Skills\n\n## Introduction\n**Krishna's Python Experience:**\nPython is my go-to language for data engineering - from pandas transformations to automation scripts. At Walgreens, I use Python for data quality checks, pipeline orchestration, and glue code between systems. Here's everything I use daily.\n\n## Pandas for Data Transformation\n\n### Data Cleaning\n**Krishna's Most Common Patterns:**\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Read data\ndf = pd.read_csv('orders.csv')\n\n# Clean and standardize\ndf_clean = (\n    df\n    # Remove duplicates\n    .drop_duplicates(subset='order_id', keep='last')\n    \n    # Handle missing values\n    .assign(\n        customer_id=lambda x: x['customer_id'].fillna('UNKNOWN'),\n        order_amount=lambda x: x['order_amount'].fillna(0)\n    )\n    \n    # Data type conversions\n    .astype({\n      ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 72
    },
    "id": "de_de_python_skills_72"
  },
  {
    "text": "es\n    .assign(\n        customer_id=lambda x: x['customer_id'].fillna('UNKNOWN'),\n        order_amount=lambda x: x['order_amount'].fillna(0)\n    )\n    \n    # Data type conversions\n    .astype({\n        'order_id': 'str',\n        'customer_id': 'str',\n        'order_amount': 'float64'\n    })\n    \n    # Parse dates\n    .assign(\n        order_date=lambda x: pd.to_datetime(x['order_date'], errors='coerce')\n    )\n    \n    # String cleaning\n    .assign(\n        customer_email=lambda x: x['customer_email'].str.lower().str.strip(),\n        product_name=lambda x: x['product_name'].str.title()\n    )\n)\n```\n\n### Complex Transformations\n**Krishna's Business Logic:**\n```python\n# Customer segmentation based on business rules\ndef calculate_customer_segment(row):\n    ltv = row['lifetime_value']\n    tenure_days = row['customer_tenure_days']\n    order_count = row['order_count']\n    \n    if ltv > 10000 or (ltv > 5000 and tenure_days > 365):\n        return 'Platinum'\n    elif ltv > 5000 or (ltv > 2000 and ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 73
    },
    "id": "de_de_python_skills_73"
  },
  {
    "text": "days = row['customer_tenure_days']\n    order_count = row['order_count']\n    \n    if ltv > 10000 or (ltv > 5000 and tenure_days > 365):\n        return 'Platinum'\n    elif ltv > 5000 or (ltv > 2000 and order_count > 10):\n        return 'Gold'\n    elif ltv > 1000 or order_count > 5:\n        return 'Silver'\n    else:\n        return 'Bronze'\n\ndf['customer_segment'] = df.apply(calculate_customer_segment, axis=1)\n\n# Vectorized alternative (faster for large datasets)\ndf['customer_segment'] = np.select(\n    [\n        (df['lifetime_value'] > 10000) | ((df['lifetime_value'] > 5000) & (df['customer_tenure_days'] > 365)),\n        (df['lifetime_value'] > 5000) | ((df['lifetime_value'] > 2000) & (df['order_count'] > 10)),\n        (df['lifetime_value'] > 1000) | (df['order_count'] > 5)\n    ],\n    ['Platinum', 'Gold', 'Silver'],\n    default='Bronze'\n)\n```\n\n### Aggregations & Grouping\n**Krishna's Analytics Patterns:**\n```python\n# Complex aggregations\ncustomer_summary = (\n    df_orders\n    .groupby('cust",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 74
    },
    "id": "de_de_python_skills_74"
  },
  {
    "text": "um', 'Gold', 'Silver'],\n    default='Bronze'\n)\n```\n\n### Aggregations & Grouping\n**Krishna's Analytics Patterns:**\n```python\n# Complex aggregations\ncustomer_summary = (\n    df_orders\n    .groupby('customer_id')\n    .agg({\n        'order_id': 'count',\n        'order_amount': ['sum', 'mean', 'std'],\n        'order_date': ['min', 'max']\n    })\n    .reset_index()\n)\n\n# Flatten multi-level columns\ncustomer_summary.columns = ['_'.join(col).strip('_') for col in customer_summary.columns.values]\n\n# Custom aggregations\ndef calculate_metrics(group):\n    return pd.Series({\n        'order_count': len(group),\n        'total_spent': group['order_amount'].sum(),\n        'avg_order_value': group['order_amount'].mean(),\n        'first_order': group['order_date'].min(),\n        'last_order': group['order_date'].max(),\n        'days_active': (group['order_date'].max() - group['order_date'].min()).days,\n        'unique_products': group['product_id'].nunique()\n    })\n\ncustomer_metrics = df_orders.groupby('cu",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 75
    },
    "id": "de_de_python_skills_75"
  },
  {
    "text": "'].max(),\n        'days_active': (group['order_date'].max() - group['order_date'].min()).days,\n        'unique_products': group['product_id'].nunique()\n    })\n\ncustomer_metrics = df_orders.groupby('customer_id').apply(calculate_metrics).reset_index()\n```\n\n## Data Quality Framework\n\n### Automated Quality Checks\n**Krishna's Production Quality System:**\n```python\nimport logging\nfrom typing import Dict, List, Any\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass DataQualityChecker:\n    \"\"\"Comprehensive data quality validation\"\"\"\n    \n    def __init__(self, df: pd.DataFrame, table_name: str):\n        self.df = df\n        self.table_name = table_name\n        self.checks = []\n        self.errors = []\n        self.warnings = []\n    \n    def check_nulls(self, columns: List[str], threshold: float = 0.05):\n        \"\"\"Check for null values exceeding threshold\"\"\"\n        for col in columns:\n            if col not in self.df.columns:\n                continue\n      ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 76
    },
    "id": "de_de_python_skills_76"
  },
  {
    "text": "mns: List[str], threshold: float = 0.05):\n        \"\"\"Check for null values exceeding threshold\"\"\"\n        for col in columns:\n            if col not in self.df.columns:\n                continue\n            \n            null_count = self.df[col].isnull().sum()\n            null_pct = null_count / len(self.df)\n            \n            if null_pct > threshold:\n                self.errors.append({\n                    'check': 'null_values',\n                    'column': col,\n                    'null_count': null_count,\n                    'null_percentage': round(null_pct * 100, 2)\n                })\n            elif null_pct > 0:\n                self.warnings.append({\n                    'check': 'null_values',\n                    'column': col,\n                    'null_count': null_count\n                })\n        \n        return self\n    \n    def check_duplicates(self, key_columns: List[str]):\n        \"\"\"Check for duplicate records\"\"\"\n        dup_count = self.df.duplicated(subset=key_c",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 77
    },
    "id": "de_de_python_skills_77"
  },
  {
    "text": "              })\n        \n        return self\n    \n    def check_duplicates(self, key_columns: List[str]):\n        \"\"\"Check for duplicate records\"\"\"\n        dup_count = self.df.duplicated(subset=key_columns).sum()\n        \n        if dup_count > 0:\n            self.errors.append({\n                'check': 'duplicates',\n                'key_columns': key_columns,\n                'duplicate_count': dup_count\n            })\n        \n        return self\n    \n    def check_data_types(self, expected_types: Dict[str, str]):\n        \"\"\"Validate column data types\"\"\"\n        for col, expected_type in expected_types.items():\n            if col not in self.df.columns:\n                self.errors.append({\n                    'check': 'missing_column',\n                    'column': col\n                })\n                continue\n            \n            actual_type = str(self.df[col].dtype)\n            if expected_type not in actual_type:\n                self.errors.append({\n                    'che",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 78
    },
    "id": "de_de_python_skills_78"
  },
  {
    "text": ")\n                continue\n            \n            actual_type = str(self.df[col].dtype)\n            if expected_type not in actual_type:\n                self.errors.append({\n                    'check': 'data_type_mismatch',\n                    'column': col,\n                    'expected': expected_type,\n                    'actual': actual_type\n                })\n        \n        return self\n    \n    def check_value_range(self, column: str, min_value: Any, max_value: Any):\n        \"\"\"Validate numeric ranges\"\"\"\n        if column not in self.df.columns:\n            return self\n        \n        out_of_range = (\n            (self.df[column] < min_value) | \n            (self.df[column] > max_value)\n        ).sum()\n        \n        if out_of_range > 0:\n            self.errors.append({\n                'check': 'value_range',\n                'column': column,\n                'out_of_range_count': out_of_range,\n                'expected_range': f'{min_value} to {max_value}'\n            })\n ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 79
    },
    "id": "de_de_python_skills_79"
  },
  {
    "text": "          'check': 'value_range',\n                'column': column,\n                'out_of_range_count': out_of_range,\n                'expected_range': f'{min_value} to {max_value}'\n            })\n        \n        return self\n    \n    def check_referential_integrity(self, column: str, reference_df: pd.DataFrame, reference_column: str):\n        \"\"\"Check foreign key integrity\"\"\"\n        orphaned = ~self.df[column].isin(reference_df[reference_column])\n        orphaned_count = orphaned.sum()\n        \n        if orphaned_count > 0:\n            self.errors.append({\n                'check': 'referential_integrity',\n                'column': column,\n                'reference_column': reference_column,\n                'orphaned_count': orphaned_count\n            })\n        \n        return self\n    \n    def generate_report(self) -> Dict:\n        \"\"\"Generate quality report\"\"\"\n        report = {\n            'table_name': self.table_name,\n            'timestamp': datetime.now().isoformat(),\n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 80
    },
    "id": "de_de_python_skills_80"
  },
  {
    "text": "   \n    def generate_report(self) -> Dict:\n        \"\"\"Generate quality report\"\"\"\n        report = {\n            'table_name': self.table_name,\n            'timestamp': datetime.now().isoformat(),\n            'row_count': len(self.df),\n            'column_count': len(self.df.columns),\n            'errors': self.errors,\n            'warnings': self.warnings,\n            'status': 'FAILED' if self.errors else 'PASSED'\n        }\n        \n        # Log report\n        if self.errors:\n            logger.error(f\"❌ Data Quality FAILED for {self.table_name}: {len(self.errors)} errors\")\n            for error in self.errors:\n                logger.error(f\"  - {error}\")\n        else:\n            logger.info(f\"✅ Data Quality PASSED for {self.table_name}\")\n        \n        return report\n\n# Usage\ndf_orders = pd.read_csv('orders.csv')\n\nquality_report = (\n    DataQualityChecker(df_orders, 'fct_orders')\n    .check_nulls(['order_id', 'customer_id'], threshold=0.01)\n    .check_duplicates(['order_id'])\n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 81
    },
    "id": "de_de_python_skills_81"
  },
  {
    "text": "s = pd.read_csv('orders.csv')\n\nquality_report = (\n    DataQualityChecker(df_orders, 'fct_orders')\n    .check_nulls(['order_id', 'customer_id'], threshold=0.01)\n    .check_duplicates(['order_id'])\n    .check_data_types({\n        'order_id': 'object',\n        'order_amount': 'float',\n        'order_date': 'datetime'\n    })\n    .check_value_range('order_amount', 0, 1000000)\n    .generate_report()\n)\n```\n\n## Azure/Databricks Integration\n\n### Reading from Azure Blob\n```python\nfrom azure.storage.blob import BlobServiceClient\nimport io\n\ndef read_blob_to_pandas(account_name: str, container_name: str, blob_name: str, account_key: str) -> pd.DataFrame:\n    \"\"\"Read CSV from Azure Blob Storage\"\"\"\n    \n    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n    \n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n    blob_client = blob_service_client.get_blob_client(container=conta",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 82
    },
    "id": "de_de_python_skills_82"
  },
  {
    "text": "t_key};EndpointSuffix=core.windows.net\"\n    \n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n    \n    # Download blob content\n    blob_data = blob_client.download_blob().readall()\n    \n    # Read into pandas\n    df = pd.read_csv(io.BytesIO(blob_data))\n    \n    return df\n\n# Usage\ndf = read_blob_to_pandas(\n    account_name='walgreensdata',\n    container_name='raw',\n    blob_name='orders/2024-01-15/orders.csv',\n    account_key='your-key-here'\n)\n```\n\n### Writing to Delta Lake\n```python\ndef pandas_to_delta(df: pd.DataFrame, delta_path: str, mode: str = 'overwrite', partition_cols: List[str] = None):\n    \"\"\"Convert pandas to Delta format via PySpark\"\"\"\n    from pyspark.sql import SparkSession\n    \n    spark = SparkSession.builder.getOrCreate()\n    \n    # Convert pandas to Spark DataFrame\n    spark_df = spark.createDataFrame(df)\n    \n    # Write to Delta\n    wr",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 83
    },
    "id": "de_de_python_skills_83"
  },
  {
    "text": "spark.sql import SparkSession\n    \n    spark = SparkSession.builder.getOrCreate()\n    \n    # Convert pandas to Spark DataFrame\n    spark_df = spark.createDataFrame(df)\n    \n    # Write to Delta\n    writer = spark_df.write.format('delta').mode(mode)\n    \n    if partition_cols:\n        writer = writer.partitionBy(*partition_cols)\n    \n    writer.save(delta_path)\n    \n    print(f\"✅ Wrote {len(df):,} rows to {delta_path}\")\n\n# Usage\npandas_to_delta(\n    df_orders,\n    delta_path='/mnt/silver/orders',\n    mode='overwrite',\n    partition_cols=['order_date']\n)\n```\n\n## Automation Scripts\n\n### Pipeline Monitoring Script\n**Krishna's Daily Monitoring:**\n```python\nimport requests\nfrom datetime import datetime, timedelta\nimport smtplib\nfrom email.mime.text import MIMEText\n\nclass PipelineMonitor:\n    \"\"\"Monitor data pipelines and alert on failures\"\"\"\n    \n    def __init__(self, adf_resource_id: str, subscription_id: str, access_token: str):\n        self.adf_resource_id = adf_resource_id\n        self.",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 84
    },
    "id": "de_de_python_skills_84"
  },
  {
    "text": "Monitor data pipelines and alert on failures\"\"\"\n    \n    def __init__(self, adf_resource_id: str, subscription_id: str, access_token: str):\n        self.adf_resource_id = adf_resource_id\n        self.subscription_id = subscription_id\n        self.access_token = access_token\n        self.base_url = f\"https://management.azure.com{adf_resource_id}\"\n    \n    def get_pipeline_runs(self, pipeline_name: str, hours: int = 24) -> List[Dict]:\n        \"\"\"Get recent pipeline runs\"\"\"\n        url = f\"{self.base_url}/queryPipelineRuns?api-version=2018-06-01\"\n        \n        end_time = datetime.utcnow()\n        start_time = end_time - timedelta(hours=hours)\n        \n        payload = {\n            \"lastUpdatedAfter\": start_time.isoformat() + \"Z\",\n            \"lastUpdatedBefore\": end_time.isoformat() + \"Z\",\n            \"filters\": [{\n                \"operand\": \"PipelineName\",\n                \"operator\": \"Equals\",\n                \"values\": [pipeline_name]\n            }]\n        }\n        \n        header",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 85
    },
    "id": "de_de_python_skills_85"
  },
  {
    "text": "\",\n            \"filters\": [{\n                \"operand\": \"PipelineName\",\n                \"operator\": \"Equals\",\n                \"values\": [pipeline_name]\n            }]\n        }\n        \n        headers = {\n            'Authorization': f'Bearer {self.access_token}',\n            'Content-Type': 'application/json'\n        }\n        \n        response = requests.post(url, json=payload, headers=headers)\n        response.raise_for_status()\n        \n        return response.json().get('value', [])\n    \n    def check_pipeline_health(self, pipeline_name: str) -> Dict:\n        \"\"\"Check pipeline health status\"\"\"\n        runs = self.get_pipeline_runs(pipeline_name, hours=24)\n        \n        if not runs:\n            return {\n                'status': 'WARNING',\n                'message': f'No runs found for {pipeline_name} in last 24 hours'\n            }\n        \n        latest_run = runs[0]\n        status = latest_run.get('status')\n        \n        if status == 'Failed':\n            return {\n      ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 86
    },
    "id": "de_de_python_skills_86"
  },
  {
    "text": " for {pipeline_name} in last 24 hours'\n            }\n        \n        latest_run = runs[0]\n        status = latest_run.get('status')\n        \n        if status == 'Failed':\n            return {\n                'status': 'CRITICAL',\n                'message': f'{pipeline_name} FAILED',\n                'run_id': latest_run.get('runId'),\n                'error': latest_run.get('message')\n            }\n        elif status == 'Succeeded':\n            return {\n                'status': 'OK',\n                'message': f'{pipeline_name} succeeded',\n                'duration': latest_run.get('durationInMs')\n            }\n        else:\n            return {\n                'status': 'WARNING',\n                'message': f'{pipeline_name} status: {status}'\n            }\n    \n    def send_alert(self, subject: str, body: str, recipients: List[str]):\n        \"\"\"Send email alert\"\"\"\n        msg = MIMEText(body)\n        msg['Subject'] = subject\n        msg['From'] = 'data-pipelines@walgreens.com'\n     ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 87
    },
    "id": "de_de_python_skills_87"
  },
  {
    "text": "subject: str, body: str, recipients: List[str]):\n        \"\"\"Send email alert\"\"\"\n        msg = MIMEText(body)\n        msg['Subject'] = subject\n        msg['From'] = 'data-pipelines@walgreens.com'\n        msg['To'] = ', '.join(recipients)\n        \n        with smtplib.SMTP('smtp.office365.com', 587) as server:\n            server.starttls()\n            server.send_message(msg)\n\n# Usage\nmonitor = PipelineMonitor(\n    adf_resource_id='/subscriptions/xxx/resourceGroups/rg-data/providers/Microsoft.DataFactory/factories/adf-walgreens',\n    subscription_id='xxx',\n    access_token='your-token'\n)\n\n# Check critical pipelines\ncritical_pipelines = ['pl_daily_orders', 'pl_customer_sync', 'pl_inventory_update']\n\nfor pipeline in critical_pipelines:\n    health = monitor.check_pipeline_health(pipeline)\n    \n    if health['status'] in ['CRITICAL', 'WARNING']:\n        monitor.send_alert(\n            subject=f\"Pipeline Alert: {pipeline}\",\n            body=health['message'],\n            recipients=['data-tea",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 88
    },
    "id": "de_de_python_skills_88"
  },
  {
    "text": "    if health['status'] in ['CRITICAL', 'WARNING']:\n        monitor.send_alert(\n            subject=f\"Pipeline Alert: {pipeline}\",\n            body=health['message'],\n            recipients=['data-team@walgreens.com']\n        )\n```\n\n### Data Reconciliation Script\n**Krishna's Quality Assurance:**\n```python\ndef reconcile_data(source_df: pd.DataFrame, target_df: pd.DataFrame, key_columns: List[str]) -> Dict:\n    \"\"\"Compare source and target data\"\"\"\n    \n    # Record counts\n    source_count = len(source_df)\n    target_count = len(target_df)\n    \n    # Find differences\n    source_keys = set(source_df[key_columns].apply(tuple, axis=1))\n    target_keys = set(target_df[key_columns].apply(tuple, axis=1))\n    \n    missing_in_target = source_keys - target_keys\n    extra_in_target = target_keys - source_keys\n    \n    # Value differences for common keys\n    common_keys = source_keys & target_keys\n    \n    value_differences = []\n    for key in common_keys:\n        source_row = source_df[source_df[ke",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 89
    },
    "id": "de_de_python_skills_89"
  },
  {
    "text": "ce_keys\n    \n    # Value differences for common keys\n    common_keys = source_keys & target_keys\n    \n    value_differences = []\n    for key in common_keys:\n        source_row = source_df[source_df[key_columns].apply(tuple, axis=1) == key].iloc[0]\n        target_row = target_df[target_df[key_columns].apply(tuple, axis=1) == key].iloc[0]\n        \n        for col in source_df.columns:\n            if col not in key_columns and col in target_df.columns:\n                if source_row[col] != target_row[col]:\n                    value_differences.append({\n                        'key': key,\n                        'column': col,\n                        'source_value': source_row[col],\n                        'target_value': target_row[col]\n                    })\n    \n    # Generate report\n    report = {\n        'source_count': source_count,\n        'target_count': target_count,\n        'count_difference': target_count - source_count,\n        'missing_in_target_count': len(missing_in_target),",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 90
    },
    "id": "de_de_python_skills_90"
  },
  {
    "text": "port = {\n        'source_count': source_count,\n        'target_count': target_count,\n        'count_difference': target_count - source_count,\n        'missing_in_target_count': len(missing_in_target),\n        'extra_in_target_count': len(extra_in_target),\n        'value_differences_count': len(value_differences),\n        'status': 'PASSED' if (\n            missing_in_target == set() and \n            extra_in_target == set() and \n            len(value_differences) == 0\n        ) else 'FAILED'\n    }\n    \n    if report['status'] == 'FAILED':\n        logger.error(f\"❌ Reconciliation FAILED: {report}\")\n    else:\n        logger.info(f\"✅ Reconciliation PASSED\")\n    \n    return report\n```\n\n## Performance Optimization\n\n### Efficient DataFrame Operations\n**Krishna's Speed Tips:**\n```python\nimport time\n\n# BAD: Iterating over rows\nstart = time.time()\nfor idx, row in df.iterrows():\n    df.loc[idx, 'new_column'] = row['col1'] + row['col2']\nprint(f\"iterrows: {time.time() - start:.2f}s\")  # 10+ seconds",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 91
    },
    "id": "de_de_python_skills_91"
  },
  {
    "text": "e\n\n# BAD: Iterating over rows\nstart = time.time()\nfor idx, row in df.iterrows():\n    df.loc[idx, 'new_column'] = row['col1'] + row['col2']\nprint(f\"iterrows: {time.time() - start:.2f}s\")  # 10+ seconds\n\n# BETTER: Apply function\nstart = time.time()\ndf['new_column'] = df.apply(lambda row: row['col1'] + row['col2'], axis=1)\nprint(f\"apply: {time.time() - start:.2f}s\")  # 2-3 seconds\n\n# BEST: Vectorized operations\nstart = time.time()\ndf['new_column'] = df['col1'] + df['col2']\nprint(f\"vectorized: {time.time() - start:.2f}s\")  # 0.01 seconds\n```\n\n### Memory Optimization\n```python\n# Reduce memory usage\ndef optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Optimize DataFrame memory usage\"\"\"\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinf",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 92
    },
    "id": "de_de_python_skills_92"
  },
  {
    "text": "bject':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n            \n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n    \n    return df\n\n# Before: 1.2GB\n# After: 300MB (75% reduction!)\n```\n\nThese Python skills help me build efficient, reliable data engineering solutions at Walgreens!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 93
    },
    "id": "de_de_python_skills_93"
  },
  {
    "text": " After: 300MB (75% reduction!)\n```\n\nThese Python skills help me build efficient, reliable data engineering solutions at Walgreens!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_python_skills.md",
      "file_name": "de_python_skills.md",
      "chunk_index": 94
    },
    "id": "de_de_python_skills_94"
  },
  {
    "text": "---\ntags: [data-engineer, krishna, experience, walgreens, azure, databricks, resume, profile]\npersona: de\n---\n\n# Krishna - Data Engineer Experience & Profile\n\n## Professional Summary\n\nI'm Krishna, a Data Engineer with 5+ years of experience building scalable data pipelines and data infrastructure. I specialize in Azure (Databricks, Data Factory, Synapse), PySpark, Delta Lake, and designing data architectures that can handle large-scale processing. My recent work at Walgreens includes processing 10TB+ monthly data, implementing medallion architectures, and reducing pipeline costs by 35% through optimization. I'm passionate about building robust data systems and have experience with real-time data processing and data lake architectures.\n\n## Current Role: Data Engineer at TCS (Walgreens, USA)\n**Feb 2022 – Present**\n\n### Project Overview\nSo at Walgreens, I'm working on this massive enterprise-scale data platform migration. It's one of America's largest pharmacy chains, so the scale is incr",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 95
    },
    "id": "de_de_krishna_experience_95"
  },
  {
    "text": "\n**Feb 2022 – Present**\n\n### Project Overview\nSo at Walgreens, I'm working on this massive enterprise-scale data platform migration. It's one of America's largest pharmacy chains, so the scale is incredible. I'm responsible for building and optimizing data pipelines that process healthcare, retail, pharmacy, and supply chain data.\n\n**What I've accomplished:**\n- Processing 10TB+ monthly data across multiple business domains\n- Supporting 500+ downstream analytics users\n- Reduced data pipeline costs by 35% through optimization\n- Improved data freshness from T+2 days to T+4 hours\n\n### Key Responsibilities\n\n**Data Pipeline Development**\nSo I built these end-to-end data pipelines using Databricks + PySpark, processing sales, pharmacy, customer loyalty, and supply chain data. What I did was implement a medallion architecture (Bronze → Silver → Gold) with proper schema enforcement, deduplication, and business logic. This really helped us organize the data flow and ensure quality at each layer.",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 96
    },
    "id": "de_de_krishna_experience_96"
  },
  {
    "text": "ment a medallion architecture (Bronze → Silver → Gold) with proper schema enforcement, deduplication, and business logic. This really helped us organize the data flow and ensure quality at each layer.\n\n**Architecture & Design**\nI designed scalable data lake architectures on Azure with Delta Lake format, which enabled ACID transactions, schema evolution, and time travel. I partitioned the data by date, state, and product category for optimal query performance. This was crucial because we were dealing with massive amounts of data and needed to make sure queries ran efficiently.\n\n**Performance Optimization**\nOne of my biggest wins was optimizing Spark jobs that were taking 4+ hours down to 45 minutes. I did this through partitioning strategies, broadcast joins, and caching. I also reduced cluster costs by right-sizing and using autoscaling effectively. The stakeholders were really happy with the performance improvements and cost savings.\n\n**Data Quality & Governance**\nI implemented data q",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 97
    },
    "id": "de_de_krishna_experience_97"
  },
  {
    "text": "ster costs by right-sizing and using autoscaling effectively. The stakeholders were really happy with the performance improvements and cost savings.\n\n**Data Quality & Governance**\nI implemented data quality frameworks with Unity Catalog for governance. I added validation checks, monitoring, and alerting to catch data issues before they impact business users. This was super important because data quality issues can really hurt business decisions.\n\n**Team Collaboration**\nI mentor offshore developers on PySpark best practices, code review standards, and Databricks workflows. I also collaborate with data analysts, BI teams, and business stakeholders to understand requirements. I believe in knowledge sharing and helping the team grow.\n\n### Technical Achievements\n\n**Pharmacy Data Pipeline:**\nSo we had this huge challenge with legacy Informatica jobs taking 8+ hours, which was blocking daily reporting. The pharmacists were getting frustrated because they couldn't get their analytics on time. ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 98
    },
    "id": "de_de_krishna_experience_98"
  },
  {
    "text": "we had this huge challenge with legacy Informatica jobs taking 8+ hours, which was blocking daily reporting. The pharmacists were getting frustrated because they couldn't get their analytics on time. What I did was rebuild the entire pipeline in PySpark with incremental processing and partition pruning. The result was amazing - runtime reduced to 45 minutes, enabling same-day analytics for pharmacists. They were really happy with the improvement.\n\n**Customer Loyalty Platform:**\nAnother major project was the customer loyalty platform where we had 50M+ customer records that needed daily refresh for marketing campaigns. The challenge was that the full load was taking 6 hours every day. I implemented Change Data Capture (CDC) with Delta Lake merge operations. This was a game-changer - we reduced full-load processing from 6 hours to 30 minutes incremental updates. The marketing team could now get fresh data much faster.\n\n**Supply Chain Analytics:**\nOne of my favorite projects was building r",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 99
    },
    "id": "de_de_krishna_experience_99"
  },
  {
    "text": "ed full-load processing from 6 hours to 30 minutes incremental updates. The marketing team could now get fresh data much faster.\n\n**Supply Chain Analytics:**\nOne of my favorite projects was building real-time inventory tracking across 9,000+ stores. The challenge was that we needed real-time visibility into inventory levels. I built a streaming pipeline with Structured Streaming + Delta Lake. The result was that we enabled real-time inventory dashboards and reduced stockouts by 15%. The operations team was thrilled with the real-time visibility.\n\n**Cost Optimization:**\nThis was a big win for the company. We had Databricks clusters costing $50K/month with poor utilization. The stakeholders were concerned about the costs. I implemented autoscaling, job clustering, and workload optimization. The result was that we reduced costs by 35% while actually improving performance. Everyone was happy with the cost savings and better performance.\n\n## Previous Experience\n\n### Data Engineer at CVS Hea",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 100
    },
    "id": "de_de_krishna_experience_100"
  },
  {
    "text": "The result was that we reduced costs by 35% while actually improving performance. Everyone was happy with the cost savings and better performance.\n\n## Previous Experience\n\n### Data Engineer at CVS Health (USA)\n**Jan 2021 – Jan 2022**\n\nAt CVS Health, I built demand forecasting pipelines in PySpark + TensorFlow, predicting sales and supply trends. This was really exciting because I got to work with both data engineering and machine learning. I improved procurement accuracy and saved $15M annually through better forecasting. The business impact was huge.\n\n**Key Projects:**\n- Engineered feature pipelines with dbt + Databricks for ML models\n- Tracked experiments with MLflow and automated retraining\n- Deployed models to production via Azure ML with drift monitoring\n- Cut data preparation time by 40% through pipeline automation\n\n### Data Science Intern at McKesson (USA)\n**May 2020 – Dec 2020**\n\nThis was my first real experience with ETL + ML scripts in Python. I reduced ingestion latency by 5",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 101
    },
    "id": "de_de_krishna_experience_101"
  },
  {
    "text": " 40% through pipeline automation\n\n### Data Science Intern at McKesson (USA)\n**May 2020 – Dec 2020**\n\nThis was my first real experience with ETL + ML scripts in Python. I reduced ingestion latency by 50%, which was a big win. I built regression + time series models forecasting patient demand, preventing supply mismatches and reducing stockouts by 22%. It was really satisfying to see the impact on patient care.\n\n### Software Developer at Inditek Pioneer Solutions (India)\n**2017 – 2019**\n\nThis was where I started my career. I built backend APIs and optimized SQL queries for ERP modules, improving response times by 35%. I also designed reporting modules for contracts and payments, reducing manual reconciliation workload. It was a great foundation for my data engineering career.\n\n## Technical Skills\n\n**Core Expertise:**\n- **Cloud Platforms:** Azure (Data Factory, Databricks, Synapse, ADLS, DevOps), AWS (S3, Glue, Redshift)\n- **Big Data:** PySpark, Databricks, Delta Lake, Apache Spark, distr",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 102
    },
    "id": "de_de_krishna_experience_102"
  },
  {
    "text": "kills\n\n**Core Expertise:**\n- **Cloud Platforms:** Azure (Data Factory, Databricks, Synapse, ADLS, DevOps), AWS (S3, Glue, Redshift)\n- **Big Data:** PySpark, Databricks, Delta Lake, Apache Spark, distributed computing\n- **Data Engineering:** ETL/ELT pipelines, data lakes, data warehousing, medallion architecture\n- **Programming:** Python (pandas, numpy), SQL (advanced), Scala, Java\n- **Databases:** SQL Server, PostgreSQL, Oracle, Snowflake, Delta Lake\n- **DevOps:** Azure DevOps, Git, CI/CD, Docker\n- **Data Governance:** Unity Catalog, data quality frameworks, monitoring\n\n**Specialized Knowledge:**\n- Medallion Architecture (Bronze/Silver/Gold layers)\n- Change Data Capture (CDC) patterns\n- Incremental data processing\n- Performance tuning & optimization\n- Real-time streaming pipelines\n- Data lake design patterns\n- Cost optimization strategies\n\n## Key Projects & Achievements\n\n**1. Enterprise Data Lake Migration (Walgreens)**\n- Migrated 200+ legacy Informatica workflows to Databricks\n- 10TB+",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 103
    },
    "id": "de_de_krishna_experience_103"
  },
  {
    "text": "ake design patterns\n- Cost optimization strategies\n\n## Key Projects & Achievements\n\n**1. Enterprise Data Lake Migration (Walgreens)**\n- Migrated 200+ legacy Informatica workflows to Databricks\n- 10TB+ monthly processing across sales, pharmacy, supply chain\n- Reduced total pipeline runtime from 20 hours to 6 hours\n- Enabled self-service analytics for 500+ business users\n\n**2. Real-Time Customer Analytics Platform**\n- Built streaming pipeline processing 50M+ daily events\n- Sub-5-minute latency from event to analytics dashboard\n- Enabled real-time personalization for marketing campaigns\n- Increased campaign ROI by 25%\n\n**3. Supply Chain Optimization**\n- Integrated data from suppliers, warehouses, stores, and pharmacies\n- Built predictive models for demand forecasting\n- Reduced inventory carrying costs by $8M annually\n- Improved on-shelf availability by 12%\n\n**4. Data Quality Framework**\n- Implemented automated data quality checks across 100+ tables\n- Reduced data incidents by 60%\n- Built ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 104
    },
    "id": "de_de_krishna_experience_104"
  },
  {
    "text": "ing costs by $8M annually\n- Improved on-shelf availability by 12%\n\n**4. Data Quality Framework**\n- Implemented automated data quality checks across 100+ tables\n- Reduced data incidents by 60%\n- Built monitoring dashboards for data ops team\n- Automated alerting for data freshness and accuracy issues\n\n## Interview Strengths\n\nWhen interviewers ask about my experience, I focus on:\n\n**Technical Depth:** I can explain PySpark optimization techniques, Delta Lake internals, partition strategies, and performance tuning with real examples from my work. I love diving deep into the technical details and showing how I've solved complex problems.\n\n**Business Impact:** I always tie technical work to business outcomes - how my pipelines enabled faster decisions, reduced costs, or improved customer experience. I believe that's what makes a good data engineer - understanding the business value of what we build.\n\n**Problem-Solving:** I share real challenges I've faced (data quality issues, performance bo",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 105
    },
    "id": "de_de_krishna_experience_105"
  },
  {
    "text": "nce. I believe that's what makes a good data engineer - understanding the business value of what we build.\n\n**Problem-Solving:** I share real challenges I've faced (data quality issues, performance bottlenecks, scalability problems) and how I solved them systematically. I think it's important to show that I can handle difficult situations and find solutions.\n\n**Collaboration:** I emphasize working with cross-functional teams - data analysts needing faster refreshes, business stakeholders wanting new metrics, and data scientists needing clean features. I believe collaboration is key to success in data engineering.\n\n**Continuous Learning:** I stay current with modern data stack (dbt, Fivetran, Airbyte), cloud innovations, and data mesh concepts. I've experimented with streaming architectures and real-time analytics. I'm always learning new things and staying up-to-date with the latest trends.\n\n## What Makes Me Different\n\n**End-to-End Ownership:** I don't just write code - I understand da",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 106
    },
    "id": "de_de_krishna_experience_106"
  },
  {
    "text": " and real-time analytics. I'm always learning new things and staying up-to-date with the latest trends.\n\n## What Makes Me Different\n\n**End-to-End Ownership:** I don't just write code - I understand data sources, design architecture, optimize performance, implement governance, and support production issues. I believe in taking ownership of the entire data pipeline lifecycle.\n\n**Business Mindset:** I ask \"Why?\" before building - understanding the business problem helps me design better solutions. I've worked directly with business users to understand their analytics needs. I think it's important to understand the business context behind what we're building.\n\n**Performance Focus:** I'm obsessed with making things fast and cost-effective. I've reduced pipeline costs by 35% while improving performance, which stakeholders love. I believe that performance optimization is not just about speed, but also about cost efficiency.\n\n**Quality-First:** I build robust pipelines with proper error handli",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 107
    },
    "id": "de_de_krishna_experience_107"
  },
  {
    "text": "ormance, which stakeholders love. I believe that performance optimization is not just about speed, but also about cost efficiency.\n\n**Quality-First:** I build robust pipelines with proper error handling, monitoring, and data quality checks. Production stability is critical for business trust. I've learned that it's better to build it right the first time than to fix issues later.\n\n**Mentorship:** I enjoy teaching others - I've mentored 10+ junior developers on PySpark, Databricks best practices, and data engineering principles. I believe in knowledge sharing and helping the team grow.\n\n## Career Goals\n\nI'm looking for opportunities to:\n- Work on large-scale data platforms (100TB+ data, 1000+ users)\n- Build real-time/streaming architectures\n- Lead data engineering teams\n- Architect modern data stacks (data mesh, data products)\n- Contribute to open-source data tools\n\nI'm passionate about building data infrastructure that empowers everyone in the organization to make data-driven decisions",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 108
    },
    "id": "de_de_krishna_experience_108"
  },
  {
    "text": "ata stacks (data mesh, data products)\n- Contribute to open-source data tools\n\nI'm passionate about building data infrastructure that empowers everyone in the organization to make data-driven decisions! I believe that good data engineering can really transform how companies operate and make decisions.\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_krishna_experience.md",
      "file_name": "de_krishna_experience.md",
      "chunk_index": 109
    },
    "id": "de_de_krishna_experience_109"
  },
  {
    "text": "---\ntags: [data-engineer, sql, query-optimization, window-functions, cte, performance-tuning]\npersona: de\n---\n\n# Advanced SQL - Krishna's Expertise\n\n## Introduction\n**Krishna's SQL Background:**\nSQL is my daily driver - from complex transformations to performance tuning. At Walgreens, I've optimized queries processing billions of rows, written complex window functions for analytics, and tuned performance for sub-second responses. Here's everything I know about advanced SQL.\n\n## Window Functions\n\n### Ranking Functions\n**Krishna's Real Use Cases:**\n\n**1. Customer Ranking by Spend:**\n```sql\n-- Rank customers within each segment\nSELECT\n    customer_id,\n    customer_name,\n    customer_segment,\n    lifetime_value,\n    ROW_NUMBER() OVER (PARTITION BY customer_segment ORDER BY lifetime_value DESC) as segment_rank,\n    RANK() OVER (PARTITION BY customer_segment ORDER BY lifetime_value DESC) as segment_rank_with_ties,\n    DENSE_RANK() OVER (PARTITION BY customer_segment ORDER BY lifetime_value D",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 110
    },
    "id": "de_de_sql_advanced_110"
  },
  {
    "text": " as segment_rank,\n    RANK() OVER (PARTITION BY customer_segment ORDER BY lifetime_value DESC) as segment_rank_with_ties,\n    DENSE_RANK() OVER (PARTITION BY customer_segment ORDER BY lifetime_value DESC) as dense_rank,\n    NTILE(4) OVER (PARTITION BY customer_segment ORDER BY lifetime_value DESC) as quartile\nFROM dim_customer\nWHERE is_active = 1;\n```\n\n**Business Question:** \"Who are our top 10 customers in each segment?\"\n```sql\nWITH ranked_customers AS (\n    SELECT\n        customer_id,\n        customer_segment,\n        lifetime_value,\n        ROW_NUMBER() OVER (PARTITION BY customer_segment ORDER BY lifetime_value DESC) as rn\n    FROM dim_customer\n)\nSELECT *\nFROM ranked_customers\nWHERE rn <= 10\nORDER BY customer_segment, rn;\n```\n\n**2. Product Performance Ranking:**\n```sql\n-- Top 5 products per category with running total\nSELECT\n    product_category,\n    product_name,\n    total_sales,\n    ROW_NUMBER() OVER (PARTITION BY product_category ORDER BY total_sales DESC) as category_rank,\n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 111
    },
    "id": "de_de_sql_advanced_111"
  },
  {
    "text": "s per category with running total\nSELECT\n    product_category,\n    product_name,\n    total_sales,\n    ROW_NUMBER() OVER (PARTITION BY product_category ORDER BY total_sales DESC) as category_rank,\n    SUM(total_sales) OVER (PARTITION BY product_category ORDER BY total_sales DESC) as running_total,\n    ROUND(100.0 * total_sales / SUM(total_sales) OVER (PARTITION BY product_category), 2) as pct_of_category\nFROM (\n    SELECT \n        p.product_category,\n        p.product_name,\n        SUM(s.sales_amount) as total_sales\n    FROM fct_sales s\n    JOIN dim_product p ON s.product_key = p.product_key\n    WHERE s.order_date >= DATEADD(month, -12, GETDATE())\n    GROUP BY p.product_category, p.product_name\n) product_sales\nWHERE category_rank <= 5;\n```\n\n### Aggregation Functions\n**Krishna's Analytics Patterns:**\n\n**1. Running Totals & Moving Averages:**\n```sql\n-- Daily sales with running total and 7-day moving average\nSELECT\n    order_date,\n    daily_sales,\n    SUM(daily_sales) OVER (ORDER BY order_",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 112
    },
    "id": "de_de_sql_advanced_112"
  },
  {
    "text": "tterns:**\n\n**1. Running Totals & Moving Averages:**\n```sql\n-- Daily sales with running total and 7-day moving average\nSELECT\n    order_date,\n    daily_sales,\n    SUM(daily_sales) OVER (ORDER BY order_date) as running_total,\n    AVG(daily_sales) OVER (ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7day,\n    daily_sales - AVG(daily_sales) OVER (ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as variance_from_avg\nFROM (\n    SELECT \n        order_date,\n        SUM(order_amount) as daily_sales\n    FROM fct_orders\n    WHERE order_date >= '2024-01-01'\n    GROUP BY order_date\n) daily_summary\nORDER BY order_date;\n```\n\n**2. Lead/Lag for Period Comparisons:**\n```sql\n-- Month-over-month growth analysis\nSELECT\n    order_month,\n    monthly_revenue,\n    LAG(monthly_revenue, 1) OVER (ORDER BY order_month) as prev_month_revenue,\n    monthly_revenue - LAG(monthly_revenue, 1) OVER (ORDER BY order_month) as mom_growth,\n    ROUND(100.0 * (monthly_revenue - LAG(mon",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 113
    },
    "id": "de_de_sql_advanced_113"
  },
  {
    "text": "hly_revenue, 1) OVER (ORDER BY order_month) as prev_month_revenue,\n    monthly_revenue - LAG(monthly_revenue, 1) OVER (ORDER BY order_month) as mom_growth,\n    ROUND(100.0 * (monthly_revenue - LAG(monthly_revenue, 1) OVER (ORDER BY order_month)) / \n          NULLIF(LAG(monthly_revenue, 1) OVER (ORDER BY order_month), 0), 2) as mom_growth_pct,\n    LAG(monthly_revenue, 12) OVER (ORDER BY order_month) as same_month_last_year,\n    ROUND(100.0 * (monthly_revenue - LAG(monthly_revenue, 12) OVER (ORDER BY order_month)) / \n          NULLIF(LAG(monthly_revenue, 12) OVER (ORDER BY order_month), 0), 2) as yoy_growth_pct\nFROM monthly_sales_summary\nORDER BY order_month DESC;\n```\n\n**3. First/Last Value:**\n```sql\n-- Customer first and most recent purchase\nSELECT\n    customer_id,\n    order_id,\n    order_date,\n    order_amount,\n    FIRST_VALUE(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as first_purchase_date,\n    LAST_VALUE(order_date) OVER (PARTITION BY customer_id ORDER BY order_",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 114
    },
    "id": "de_de_sql_advanced_114"
  },
  {
    "text": "ate,\n    order_amount,\n    FIRST_VALUE(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as first_purchase_date,\n    LAST_VALUE(order_date) OVER (PARTITION BY customer_id ORDER BY order_date \n        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as last_purchase_date,\n    DATEDIFF(day, \n        FIRST_VALUE(order_date) OVER (PARTITION BY customer_id ORDER BY order_date),\n        order_date) as days_since_first_purchase\nFROM fct_orders;\n```\n\n## Common Table Expressions (CTEs)\n\n### Recursive CTEs\n**Krishna's Hierarchical Data Queries:**\n\n**1. Organization Hierarchy:**\n```sql\n-- Build employee reporting chain\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: CEO (no manager)\n    SELECT \n        employee_id,\n        employee_name,\n        manager_id,\n        job_title,\n        1 as level,\n        CAST(employee_name AS VARCHAR(500)) as path\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: employees with managers\n ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 115
    },
    "id": "de_de_sql_advanced_115"
  },
  {
    "text": "_title,\n        1 as level,\n        CAST(employee_name AS VARCHAR(500)) as path\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: employees with managers\n    SELECT \n        e.employee_id,\n        e.employee_name,\n        e.manager_id,\n        e.job_title,\n        eh.level + 1,\n        CAST(eh.path || ' > ' || e.employee_name AS VARCHAR(500))\n    FROM employees e\n    INNER JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n)\nSELECT * FROM employee_hierarchy\nORDER BY path;\n```\n\n**2. Date Series Generation:**\n```sql\n-- Generate all dates for a year\nWITH RECURSIVE date_series AS (\n    SELECT CAST('2024-01-01' AS DATE) as date\n    UNION ALL\n    SELECT DATEADD(day, 1, date)\n    FROM date_series\n    WHERE date < '2024-12-31'\n)\nSELECT \n    date,\n    DATEPART(weekday, date) as day_of_week,\n    DATENAME(weekday, date) as day_name,\n    CASE WHEN DATEPART(weekday, date) IN (1, 7) THEN 1 ELSE 0 END as is_weekend\nFROM date_series;\n```\n\n### Compl",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 116
    },
    "id": "de_de_sql_advanced_116"
  },
  {
    "text": "e,\n    DATEPART(weekday, date) as day_of_week,\n    DATENAME(weekday, date) as day_name,\n    CASE WHEN DATEPART(weekday, date) IN (1, 7) THEN 1 ELSE 0 END as is_weekend\nFROM date_series;\n```\n\n### Complex CTEs for Analytics\n**Krishna's Multi-Stage Transformations:**\n\n```sql\n-- Customer cohort analysis with multiple CTEs\nWITH first_purchases AS (\n    SELECT\n        customer_id,\n        MIN(order_date) as cohort_date,\n        DATE_TRUNC('month', MIN(order_date)) as cohort_month\n    FROM fct_orders\n    GROUP BY customer_id\n),\nmonthly_activity AS (\n    SELECT\n        o.customer_id,\n        DATE_TRUNC('month', o.order_date) as activity_month,\n        SUM(o.order_amount) as monthly_spend,\n        COUNT(DISTINCT o.order_id) as monthly_orders\n    FROM fct_orders o\n    GROUP BY o.customer_id, DATE_TRUNC('month', o.order_date)\n),\ncohort_activity AS (\n    SELECT\n        f.cohort_month,\n        m.activity_month,\n        DATEDIFF('month', f.cohort_month, m.activity_month) as months_since_cohort,\n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 117
    },
    "id": "de_de_sql_advanced_117"
  },
  {
    "text": "UNC('month', o.order_date)\n),\ncohort_activity AS (\n    SELECT\n        f.cohort_month,\n        m.activity_month,\n        DATEDIFF('month', f.cohort_month, m.activity_month) as months_since_cohort,\n        COUNT(DISTINCT m.customer_id) as active_customers,\n        SUM(m.monthly_spend) as cohort_revenue\n    FROM first_purchases f\n    JOIN monthly_activity m ON f.customer_id = m.customer_id\n    GROUP BY f.cohort_month, m.activity_month\n),\ncohort_sizes AS (\n    SELECT\n        cohort_month,\n        COUNT(DISTINCT customer_id) as cohort_size\n    FROM first_purchases\n    GROUP BY cohort_month\n)\nSELECT\n    ca.cohort_month,\n    ca.months_since_cohort,\n    ca.active_customers,\n    cs.cohort_size,\n    ROUND(100.0 * ca.active_customers / cs.cohort_size, 2) as retention_rate,\n    ca.cohort_revenue,\n    ROUND(ca.cohort_revenue / ca.active_customers, 2) as revenue_per_active_customer\nFROM cohort_activity ca\nJOIN cohort_sizes cs ON ca.cohort_month = cs.cohort_month\nORDER BY ca.cohort_month, ca.months_s",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 118
    },
    "id": "de_de_sql_advanced_118"
  },
  {
    "text": "ROUND(ca.cohort_revenue / ca.active_customers, 2) as revenue_per_active_customer\nFROM cohort_activity ca\nJOIN cohort_sizes cs ON ca.cohort_month = cs.cohort_month\nORDER BY ca.cohort_month, ca.months_since_cohort;\n```\n\n## Performance Optimization\n\n### Index Strategies\n**Krishna's Indexing Rules:**\n\n**1. Clustered Index (Primary Key):**\n```sql\n-- Orders table - clustered on order_id\nCREATE CLUSTERED INDEX IX_Orders_OrderID \nON fct_orders (order_id);\n\n-- Dimensions - cluster on surrogate key\nCREATE CLUSTERED INDEX IX_Customer_CustomerKey\nON dim_customer (customer_key);\n```\n\n**2. Non-Clustered Indexes:**\n```sql\n-- Index frequently filtered columns\nCREATE NONCLUSTERED INDEX IX_Orders_CustomerDate\nON fct_orders (customer_id, order_date)\nINCLUDE (order_amount, order_status);\n\n-- Covering index for common query\nCREATE NONCLUSTERED INDEX IX_Orders_DateStatus\nON fct_orders (order_date, order_status)\nINCLUDE (customer_id, order_amount, product_id);\n```\n\n**3. Filtered Indexes:**\n```sql\n-- Index on",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 119
    },
    "id": "de_de_sql_advanced_119"
  },
  {
    "text": "r common query\nCREATE NONCLUSTERED INDEX IX_Orders_DateStatus\nON fct_orders (order_date, order_status)\nINCLUDE (customer_id, order_amount, product_id);\n```\n\n**3. Filtered Indexes:**\n```sql\n-- Index only active records\nCREATE NONCLUSTERED INDEX IX_Customers_Active\nON dim_customer (customer_id, customer_segment)\nWHERE is_active = 1;\n```\n\n### Query Optimization Techniques\n**Krishna's Performance Fixes:**\n\n**BAD Query (Table Scan):**\n```sql\n-- Slow: No indexes, OR logic, functions on columns\nSELECT *\nFROM fct_orders\nWHERE YEAR(order_date) = 2024\n   OR customer_id IS NULL\n   OR LOWER(order_status) = 'completed';\n```\n\n**GOOD Query (Index Seek):**\n```sql\n-- Fast: Uses indexes, sargable predicates\nSELECT \n    order_id, customer_id, order_date, order_amount\nFROM fct_orders WITH (INDEX(IX_Orders_DateStatus))\nWHERE order_date >= '2024-01-01'\n  AND order_date < '2025-01-01'\n  AND order_status = 'completed'\n  AND customer_id IS NOT NULL;\n```\n\n### Execution Plan Analysis\n**Krishna's Debugging Proces",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 120
    },
    "id": "de_de_sql_advanced_120"
  },
  {
    "text": "eStatus))\nWHERE order_date >= '2024-01-01'\n  AND order_date < '2025-01-01'\n  AND order_status = 'completed'\n  AND customer_id IS NOT NULL;\n```\n\n### Execution Plan Analysis\n**Krishna's Debugging Process:**\n\n```sql\n-- Enable actual execution plan\nSET STATISTICS TIME ON;\nSET STATISTICS IO ON;\n\n-- Run query\nSELECT \n    c.customer_segment,\n    COUNT(*) as customer_count,\n    SUM(o.order_amount) as total_revenue\nFROM dim_customer c\nLEFT JOIN fct_orders o ON c.customer_id = o.customer_id\nWHERE c.is_active = 1\n  AND o.order_date >= '2024-01-01'\nGROUP BY c.customer_segment;\n\n-- Check for:\n-- 1. Table scans → Add indexes\n-- 2. Hash matches → Consider merge join\n-- 3. High logical reads → Add covering index\n-- 4. Implicit conversions → Fix data types\n```\n\n## Complex Interview Questions\n\n### 1. Second Highest Salary\n**Krishna's Approach:**\n```sql\n-- Method 1: Using ROW_NUMBER\nWITH ranked_salaries AS (\n    SELECT \n        salary,\n        ROW_NUMBER() OVER (ORDER BY salary DESC) as rn\n    FROM emplo",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 121
    },
    "id": "de_de_sql_advanced_121"
  },
  {
    "text": " Highest Salary\n**Krishna's Approach:**\n```sql\n-- Method 1: Using ROW_NUMBER\nWITH ranked_salaries AS (\n    SELECT \n        salary,\n        ROW_NUMBER() OVER (ORDER BY salary DESC) as rn\n    FROM employees\n    GROUP BY salary\n)\nSELECT salary as second_highest_salary\nFROM ranked_salaries\nWHERE rn = 2;\n\n-- Method 2: Using OFFSET-FETCH\nSELECT DISTINCT salary as second_highest_salary\nFROM employees\nORDER BY salary DESC\nOFFSET 1 ROW\nFETCH NEXT 1 ROW ONLY;\n```\n\n### 2. Find Duplicates\n```sql\n-- Find duplicate customer records\nSELECT \n    customer_email,\n    COUNT(*) as duplicate_count,\n    STRING_AGG(CAST(customer_id AS VARCHAR), ', ') as duplicate_ids\nFROM dim_customer\nGROUP BY customer_email\nHAVING COUNT(*) > 1\nORDER BY duplicate_count DESC;\n```\n\n### 3. Running Total with Reset\n```sql\n-- Daily sales with running total that resets monthly\nSELECT\n    order_date,\n    daily_sales,\n    SUM(daily_sales) OVER (\n        PARTITION BY YEAR(order_date), MONTH(order_date)\n        ORDER BY order_date\n   ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 122
    },
    "id": "de_de_sql_advanced_122"
  },
  {
    "text": "ales with running total that resets monthly\nSELECT\n    order_date,\n    daily_sales,\n    SUM(daily_sales) OVER (\n        PARTITION BY YEAR(order_date), MONTH(order_date)\n        ORDER BY order_date\n    ) as monthly_running_total\nFROM daily_sales_summary;\n```\n\n### 4. Customer Churn Analysis\n```sql\n-- Identify churned customers (no order in 90 days)\nWITH last_orders AS (\n    SELECT\n        customer_id,\n        MAX(order_date) as last_order_date,\n        COUNT(*) as total_orders,\n        SUM(order_amount) as lifetime_value\n    FROM fct_orders\n    GROUP BY customer_id\n)\nSELECT\n    c.customer_id,\n    c.customer_name,\n    c.customer_segment,\n    lo.last_order_date,\n    DATEDIFF(day, lo.last_order_date, GETDATE()) as days_since_last_order,\n    lo.total_orders,\n    lo.lifetime_value,\n    CASE\n        WHEN DATEDIFF(day, lo.last_order_date, GETDATE()) > 180 THEN 'High Risk'\n        WHEN DATEDIFF(day, lo.last_order_date, GETDATE()) > 90 THEN 'At Risk'\n        ELSE 'Active'\n    END as churn_status\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 123
    },
    "id": "de_de_sql_advanced_123"
  },
  {
    "text": "   WHEN DATEDIFF(day, lo.last_order_date, GETDATE()) > 180 THEN 'High Risk'\n        WHEN DATEDIFF(day, lo.last_order_date, GETDATE()) > 90 THEN 'At Risk'\n        ELSE 'Active'\n    END as churn_status\nFROM dim_customer c\nLEFT JOIN last_orders lo ON c.customer_id = lo.customer_id\nWHERE c.is_active = 1\nORDER BY days_since_last_order DESC;\n```\n\n### 5. Gap Analysis\n```sql\n-- Find missing dates in order sequence\nWITH date_range AS (\n    SELECT CAST('2024-01-01' AS DATE) as date\n    UNION ALL\n    SELECT DATEADD(day, 1, date)\n    FROM date_range\n    WHERE date < '2024-12-31'\n),\nactual_dates AS (\n    SELECT DISTINCT order_date\n    FROM fct_orders\n    WHERE order_date >= '2024-01-01'\n)\nSELECT dr.date as missing_date\nFROM date_range dr\nLEFT JOIN actual_dates ad ON dr.date = ad.order_date\nWHERE ad.order_date IS NULL\n  AND DATEPART(weekday, dr.date) NOT IN (1, 7);  -- Exclude weekends\n```\n\n## Business-Focused SQL\n\n### Revenue Analysis\n**Krishna's Executive Queries:**\n\n```sql\n-- Comprehensive revenu",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 124
    },
    "id": "de_de_sql_advanced_124"
  },
  {
    "text": "er_date IS NULL\n  AND DATEPART(weekday, dr.date) NOT IN (1, 7);  -- Exclude weekends\n```\n\n## Business-Focused SQL\n\n### Revenue Analysis\n**Krishna's Executive Queries:**\n\n```sql\n-- Comprehensive revenue analysis for CFO\nSELECT\n    DATE_TRUNC('month', order_date) as month,\n    \n    -- Current month metrics\n    SUM(order_amount) as gross_revenue,\n    SUM(CASE WHEN order_status = 'returned' THEN order_amount ELSE 0 END) as returns,\n    SUM(order_amount) - SUM(CASE WHEN order_status = 'returned' THEN order_amount ELSE 0 END) as net_revenue,\n    COUNT(DISTINCT order_id) as order_count,\n    COUNT(DISTINCT customer_id) as unique_customers,\n    SUM(order_amount) / NULLIF(COUNT(DISTINCT customer_id), 0) as revenue_per_customer,\n    \n    -- YoY comparison\n    LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date)) as revenue_same_month_last_year,\n    ROUND(100.0 * (SUM(order_amount) - LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) /\n          NULLIF",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 125
    },
    "id": "de_de_sql_advanced_125"
  },
  {
    "text": " DATE_TRUNC('month', order_date)) as revenue_same_month_last_year,\n    ROUND(100.0 * (SUM(order_amount) - LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) /\n          NULLIF(LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date)), 0), 2) as yoy_growth_pct\n          \nFROM fct_orders\nWHERE order_date >= '2023-01-01'\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month DESC;\n```\n\nThis SQL expertise helps me solve complex business problems and optimize data pipelines at Walgreens!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_sql_advanced.md",
      "file_name": "de_sql_advanced.md",
      "chunk_index": 126
    },
    "id": "de_de_sql_advanced_126"
  },
  {
    "text": "---\ntags: [data-engineer, etl-migration, informatica, pyspark, modernization, legacy-migration]\npersona: de\n---\n\n# ETL Migration Patterns - Krishna's Experience\n\n## Introduction\n**Krishna's Migration Background:**\nAt Walgreens, I led the migration of 200+ Informatica workflows to Databricks PySpark. This transformation reduced processing time from 20 hours to 6 hours and cut costs by 40%. Here's everything I learned.\n\n## Why Migrate from Informatica\n\n### Challenges with Informatica\n**Krishna's Pain Points:**\n- **Cost:** Expensive licensing ($500K+ annually)\n- **Scalability:** Limited horizontal scaling\n- **Development:** GUI-based, hard to version control\n- **Maintenance:** Complex dependency management\n- **Performance:** Long processing times on large datasets\n- **Cloud:** Not cloud-native, requires VMs\n\n### Benefits of PySpark/Databricks\n**Krishna's Wins:**\n- **Cost:** 40-60% cheaper with cloud elasticity\n- **Scalability:** Auto-scaling clusters\n- **Development:** Code-based, Git-fri",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 127
    },
    "id": "de_de_etl_migration_patterns_127"
  },
  {
    "text": "e, requires VMs\n\n### Benefits of PySpark/Databricks\n**Krishna's Wins:**\n- **Cost:** 40-60% cheaper with cloud elasticity\n- **Scalability:** Auto-scaling clusters\n- **Development:** Code-based, Git-friendly\n- **Performance:** Distributed processing, 3-5x faster\n- **Cloud-Native:** Serverless, managed platform\n- **Skills:** Python/SQL easier to hire\n\n## Migration Patterns\n\n### Pattern 1: Source Qualifier → DataFrame Read\n**Informatica:**\n```\nSource Qualifier: ORDERS table\nFilter: ORDER_DATE >= $LastRunDate\nColumns: ORDER_ID, CUSTOMER_ID, ORDER_DATE, AMOUNT\n```\n\n**PySpark Equivalent:**\n```python\nfrom pyspark.sql import functions as F\n\n# Read from source\ndf_orders = (\n    spark.read\n    .format(\"jdbc\")\n    .option(\"url\", \"jdbc:oracle:thin:@//hostname:1521/service\")\n    .option(\"dbtable\", \"ORDERS\")\n    .option(\"user\", dbutils.secrets.get(\"scope\", \"db-user\"))\n    .option(\"password\", dbutils.secrets.get(\"scope\", \"db-password\"))\n    .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\n    .loa",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 128
    },
    "id": "de_de_etl_migration_patterns_128"
  },
  {
    "text": "RS\")\n    .option(\"user\", dbutils.secrets.get(\"scope\", \"db-user\"))\n    .option(\"password\", dbutils.secrets.get(\"scope\", \"db-password\"))\n    .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\n    .load()\n    \n    # Apply filter (pushdown to source)\n    .filter(F.col(\"ORDER_DATE\") >= last_run_date)\n    \n    # Select columns\n    .select(\"ORDER_ID\", \"CUSTOMER_ID\", \"ORDER_DATE\", \"AMOUNT\")\n)\n```\n\n### Pattern 2: Expression Transformation → withColumn\n**Informatica:**\n```\nExpression Transformation:\n- FULL_NAME = FIRST_NAME || ' ' || LAST_NAME\n- ORDER_YEAR = TO_CHAR(ORDER_DATE, 'YYYY')\n- DISCOUNT_PCT = (DISCOUNT_AMOUNT / ORDER_AMOUNT) * 100\n- IS_HIGH_VALUE = IIF(ORDER_AMOUNT > 1000, 'Y', 'N')\n```\n\n**PySpark Equivalent:**\n```python\ndf_transformed = (\n    df_orders\n    .withColumn(\"FULL_NAME\", F.concat(F.col(\"FIRST_NAME\"), F.lit(\" \"), F.col(\"LAST_NAME\")))\n    .withColumn(\"ORDER_YEAR\", F.year(\"ORDER_DATE\"))\n    .withColumn(\"DISCOUNT_PCT\", (F.col(\"DISCOUNT_AMOUNT\") / F.col(\"ORDER_AMOUNT\")) * 100)\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 129
    },
    "id": "de_de_etl_migration_patterns_129"
  },
  {
    "text": "cat(F.col(\"FIRST_NAME\"), F.lit(\" \"), F.col(\"LAST_NAME\")))\n    .withColumn(\"ORDER_YEAR\", F.year(\"ORDER_DATE\"))\n    .withColumn(\"DISCOUNT_PCT\", (F.col(\"DISCOUNT_AMOUNT\") / F.col(\"ORDER_AMOUNT\")) * 100)\n    .withColumn(\"IS_HIGH_VALUE\", F.when(F.col(\"ORDER_AMOUNT\") > 1000, \"Y\").otherwise(\"N\"))\n)\n```\n\n### Pattern 3: Aggregator → groupBy + agg\n**Informatica:**\n```\nAggregator Transformation:\nGroup By: CUSTOMER_ID, ORDER_MONTH\nAggregations:\n- ORDER_COUNT = COUNT(ORDER_ID)\n- TOTAL_AMOUNT = SUM(ORDER_AMOUNT)\n- AVG_AMOUNT = AVG(ORDER_AMOUNT)\n```\n\n**PySpark Equivalent:**\n```python\ndf_aggregated = (\n    df_orders\n    .groupBy(\"CUSTOMER_ID\", F.date_trunc(\"month\", \"ORDER_DATE\").alias(\"ORDER_MONTH\"))\n    .agg(\n        F.count(\"ORDER_ID\").alias(\"ORDER_COUNT\"),\n        F.sum(\"ORDER_AMOUNT\").alias(\"TOTAL_AMOUNT\"),\n        F.avg(\"ORDER_AMOUNT\").alias(\"AVG_AMOUNT\")\n    )\n)\n```\n\n### Pattern 4: Joiner → join\n**Informatica:**\n```\nJoiner Transformation:\nMaster: CUSTOMERS\nDetail: ORDERS\nJoin Type: INNER JOIN\nCo",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 130
    },
    "id": "de_de_etl_migration_patterns_130"
  },
  {
    "text": "UNT\"),\n        F.avg(\"ORDER_AMOUNT\").alias(\"AVG_AMOUNT\")\n    )\n)\n```\n\n### Pattern 4: Joiner → join\n**Informatica:**\n```\nJoiner Transformation:\nMaster: CUSTOMERS\nDetail: ORDERS\nJoin Type: INNER JOIN\nCondition: CUSTOMERS.CUSTOMER_ID = ORDERS.CUSTOMER_ID\n```\n\n**PySpark Equivalent:**\n```python\n# Read customers\ndf_customers = spark.read.format(\"delta\").load(\"/mnt/dim/customers\")\n\n# Join\ndf_joined = (\n    df_orders.alias(\"o\")\n    .join(\n        df_customers.alias(\"c\"),\n        on=F.col(\"o.CUSTOMER_ID\") == F.col(\"c.CUSTOMER_ID\"),\n        how=\"inner\"\n    )\n    .select(\n        \"o.ORDER_ID\",\n        \"o.ORDER_DATE\",\n        \"o.ORDER_AMOUNT\",\n        \"c.CUSTOMER_NAME\",\n        \"c.CUSTOMER_SEGMENT\"\n    )\n)\n```\n\n### Pattern 5: Lookup → broadcast join\n**Informatica:**\n```\nLookup Transformation:\nLookup Table: PRODUCT_DIM (small)\nLookup Condition: SOURCE.PRODUCT_ID = LOOKUP.PRODUCT_ID\nReturn: PRODUCT_NAME, PRODUCT_CATEGORY\nCache: Yes\n```\n\n**PySpark Equivalent:**\n```python\n# Read small lookup table\ndf_",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 131
    },
    "id": "de_de_etl_migration_patterns_131"
  },
  {
    "text": "able: PRODUCT_DIM (small)\nLookup Condition: SOURCE.PRODUCT_ID = LOOKUP.PRODUCT_ID\nReturn: PRODUCT_NAME, PRODUCT_CATEGORY\nCache: Yes\n```\n\n**PySpark Equivalent:**\n```python\n# Read small lookup table\ndf_products = spark.read.format(\"delta\").load(\"/mnt/dim/products\")\n\n# Broadcast join (efficient for small tables)\ndf_enriched = (\n    df_orders\n    .join(\n        F.broadcast(df_products),\n        \"PRODUCT_ID\",\n        \"left\"\n    )\n    .select(\n        \"ORDER_ID\",\n        \"ORDER_AMOUNT\",\n        \"PRODUCT_NAME\",\n        \"PRODUCT_CATEGORY\"\n    )\n)\n```\n\n### Pattern 6: Router → filter + union\n**Informatica:**\n```\nRouter Transformation:\nGroup 1 (HIGH_VALUE): ORDER_AMOUNT > 1000\nGroup 2 (MEDIUM_VALUE): ORDER_AMOUNT BETWEEN 100 AND 1000\nGroup 3 (LOW_VALUE): ORDER_AMOUNT < 100\nDefault: Reject\n```\n\n**PySpark Equivalent:**\n```python\n# Split data into groups\ndf_high = df_orders.filter(F.col(\"ORDER_AMOUNT\") > 1000).withColumn(\"VALUE_TIER\", F.lit(\"HIGH\"))\ndf_medium = df_orders.filter((F.col(\"ORDER_AMOUNT\"",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 132
    },
    "id": "de_de_etl_migration_patterns_132"
  },
  {
    "text": "ark Equivalent:**\n```python\n# Split data into groups\ndf_high = df_orders.filter(F.col(\"ORDER_AMOUNT\") > 1000).withColumn(\"VALUE_TIER\", F.lit(\"HIGH\"))\ndf_medium = df_orders.filter((F.col(\"ORDER_AMOUNT\") >= 100) & (F.col(\"ORDER_AMOUNT\") <= 1000)).withColumn(\"VALUE_TIER\", F.lit(\"MEDIUM\"))\ndf_low = df_orders.filter(F.col(\"ORDER_AMOUNT\") < 100).withColumn(\"VALUE_TIER\", F.lit(\"LOW\"))\n\n# Union back if needed\ndf_categorized = df_high.union(df_medium).union(df_low)\n\n# Or use single when/otherwise (more efficient)\ndf_categorized = (\n    df_orders\n    .withColumn(\n        \"VALUE_TIER\",\n        F.when(F.col(\"ORDER_AMOUNT\") > 1000, \"HIGH\")\n         .when(F.col(\"ORDER_AMOUNT\") >= 100, \"MEDIUM\")\n         .otherwise(\"LOW\")\n    )\n)\n```\n\n### Pattern 7: Sorter → orderBy\n**Informatica:**\n```\nSorter Transformation:\nSort Keys: CUSTOMER_ID (ASC), ORDER_DATE (DESC)\n```\n\n**PySpark Equivalent:**\n```python\ndf_sorted = (\n    df_orders\n    .orderBy(\n        F.col(\"CUSTOMER_ID\").asc(),\n        F.col(\"ORDER_DATE\").d",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 133
    },
    "id": "de_de_etl_migration_patterns_133"
  },
  {
    "text": "tion:\nSort Keys: CUSTOMER_ID (ASC), ORDER_DATE (DESC)\n```\n\n**PySpark Equivalent:**\n```python\ndf_sorted = (\n    df_orders\n    .orderBy(\n        F.col(\"CUSTOMER_ID\").asc(),\n        F.col(\"ORDER_DATE\").desc()\n    )\n)\n```\n\n### Pattern 8: Rank → window functions\n**Informatica:**\n```\nRank Transformation:\nGroup By: CUSTOMER_ID\nOrder By: ORDER_AMOUNT DESC\nRank Type: DENSE_RANK\n```\n\n**PySpark Equivalent:**\n```python\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"CUSTOMER_ID\").orderBy(F.desc(\"ORDER_AMOUNT\"))\n\ndf_ranked = (\n    df_orders\n    .withColumn(\"RANK\", F.dense_rank().over(window_spec))\n    .withColumn(\"ROW_NUMBER\", F.row_number().over(window_spec))\n)\n```\n\n### Pattern 9: Update Strategy → Merge/Upsert\n**Informatica:**\n```\nUpdate Strategy Transformation:\nInsert: FLAG_INSERT = 1\nUpdate: FLAG_UPDATE = 1\nDelete: FLAG_DELETE = 1\n```\n\n**PySpark Equivalent:**\n```python\nfrom delta.tables import DeltaTable\n\n# Read target table\ndelta_table = DeltaTable.forPath(spark, \"/mnt",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 134
    },
    "id": "de_de_etl_migration_patterns_134"
  },
  {
    "text": "INSERT = 1\nUpdate: FLAG_UPDATE = 1\nDelete: FLAG_DELETE = 1\n```\n\n**PySpark Equivalent:**\n```python\nfrom delta.tables import DeltaTable\n\n# Read target table\ndelta_table = DeltaTable.forPath(spark, \"/mnt/silver/customers\")\n\n# Merge (upsert)\n(\n    delta_table.alias(\"target\")\n    .merge(\n        df_source.alias(\"source\"),\n        \"target.CUSTOMER_ID = source.CUSTOMER_ID\"\n    )\n    .whenMatchedUpdate(\n        condition=\"source.UPDATED_AT > target.UPDATED_AT\",\n        set={\n            \"CUSTOMER_NAME\": \"source.CUSTOMER_NAME\",\n            \"CUSTOMER_SEGMENT\": \"source.CUSTOMER_SEGMENT\",\n            \"UPDATED_AT\": \"source.UPDATED_AT\"\n        }\n    )\n    .whenNotMatchedInsert(\n        values={\n            \"CUSTOMER_ID\": \"source.CUSTOMER_ID\",\n            \"CUSTOMER_NAME\": \"source.CUSTOMER_NAME\",\n            \"CUSTOMER_SEGMENT\": \"source.CUSTOMER_SEGMENT\",\n            \"CREATED_AT\": \"source.CREATED_AT\",\n            \"UPDATED_AT\": \"source.UPDATED_AT\"\n        }\n    )\n    .execute()\n)\n```\n\n## Real Migration ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 135
    },
    "id": "de_de_etl_migration_patterns_135"
  },
  {
    "text": "    \"CUSTOMER_SEGMENT\": \"source.CUSTOMER_SEGMENT\",\n            \"CREATED_AT\": \"source.CREATED_AT\",\n            \"UPDATED_AT\": \"source.UPDATED_AT\"\n        }\n    )\n    .execute()\n)\n```\n\n## Real Migration Example\n\n### Informatica Workflow (Before)\n**Complex Customer Data Pipeline:**\n```\nWorkflow: WF_CUSTOMER_DAILY\n├── Session: SQ_CUSTOMERS (Source Qualifier)\n├── Transformation: EXP_CLEAN (Expression - clean data)\n├── Transformation: LKP_SEGMENT (Lookup - get segment)\n├── Transformation: AGG_METRICS (Aggregator - calculate metrics)\n├── Transformation: JNR_ORDERS (Joiner - join with orders)\n├── Transformation: RTR_SPLIT (Router - split by segment)\n├── Session: TGT_GOLD_CUSTOMERS (Target - Gold table)\n└── Session: TGT_SILVER_CUSTOMERS (Target - Silver table)\n\nRuntime: 4 hours\n```\n\n### PySpark Equivalent (After)\n**Modernized Pipeline:**\n```python\n# Complete pipeline in PySpark\nfrom pyspark.sql import functions as F, Window\nfrom delta.tables import DeltaTable\n\ndef customer_daily_pipeline(executi",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 136
    },
    "id": "de_de_etl_migration_patterns_136"
  },
  {
    "text": "valent (After)\n**Modernized Pipeline:**\n```python\n# Complete pipeline in PySpark\nfrom pyspark.sql import functions as F, Window\nfrom delta.tables import DeltaTable\n\ndef customer_daily_pipeline(execution_date: str):\n    \"\"\"\n    Migrated customer pipeline\n    Runtime: 45 minutes (5x faster!)\n    \"\"\"\n    \n    # 1. Source Qualifier - Read customers\n    df_customers = (\n        spark.read\n        .format(\"jdbc\")\n        .option(\"url\", jdbc_url)\n        .option(\"dbtable\", \"CUSTOMERS\")\n        .option(\"fetchsize\", \"10000\")\n        .load()\n        .filter(F.col(\"UPDATED_DATE\") >= execution_date)\n    )\n    \n    # 2. Expression - Clean data\n    df_clean = (\n        df_customers\n        .withColumn(\"CUSTOMER_NAME\", F.trim(F.upper(F.col(\"CUSTOMER_NAME\"))))\n        .withColumn(\"EMAIL\", F.lower(F.trim(F.col(\"EMAIL\"))))\n        .withColumn(\"PHONE\", F.regexp_replace(\"PHONE\", \"[^0-9]\", \"\"))\n        .dropDuplicates([\"CUSTOMER_ID\"])\n    )\n    \n    # 3. Lookup - Get segment (broadcast join for small table",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 137
    },
    "id": "de_de_etl_migration_patterns_137"
  },
  {
    "text": ".col(\"EMAIL\"))))\n        .withColumn(\"PHONE\", F.regexp_replace(\"PHONE\", \"[^0-9]\", \"\"))\n        .dropDuplicates([\"CUSTOMER_ID\"])\n    )\n    \n    # 3. Lookup - Get segment (broadcast join for small table)\n    df_segments = spark.read.format(\"delta\").load(\"/mnt/dim/segments\")\n    \n    df_with_segment = (\n        df_clean\n        .join(F.broadcast(df_segments), \"SEGMENT_CODE\", \"left\")\n        .select(\"df_clean.*\", \"df_segments.SEGMENT_NAME\")\n    )\n    \n    # 4. Aggregator - Calculate metrics\n    df_orders = spark.read.format(\"delta\").load(\"/mnt/silver/orders\")\n    \n    df_metrics = (\n        df_orders\n        .groupBy(\"CUSTOMER_ID\")\n        .agg(\n            F.count(\"ORDER_ID\").alias(\"ORDER_COUNT\"),\n            F.sum(\"ORDER_AMOUNT\").alias(\"LIFETIME_VALUE\"),\n            F.max(\"ORDER_DATE\").alias(\"LAST_ORDER_DATE\")\n        )\n    )\n    \n    # 5. Joiner - Join with orders metrics\n    df_enriched = (\n        df_with_segment.alias(\"c\")\n        .join(df_metrics.alias(\"m\"), \"CUSTOMER_ID\", \"left\")\n ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 138
    },
    "id": "de_de_etl_migration_patterns_138"
  },
  {
    "text": "(\"LAST_ORDER_DATE\")\n        )\n    )\n    \n    # 5. Joiner - Join with orders metrics\n    df_enriched = (\n        df_with_segment.alias(\"c\")\n        .join(df_metrics.alias(\"m\"), \"CUSTOMER_ID\", \"left\")\n        .select(\n            \"c.*\",\n            \"m.ORDER_COUNT\",\n            \"m.LIFETIME_VALUE\",\n            \"m.LAST_ORDER_DATE\"\n        )\n    )\n    \n    # 6. Router - Split by segment (filter + write separately)\n    # Gold customers (high value)\n    df_gold = df_enriched.filter(F.col(\"LIFETIME_VALUE\") > 10000)\n    df_gold.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/gold/customers_premium\")\n    \n    # Silver customers (all customers)\n    df_enriched.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/silver/customers\")\n    \n    print(f\"✅ Processed {df_enriched.count():,} customers in {execution_date}\")\n\n# Run pipeline\ncustomer_daily_pipeline(\"2024-01-15\")\n```\n\n**Results:**\n- Runtime: 4 hours → 45 minutes (5.3x faster)\n- Cost: $500/month → $150/month (70% reduction)\n- Maintainability: ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 139
    },
    "id": "de_de_etl_migration_patterns_139"
  },
  {
    "text": "cution_date}\")\n\n# Run pipeline\ncustomer_daily_pipeline(\"2024-01-15\")\n```\n\n**Results:**\n- Runtime: 4 hours → 45 minutes (5.3x faster)\n- Cost: $500/month → $150/month (70% reduction)\n- Maintainability: 10 transformations → 1 Python script\n- Version Control: GUI changes → Git commits\n\n## Migration Best Practices\n\n### 1. Assess & Prioritize\n**Krishna's Assessment Framework:**\n```\nPriority Matrix:\nHigh Priority:\n- Critical business processes\n- Long-running workflows (>2 hours)\n- Frequently modified workflows\n- Simple logic (easier to migrate)\n\nLow Priority:\n- Infrequently run jobs\n- Complex custom transformations\n- Legacy workflows with unclear business logic\n```\n\n### 2. Test Thoroughly\n**Krishna's Testing Strategy:**\n```python\ndef validate_migration(informatica_output_path, pyspark_output_path):\n    \"\"\"\n    Compare Informatica vs PySpark output\n    \"\"\"\n    # Read both outputs\n    df_infa = spark.read.csv(informatica_output_path)\n    df_pyspark = spark.read.csv(pyspark_output_path)\n    \n   ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 140
    },
    "id": "de_de_etl_migration_patterns_140"
  },
  {
    "text": "):\n    \"\"\"\n    Compare Informatica vs PySpark output\n    \"\"\"\n    # Read both outputs\n    df_infa = spark.read.csv(informatica_output_path)\n    df_pyspark = spark.read.csv(pyspark_output_path)\n    \n    # Compare record counts\n    infa_count = df_infa.count()\n    pyspark_count = df_pyspark.count()\n    assert infa_count == pyspark_count, f\"Count mismatch: {infa_count} vs {pyspark_count}\"\n    \n    # Compare checksums\n    infa_checksum = df_infa.select(F.sum(\"AMOUNT\")).first()[0]\n    pyspark_checksum = df_pyspark.select(F.sum(\"AMOUNT\")).first()[0]\n    assert infa_checksum == pyspark_checksum, f\"Sum mismatch: {infa_checksum} vs {pyspark_checksum}\"\n    \n    # Sample row comparison\n    df_diff = df_infa.exceptAll(df_pyspark)\n    diff_count = df_diff.count()\n    \n    if diff_count > 0:\n        print(f\"⚠️  Found {diff_count} different rows\")\n        df_diff.show(20, False)\n        return False\n    \n    print(\"✅ Validation passed!\")\n    return True\n```\n\n### 3. Parallel Run Period\n**Krishna's Cuto",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 141
    },
    "id": "de_de_etl_migration_patterns_141"
  },
  {
    "text": "t(f\"⚠️  Found {diff_count} different rows\")\n        df_diff.show(20, False)\n        return False\n    \n    print(\"✅ Validation passed!\")\n    return True\n```\n\n### 3. Parallel Run Period\n**Krishna's Cutover Strategy:**\n```\nWeek 1-2: Run both Informatica and PySpark, compare outputs daily\nWeek 3-4: PySpark primary, Informatica backup\nWeek 5: Decommission Informatica\n```\n\n### 4. Documentation\n**Krishna's Migration Documentation:**\n```markdown\n# Migration: WF_CUSTOMER_DAILY\n\n## Source Workflow\n- **Informatica Folder:** PROD/CUSTOMER\n- **Workflow Name:** WF_CUSTOMER_DAILY\n- **Schedule:** Daily at 2 AM\n- **Dependencies:** WF_ORDERS_LOAD (must complete first)\n- **Runtime:** 4 hours\n- **Transformations:** 12\n\n## Target Pipeline\n- **Databricks Notebook:** /Pipelines/Customer/daily_load\n- **Cluster:** production-etl\n- **Schedule:** ADF trigger, daily at 3 AM\n- **Dependencies:** pl_orders_load (ADF)\n- **Runtime:** 45 minutes\n- **Logic:** Single PySpark script (150 lines)\n\n## Key Changes\n1. Source Q",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 142
    },
    "id": "de_de_etl_migration_patterns_142"
  },
  {
    "text": " production-etl\n- **Schedule:** ADF trigger, daily at 3 AM\n- **Dependencies:** pl_orders_load (ADF)\n- **Runtime:** 45 minutes\n- **Logic:** Single PySpark script (150 lines)\n\n## Key Changes\n1. Source Qualifier → JDBC read with pushdown predicates\n2. 5 Expression transforms → chained withColumn operations\n3. Lookup → broadcast join (10x faster)\n4. Aggregator → groupBy + agg\n5. Update Strategy → Delta Lake merge\n\n## Validation Results\n- Record count match: ✅\n- Sum validation: ✅\n- Sample comparison: ✅\n- Business user sign-off: ✅ (Jane Doe, 2024-01-20)\n\n## Rollback Plan\nIf issues found:\n1. Disable ADF pipeline\n2. Re-enable Informatica workflow\n3. Investigate PySpark issues\n4. Fix and retest\n\n## Sign-off\n- Data Engineer: Krishna (2024-01-15)\n- Business Owner: Jane Doe (2024-01-20)\n- Production Cutover: 2024-01-25\n```\n\nThis migration expertise helped Walgreens modernize their data platform and achieve 40% cost savings!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 143
    },
    "id": "de_de_etl_migration_patterns_143"
  },
  {
    "text": "utover: 2024-01-25\n```\n\nThis migration expertise helped Walgreens modernize their data platform and achieve 40% cost savings!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_etl_migration_patterns.md",
      "file_name": "de_etl_migration_patterns.md",
      "chunk_index": 144
    },
    "id": "de_de_etl_migration_patterns_144"
  },
  {
    "text": "---\ntags: [data-engineer, aws, s3, glue, redshift, athena, emr, lambda]\npersona: de\n---\n\n# AWS Cloud for Data Engineering - Krishna's Experience\n\n## Introduction\n**Krishna's AWS Background:**\nWhile my primary platform is Azure, I've worked with AWS data services on several projects. Here's what I know about building data pipelines on AWS.\n\n## Amazon S3\n\n### Data Lake Organization\n**Krishna's S3 Structure:**\n```\ns3://walgreens-data-lake/\n\n├── raw/                                # Landing zone\n│   ├── salesforce/\n│   │   └── 2024-01-15/\n│   ├── database_extracts/\n│   │   └── orders/2024-01-15/\n│   └── api_data/\n│\n├── processed/                          # Cleaned data\n│   ├── customers/\n│   │   └── year=2024/month=01/day=15/\n│   ├── orders/\n│   └── products/\n│\n├── analytics/                          # Business-ready\n│   ├── daily_sales/\n│   ├── customer_metrics/\n│   └── product_performance/\n│\n└── archive/                            # Historical data\n    └── 2023/\n```\n\n### S3 Performance O",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 145
    },
    "id": "de_de_aws_cloud_145"
  },
  {
    "text": "        # Business-ready\n│   ├── daily_sales/\n│   ├── customer_metrics/\n│   └── product_performance/\n│\n└── archive/                            # Historical data\n    └── 2023/\n```\n\n### S3 Performance Optimization\n**Krishna's Best Practices:**\n\n**1. Partitioning Strategy:**\n```python\n# Write partitioned data\ndf.write \\\n    .partitionBy(\"year\", \"month\", \"day\") \\\n    .mode(\"overwrite\") \\\n    .parquet(\"s3://bucket/processed/orders/\")\n\n# Read specific partition (fast)\ndf = spark.read.parquet(\"s3://bucket/processed/orders/year=2024/month=01/\")\n```\n\n**2. File Formats:**\n```python\n# Parquet (columnar, compressed) - BEST for analytics\ndf.write.mode(\"overwrite\").parquet(\"s3://bucket/data.parquet\")\n\n# ORC (columnar) - Good for Hive/Spark\ndf.write.mode(\"overwrite\").orc(\"s3://bucket/data.orc\")\n\n# CSV (human-readable) - Use only for raw data\ndf.write.mode(\"overwrite\").csv(\"s3://bucket/data.csv\")\n```\n\n**3. Lifecycle Policies:**\n```json\n{\n  \"Rules\": [\n    {\n      \"Id\": \"archive-old-raw-data\",\n      \"St",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 146
    },
    "id": "de_de_aws_cloud_146"
  },
  {
    "text": "uman-readable) - Use only for raw data\ndf.write.mode(\"overwrite\").csv(\"s3://bucket/data.csv\")\n```\n\n**3. Lifecycle Policies:**\n```json\n{\n  \"Rules\": [\n    {\n      \"Id\": \"archive-old-raw-data\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\n        \"Prefix\": \"raw/\"\n      },\n      \"Transitions\": [\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"STANDARD_IA\"\n        },\n        {\n          \"Days\": 365,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    },\n    {\n      \"Id\": \"delete-temp-data\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\n        \"Prefix\": \"temp/\"\n      },\n      \"Expiration\": {\n        \"Days\": 7\n      }\n    }\n  ]\n}\n```\n\n## AWS Glue\n\n### Glue ETL Jobs\n**Krishna's Glue Job Pattern:**\n\n```python\n# Glue Job: transform_orders.py\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\n\n# I",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 147
    },
    "id": "de_de_aws_cloud_147"
  },
  {
    "text": " *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\n\n# Initialize\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 'execution_date'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Read from S3\ndf_raw = spark.read.parquet(f\"s3://bucket/raw/orders/date={args['execution_date']}\")\n\n# Transform\ndf_clean = (\n    df_raw\n    .filter(F.col(\"order_status\") == \"completed\")\n    .withColumn(\"order_amount\", F.col(\"order_amount\").cast(\"decimal(18,2)\"))\n    .dropDuplicates([\"order_id\"])\n    .withColumn(\"processed_at\", F.current_timestamp())\n)\n\n# Write to processed\ndf_clean.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"order_date\") \\\n    .parquet(f\"s3://bucket/processed/orders/\")\n\njob.commit()\n```\n\n### Glue Data Catalog\n**Krishna's Catalog Management:**\n\n```pyt",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 148
    },
    "id": "de_de_aws_cloud_148"
  },
  {
    "text": "lean.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"order_date\") \\\n    .parquet(f\"s3://bucket/processed/orders/\")\n\njob.commit()\n```\n\n### Glue Data Catalog\n**Krishna's Catalog Management:**\n\n```python\n# Create Glue Crawler via boto3\nimport boto3\n\nglue = boto3.client('glue')\n\nresponse = glue.create_crawler(\n    Name='orders-crawler',\n    Role='AWSGlueServiceRole',\n    DatabaseName='walgreens_data',\n    Description='Crawl orders data',\n    Targets={\n        'S3Targets': [\n            {\n                'Path': 's3://bucket/processed/orders/',\n                'Exclusions': ['*.tmp', '*.log']\n            }\n        ]\n    },\n    Schedule='cron(0 8 * * ? *)',  # Daily at 8 AM\n    SchemaChangePolicy={\n        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n        'DeleteBehavior': 'LOG'\n    }\n)\n```\n\n## Amazon Redshift\n\n### Redshift Architecture\n**Krishna's Redshift Setup:**\n\n**1. Create Tables with Distribution:**\n```sql\n-- Fact table with KEY distribution\nCREATE TABLE fct_orders (\n    order_id VAR",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 149
    },
    "id": "de_de_aws_cloud_149"
  },
  {
    "text": " Amazon Redshift\n\n### Redshift Architecture\n**Krishna's Redshift Setup:**\n\n**1. Create Tables with Distribution:**\n```sql\n-- Fact table with KEY distribution\nCREATE TABLE fct_orders (\n    order_id VARCHAR(50) NOT NULL,\n    customer_id VARCHAR(50) DISTKEY,    -- Distribute by join key\n    order_date DATE SORTKEY,             -- Sort for date queries\n    order_amount DECIMAL(18,2),\n    product_id VARCHAR(50),\n    created_at TIMESTAMP\n);\n\n-- Dimension with ALL distribution\nCREATE TABLE dim_customer (\n    customer_id VARCHAR(50) NOT NULL SORTKEY,\n    customer_name VARCHAR(200),\n    customer_segment VARCHAR(50),\n    city VARCHAR(100),\n    state VARCHAR(50)\n)\nDISTSTYLE ALL;  -- Replicate to all nodes\n```\n\n**2. Load Data from S3:**\n```sql\n-- COPY command (fastest way to load)\nCOPY fct_orders\nFROM 's3://bucket/processed/orders/'\nIAM_ROLE 'arn:aws:iam::123456789:role/RedshiftRole'\nFORMAT AS PARQUET\nCOMPUPDATE ON\nSTATUPDATE ON;\n```\n\n**3. Unload Data to S3:**\n```sql\n-- Export query results to S3\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 150
    },
    "id": "de_de_aws_cloud_150"
  },
  {
    "text": " 's3://bucket/processed/orders/'\nIAM_ROLE 'arn:aws:iam::123456789:role/RedshiftRole'\nFORMAT AS PARQUET\nCOMPUPDATE ON\nSTATUPDATE ON;\n```\n\n**3. Unload Data to S3:**\n```sql\n-- Export query results to S3\nUNLOAD (\n    'SELECT * FROM fct_orders WHERE order_date >= \\'2024-01-01\\''\n)\nTO 's3://bucket/exports/orders_'\nIAM_ROLE 'arn:aws:iam::123456789:role/RedshiftRole'\nFORMAT AS PARQUET\nPARALLEL ON\nALLOWOVERWRITE;\n```\n\n### Redshift Performance\n**Krishna's Optimization:**\n\n**1. Vacuum and Analyze:**\n```sql\n-- Reclaim space and resort\nVACUUM fct_orders;\n\n-- Update statistics for query planner\nANALYZE fct_orders;\n\n-- Vacuum specific table with options\nVACUUM DELETE ONLY fct_orders TO 75 PERCENT;\n```\n\n**2. Workload Management (WLM):**\n```json\n{\n  \"query_concurrency\": 5,\n  \"query_group\": [\n    {\n      \"name\": \"etl\",\n      \"query_group_wildcard\": \"etl_*\",\n      \"memory_percent\": 40,\n      \"query_concurrency\": 2\n    },\n    {\n      \"name\": \"reporting\",\n      \"query_group_wildcard\": \"report_*\",\n      \"me",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 151
    },
    "id": "de_de_aws_cloud_151"
  },
  {
    "text": "ame\": \"etl\",\n      \"query_group_wildcard\": \"etl_*\",\n      \"memory_percent\": 40,\n      \"query_concurrency\": 2\n    },\n    {\n      \"name\": \"reporting\",\n      \"query_group_wildcard\": \"report_*\",\n      \"memory_percent\": 40,\n      \"query_concurrency\": 3\n    }\n  ]\n}\n```\n\n## Amazon Athena\n\n### Querying S3 with Athena\n**Krishna's Athena Patterns:**\n\n**1. Create External Tables:**\n```sql\n-- Create table on S3 data\nCREATE EXTERNAL TABLE IF NOT EXISTS orders (\n    order_id STRING,\n    customer_id STRING,\n    order_date DATE,\n    order_amount DECIMAL(18,2),\n    product_id STRING\n)\nPARTITIONED BY (year INT, month INT, day INT)\nSTORED AS PARQUET\nLOCATION 's3://bucket/processed/orders/';\n\n-- Add partitions\nMSCK REPAIR TABLE orders;\n\n-- Or add specific partition\nALTER TABLE orders ADD IF NOT EXISTS\nPARTITION (year=2024, month=1, day=15)\nLOCATION 's3://bucket/processed/orders/year=2024/month=01/day=15/';\n```\n\n**2. Query Optimization:**\n```sql\n-- BAD: Scans entire table\nSELECT * FROM orders WHERE order_d",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 152
    },
    "id": "de_de_aws_cloud_152"
  },
  {
    "text": "ON (year=2024, month=1, day=15)\nLOCATION 's3://bucket/processed/orders/year=2024/month=01/day=15/';\n```\n\n**2. Query Optimization:**\n```sql\n-- BAD: Scans entire table\nSELECT * FROM orders WHERE order_date = '2024-01-15';\n\n-- GOOD: Uses partitions\nSELECT * FROM orders \nWHERE year = 2024 AND month = 1 AND day = 15;\n\n-- BETTER: Limit columns\nSELECT order_id, customer_id, order_amount\nFROM orders\nWHERE year = 2024 AND month = 1 AND day = 15;\n```\n\n**3. CTAS for Performance:**\n```sql\n-- Create optimized table from query\nCREATE TABLE orders_summary\nWITH (\n    format = 'PARQUET',\n    parquet_compression = 'SNAPPY',\n    partitioned_by = ARRAY['year', 'month']\n) AS\nSELECT\n    year,\n    month,\n    customer_id,\n    COUNT(*) as order_count,\n    SUM(order_amount) as total_spent\nFROM orders\nGROUP BY year, month, customer_id;\n```\n\n## AWS Lambda for Data Pipelines\n\n### Event-Driven Processing\n**Krishna's Lambda Functions:**\n\n```python\n# Lambda: validate_new_file.py\nimport json\nimport boto3\nimport pandas",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 153
    },
    "id": "de_de_aws_cloud_153"
  },
  {
    "text": " month, customer_id;\n```\n\n## AWS Lambda for Data Pipelines\n\n### Event-Driven Processing\n**Krishna's Lambda Functions:**\n\n```python\n# Lambda: validate_new_file.py\nimport json\nimport boto3\nimport pandas as pd\nfrom io import BytesIO\n\ns3 = boto3.client('s3')\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Triggered when new file lands in S3\n    Validates data before processing\n    \"\"\"\n    # Get file info from S3 event\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n    \n    print(f\"Validating: s3://{bucket}/{key}\")\n    \n    try:\n        # Download and read file\n        obj = s3.get_object(Bucket=bucket, Key=key)\n        df = pd.read_csv(BytesIO(obj['Body'].read()))\n        \n        # Validation checks\n        errors = []\n        \n        # Check required columns\n        required_cols = ['order_id', 'customer_id', 'order_date', 'order_amount']\n        missing_cols = [col for col in required_cols if col ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 154
    },
    "id": "de_de_aws_cloud_154"
  },
  {
    "text": "    errors = []\n        \n        # Check required columns\n        required_cols = ['order_id', 'customer_id', 'order_date', 'order_amount']\n        missing_cols = [col for col in required_cols if col not in df.columns]\n        if missing_cols:\n            errors.append(f\"Missing columns: {missing_cols}\")\n        \n        # Check for nulls\n        null_counts = df[required_cols].isnull().sum()\n        if null_counts.any():\n            errors.append(f\"Null values found: {null_counts.to_dict()}\")\n        \n        # Check data types\n        if not pd.api.types.is_numeric_dtype(df['order_amount']):\n            errors.append(\"order_amount must be numeric\")\n        \n        if errors:\n            # Move to error folder\n            error_key = key.replace('raw/', 'errors/')\n            s3.copy_object(\n                Bucket=bucket,\n                CopySource={'Bucket': bucket, 'Key': key},\n                Key=error_key\n            )\n            \n            # Send alert\n            sns.publish",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 155
    },
    "id": "de_de_aws_cloud_155"
  },
  {
    "text": "ect(\n                Bucket=bucket,\n                CopySource={'Bucket': bucket, 'Key': key},\n                Key=error_key\n            )\n            \n            # Send alert\n            sns.publish(\n                TopicArn='arn:aws:sns:us-east-1:123456789:data-validation-alerts',\n                Subject='Data Validation Failed',\n                Message=f\"File: {key}\\nErrors: {errors}\"\n            )\n            \n            return {'statusCode': 400, 'body': json.dumps({'errors': errors})}\n        \n        else:\n            # Move to validated folder\n            validated_key = key.replace('raw/', 'validated/')\n            s3.copy_object(\n                Bucket=bucket,\n                CopySource={'Bucket': bucket, 'Key': key},\n                Key=validated_key\n            )\n            s3.delete_object(Bucket=bucket, Key=key)\n            \n            return {'statusCode': 200, 'body': json.dumps({'status': 'validated'})}\n            \n    except Exception as e:\n        print(f\"Error:",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 156
    },
    "id": "de_de_aws_cloud_156"
  },
  {
    "text": "s3.delete_object(Bucket=bucket, Key=key)\n            \n            return {'statusCode': 200, 'body': json.dumps({'status': 'validated'})}\n            \n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        raise\n```\n\n## AWS Step Functions\n\n### Orchestrating Data Pipelines\n**Krishna's Step Functions Workflow:**\n\n```json\n{\n  \"Comment\": \"Daily ETL Pipeline\",\n  \"StartAt\": \"ValidateFiles\",\n  \"States\": {\n    \"ValidateFiles\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:validate-files\",\n      \"Next\": \"RunGlueJob\"\n    },\n    \"RunGlueJob\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\",\n      \"Parameters\": {\n        \"JobName\": \"transform-orders\",\n        \"Arguments\": {\n          \"--execution_date.$\": \"$.execution_date\"\n        }\n      },\n      \"Next\": \"LoadToRedshift\"\n    },\n    \"LoadToRedshift\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:load-redshift\",\n      ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 157
    },
    "id": "de_de_aws_cloud_157"
  },
  {
    "text": "ution_date\"\n        }\n      },\n      \"Next\": \"LoadToRedshift\"\n    },\n    \"LoadToRedshift\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:load-redshift\",\n      \"Next\": \"RefreshViews\"\n    },\n    \"RefreshViews\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:refresh-views\",\n      \"Next\": \"SendNotification\",\n      \"Catch\": [{\n        \"ErrorEquals\": [\"States.ALL\"],\n        \"Next\": \"HandleError\"\n      }]\n    },\n    \"SendNotification\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::sns:publish\",\n      \"Parameters\": {\n        \"TopicArn\": \"arn:aws:sns:us-east-1:123456789:pipeline-success\",\n        \"Message\": \"Pipeline completed successfully\"\n      },\n      \"End\": true\n    },\n    \"HandleError\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::sns:publish\",\n      \"Parameters\": {\n        \"TopicArn\": \"arn:aws:sns:us-east-1:123456789:pipeline-failure\",\n        \"Message.$\": \"$.Error\"\n      },\n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 158
    },
    "id": "de_de_aws_cloud_158"
  },
  {
    "text": "e\": \"Task\",\n      \"Resource\": \"arn:aws:states:::sns:publish\",\n      \"Parameters\": {\n        \"TopicArn\": \"arn:aws:sns:us-east-1:123456789:pipeline-failure\",\n        \"Message.$\": \"$.Error\"\n      },\n      \"End\": true\n    }\n  }\n}\n```\n\n## Cost Optimization\n\n**Krishna's AWS Cost Strategies:**\n\n**1. S3 Storage Classes:**\n- Standard: Frequently accessed data\n- Intelligent-Tiering: Unknown access patterns (auto-optimize)\n- Standard-IA: Infrequent access (30+ days)\n- Glacier: Archive (90+ days)\n\n**2. Glue Job Optimization:**\n```python\n# Use Glue job bookmarks (avoid reprocessing)\njob.init(args['JOB_NAME'], args)\njob.commit()\n\n# Use Glue Flex for non-urgent jobs (50% cheaper)\nglue.start_job_run(\n    JobName='my-job',\n    Arguments={\n        '--job-bookmark-option': 'job-bookmark-enable',\n        '--enable-glue-datacatalog': 'true'\n    }\n)\n```\n\n**3. Redshift Reserved Nodes:**\n- 1-year commitment: 40% savings\n- 3-year commitment: 65% savings\n\n**4. Athena Query Optimization:**\n```sql\n-- Partition pr",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 159
    },
    "id": "de_de_aws_cloud_159"
  },
  {
    "text": "nable-glue-datacatalog': 'true'\n    }\n)\n```\n\n**3. Redshift Reserved Nodes:**\n- 1-year commitment: 40% savings\n- 3-year commitment: 65% savings\n\n**4. Athena Query Optimization:**\n```sql\n-- Partition pruning saves money\n-- BAD: Scans 1TB, costs $5\nSELECT * FROM large_table WHERE date = '2024-01-15';\n\n-- GOOD: Scans 10GB, costs $0.05\nSELECT * FROM large_table \nWHERE year = 2024 AND month = 1 AND day = 15;\n```\n\nMy AWS experience complements my Azure expertise, making me versatile across cloud platforms!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_aws_cloud.md",
      "file_name": "de_aws_cloud.md",
      "chunk_index": 160
    },
    "id": "de_de_aws_cloud_160"
  },
  {
    "text": "---\ntags: [data-engineer, pipeline-architecture, medallion-architecture, etl, elt, data-lake]\npersona: de\n---\n\n# Data Pipeline Architecture - Krishna's Design Patterns\n\n## Introduction\n**Krishna's Architecture Philosophy:**\nAt Walgreens, I've designed and implemented modern data architectures processing 10TB+ monthly. Good architecture isn't just about technology - it's about reliability, scalability, and making data accessible to business users.\n\n## Medallion Architecture\n\n### Overview\n**Krishna's Implementation:**\nMedallion architecture is the foundation of our Walgreens data platform - Bronze (raw), Silver (cleaned), Gold (business).\n\n**Benefits I've Seen:**\n- Clear data lineage and quality progression\n- Separation of concerns (ingestion vs transformation)\n- Easy rollback and recovery\n- Scalable and maintainable\n\n### Bronze Layer (Raw/Landing)\n**Purpose:** Ingest data as-is, no transformation\n\n```python\n# Bronze Layer - Raw Ingestion\ndef bronze_ingestion(source_path: str, bronze_pat",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 161
    },
    "id": "de_de_data_pipeline_architecture_161"
  },
  {
    "text": " Scalable and maintainable\n\n### Bronze Layer (Raw/Landing)\n**Purpose:** Ingest data as-is, no transformation\n\n```python\n# Bronze Layer - Raw Ingestion\ndef bronze_ingestion(source_path: str, bronze_path: str, execution_date: str):\n    \"\"\"\n    Ingest raw data with minimal transformation\n    - Schema-on-read (no enforcement)\n    - Keep all source columns\n    - Add ingestion metadata\n    \"\"\"\n    from pyspark.sql import functions as F\n    \n    df_raw = (\n        spark.read\n        .format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .load(f\"{source_path}/date={execution_date}\")\n        \n        # Add metadata only\n        .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n        .withColumn(\"_ingestion_date\", F.lit(execution_date))\n        .withColumn(\"_source_file\", F.input_file_name())\n    )\n    \n    # Write to Bronze (append mode)\n    df_raw.write \\\n        .format(\"delta\") \\\n        .mode(\"append\") \\\n        .partitionBy(\"_ingestion_da",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 162
    },
    "id": "de_de_data_pipeline_architecture_162"
  },
  {
    "text": "thColumn(\"_source_file\", F.input_file_name())\n    )\n    \n    # Write to Bronze (append mode)\n    df_raw.write \\\n        .format(\"delta\") \\\n        .mode(\"append\") \\\n        .partitionBy(\"_ingestion_date\") \\\n        .save(bronze_path)\n```\n\n### Silver Layer (Cleaned/Standardized)\n**Purpose:** Clean, validate, standardize data\n\n```python\n# Silver Layer - Data Quality & Standardization\ndef silver_transformation(bronze_path: str, silver_path: str, execution_date: str):\n    \"\"\"\n    Apply data quality rules and standardization\n    - Schema enforcement\n    - Data type conversions\n    - Deduplication\n    - Validation\n    - Business rules\n    \"\"\"\n    from pyspark.sql import functions as F, Window\n    \n    df_bronze = spark.read.format(\"delta\").load(f\"{bronze_path}/_ingestion_date={execution_date}\")\n    \n    # Data quality transformation\n    df_silver = (\n        df_bronze\n        \n        # Schema enforcement & type casting\n        .select(\n            F.col(\"order_id\").cast(\"string\"),\n         ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 163
    },
    "id": "de_de_data_pipeline_architecture_163"
  },
  {
    "text": "    \n    # Data quality transformation\n    df_silver = (\n        df_bronze\n        \n        # Schema enforcement & type casting\n        .select(\n            F.col(\"order_id\").cast(\"string\"),\n            F.col(\"customer_id\").cast(\"string\"),\n            F.to_date(\"order_date\", \"yyyy-MM-dd\").alias(\"order_date\"),\n            F.col(\"order_amount\").cast(\"decimal(18,2)\"),\n            F.col(\"order_status\").cast(\"string\")\n        )\n        \n        # Data cleaning\n        .filter(F.col(\"order_id\").isNotNull())\n        .filter(F.col(\"order_amount\") >= 0)\n        .withColumn(\"order_status\", F.upper(F.trim(F.col(\"order_status\"))))\n        \n        # Deduplication (keep latest record)\n        .withColumn(\n            \"row_num\",\n            F.row_number().over(\n                Window.partitionBy(\"order_id\")\n                .orderBy(F.desc(\"_ingestion_timestamp\"))\n            )\n        )\n        .filter(F.col(\"row_num\") == 1)\n        .drop(\"row_num\")\n        \n        # Add quality metadata\n        .w",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 164
    },
    "id": "de_de_data_pipeline_architecture_164"
  },
  {
    "text": "id\")\n                .orderBy(F.desc(\"_ingestion_timestamp\"))\n            )\n        )\n        .filter(F.col(\"row_num\") == 1)\n        .drop(\"row_num\")\n        \n        # Add quality metadata\n        .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n        .withColumn(\"_data_quality_score\", F.lit(100))  # Could be calculated\n    )\n    \n    # Write to Silver (overwrite by partition)\n    df_silver.write \\\n        .format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .partitionBy(\"order_date\") \\\n        .option(\"overwriteSchema\", \"true\") \\\n        .save(silver_path)\n```\n\n### Gold Layer (Business/Aggregated)\n**Purpose:** Business-ready data models\n\n```python\n# Gold Layer - Business Logic & Aggregations\ndef gold_aggregation(silver_path: str, gold_path: str):\n    \"\"\"\n    Create business-ready aggregations\n    - Fact and dimension tables\n    - Business metrics\n    - Optimized for analytics\n    \"\"\"\n    from pyspark.sql import functions as F\n    \n    df_silver = spark.read.form",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 165
    },
    "id": "de_de_data_pipeline_architecture_165"
  },
  {
    "text": "business-ready aggregations\n    - Fact and dimension tables\n    - Business metrics\n    - Optimized for analytics\n    \"\"\"\n    from pyspark.sql import functions as F\n    \n    df_silver = spark.read.format(\"delta\").load(silver_path)\n    \n    # Daily sales summary (Gold fact table)\n    df_gold = (\n        df_silver\n        .filter(F.col(\"order_status\") == \"COMPLETED\")\n        .groupBy(\n            F.col(\"order_date\"),\n            F.col(\"customer_segment\"),\n            F.col(\"product_category\")\n        )\n        .agg(\n            F.count(\"order_id\").alias(\"order_count\"),\n            F.sum(\"order_amount\").alias(\"total_sales\"),\n            F.avg(\"order_amount\").alias(\"avg_order_value\"),\n            F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n            F.min(\"order_amount\").alias(\"min_order\"),\n            F.max(\"order_amount\").alias(\"max_order\")\n        )\n        .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n    )\n    \n    # Write to Gold (overwrite)\n    df_gold",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 166
    },
    "id": "de_de_data_pipeline_architecture_166"
  },
  {
    "text": "min_order\"),\n            F.max(\"order_amount\").alias(\"max_order\")\n        )\n        .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n    )\n    \n    # Write to Gold (overwrite)\n    df_gold.write \\\n        .format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .partitionBy(\"order_date\") \\\n        .save(gold_path)\n```\n\n## Incremental Processing Patterns\n\n### Change Data Capture (CDC)\n**Krishna's CDC Implementation:**\n\n```python\n# CDC Pattern with Merge\nfrom delta.tables import DeltaTable\n\ndef incremental_upsert(source_path: str, target_path: str, key_column: str):\n    \"\"\"\n    Apply incremental changes using merge (upsert)\n    \"\"\"\n    # Read incremental data\n    df_incremental = spark.read.format(\"delta\").load(source_path)\n    \n    # Get target Delta table\n    delta_table = DeltaTable.forPath(spark, target_path)\n    \n    # Merge (upsert) logic\n    (\n        delta_table.alias(\"target\")\n        .merge(\n            df_incremental.alias(\"source\"),\n            f\"target.{key_column",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 167
    },
    "id": "de_de_data_pipeline_architecture_167"
  },
  {
    "text": "Table.forPath(spark, target_path)\n    \n    # Merge (upsert) logic\n    (\n        delta_table.alias(\"target\")\n        .merge(\n            df_incremental.alias(\"source\"),\n            f\"target.{key_column} = source.{key_column}\"\n        )\n        .whenMatchedUpdate(\n            condition=\"source.updated_at > target.updated_at\",\n            set={\n                \"order_status\": \"source.order_status\",\n                \"order_amount\": \"source.order_amount\",\n                \"updated_at\": \"source.updated_at\"\n            }\n        )\n        .whenNotMatchedInsertAll()\n        .execute()\n    )\n```\n\n### Watermark-Based Incremental\n**Krishna's Watermark Pattern:**\n\n```python\n# Watermark tracking for incremental loads\ndef get_last_watermark(table_name: str) -> str:\n    \"\"\"Get last successfully processed watermark\"\"\"\n    watermark_df = spark.sql(f\"\"\"\n        SELECT MAX(watermark_value) as last_watermark\n        FROM control.watermarks\n        WHERE table_name = '{table_name}'\n          AND status = 'SU",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 168
    },
    "id": "de_de_data_pipeline_architecture_168"
  },
  {
    "text": "atermark\"\"\"\n    watermark_df = spark.sql(f\"\"\"\n        SELECT MAX(watermark_value) as last_watermark\n        FROM control.watermarks\n        WHERE table_name = '{table_name}'\n          AND status = 'SUCCESS'\n    \"\"\")\n    \n    return watermark_df.first()['last_watermark'] or '1900-01-01'\n\ndef update_watermark(table_name: str, watermark_value: str, status: str):\n    \"\"\"Update watermark after processing\"\"\"\n    spark.sql(f\"\"\"\n        INSERT INTO control.watermarks\n        VALUES ('{table_name}', '{watermark_value}', '{status}', CURRENT_TIMESTAMP())\n    \"\"\")\n\n# Incremental processing with watermark\ndef incremental_load(source_table: str, target_path: str):\n    \"\"\"Process only new/changed records\"\"\"\n    last_watermark = get_last_watermark(source_table)\n    \n    df_incremental = spark.read \\\n        .format(\"delta\") \\\n        .load(f\"/mnt/raw/{source_table}\") \\\n        .filter(F.col(\"updated_at\") > last_watermark)\n    \n    if df_incremental.count() == 0:\n        print(\"No new data to process\")",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 169
    },
    "id": "de_de_data_pipeline_architecture_169"
  },
  {
    "text": "   .format(\"delta\") \\\n        .load(f\"/mnt/raw/{source_table}\") \\\n        .filter(F.col(\"updated_at\") > last_watermark)\n    \n    if df_incremental.count() == 0:\n        print(\"No new data to process\")\n        return\n    \n    # Process and write\n    df_clean = transform_data(df_incremental)\n    df_clean.write.format(\"delta\").mode(\"append\").save(target_path)\n    \n    # Update watermark\n    new_watermark = df_incremental.agg(F.max(\"updated_at\")).first()[0]\n    update_watermark(source_table, str(new_watermark), 'SUCCESS')\n```\n\n## ETL vs ELT\n\n### ETL (Extract-Transform-Load)\n**When Krishna Uses ETL:**\n```python\n# ETL: Transform before loading\ndef etl_pattern(source_path: str, target_path: str):\n    \"\"\"\n    Transform data BEFORE loading to target\n    Good for: Complex transformations, data reduction, legacy systems\n    \"\"\"\n    # Extract\n    df_raw = spark.read.csv(source_path)\n    \n    # Transform (heavy transformation)\n    df_transformed = (\n        df_raw\n        .filter(F.col(\"status\") ==",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 170
    },
    "id": "de_de_data_pipeline_architecture_170"
  },
  {
    "text": "tion, legacy systems\n    \"\"\"\n    # Extract\n    df_raw = spark.read.csv(source_path)\n    \n    # Transform (heavy transformation)\n    df_transformed = (\n        df_raw\n        .filter(F.col(\"status\") == \"active\")\n        .join(reference_data, \"id\")\n        .groupBy(\"category\").agg(F.sum(\"amount\"))\n        .filter(F.col(\"sum(amount)\") > 1000)\n    )\n    \n    # Load (small, transformed data)\n    df_transformed.write.mode(\"overwrite\").save(target_path)\n```\n\n### ELT (Extract-Load-Transform)\n**When Krishna Uses ELT:**\n```python\n# ELT: Load raw, then transform\ndef elt_pattern(source_path: str, raw_path: str, transformed_path: str):\n    \"\"\"\n    Load raw data THEN transform\n    Good for: Data lakes, cloud platforms, flexibility\n    \"\"\"\n    # Extract and Load (minimal transformation)\n    df_raw = spark.read.csv(source_path)\n    df_raw.write.mode(\"append\").save(raw_path)\n    \n    # Transform later (on demand or scheduled)\n    df_raw_stored = spark.read.parquet(raw_path)\n    df_transformed = transfo",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 171
    },
    "id": "de_de_data_pipeline_architecture_171"
  },
  {
    "text": "k.read.csv(source_path)\n    df_raw.write.mode(\"append\").save(raw_path)\n    \n    # Transform later (on demand or scheduled)\n    df_raw_stored = spark.read.parquet(raw_path)\n    df_transformed = transform_complex_logic(df_raw_stored)\n    df_transformed.write.mode(\"overwrite\").save(transformed_path)\n```\n\n**Krishna's Decision Matrix:**\n- **ETL:** Legacy source systems, complex transformations, small result sets\n- **ELT:** Cloud data lakes, large data volumes, need for flexibility\n\n## Batch vs Streaming\n\n### Batch Processing\n**Krishna's Batch Pattern:**\n```python\n# Daily batch processing\ndef daily_batch_pipeline(execution_date: str):\n    \"\"\"\n    Process full day of data in batch\n    Latency: Hours, Throughput: High, Cost: Low\n    \"\"\"\n    # Read full partition\n    df = spark.read.format(\"delta\") \\\n        .load(f\"/mnt/raw/orders/date={execution_date}\")\n    \n    # Process entire partition\n    df_processed = transform_batch(df)\n    \n    # Write results\n    df_processed.write.format(\"delta\") \\\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 172
    },
    "id": "de_de_data_pipeline_architecture_172"
  },
  {
    "text": " \\\n        .load(f\"/mnt/raw/orders/date={execution_date}\")\n    \n    # Process entire partition\n    df_processed = transform_batch(df)\n    \n    # Write results\n    df_processed.write.format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .save(f\"/mnt/silver/orders/date={execution_date}\")\n```\n\n### Streaming Processing\n**Krishna's Streaming Pattern:**\n```python\n# Real-time streaming\ndef realtime_streaming_pipeline():\n    \"\"\"\n    Process data as it arrives\n    Latency: Seconds, Throughput: Medium, Cost: High\n    \"\"\"\n    from pyspark.sql import functions as F\n    \n    # Read stream from Event Hub\n    df_stream = (\n        spark.readStream\n        .format(\"kafka\")\n        .option(\"kafka.bootstrap.servers\", \"eventhub:9093\")\n        .option(\"subscribe\", \"orders\")\n        .load()\n    )\n    \n    # Parse and transform\n    df_parsed = (\n        df_stream\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n        .select(\"data.*\")\n        .withColumn(\"processing_time\"",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 173
    },
    "id": "de_de_data_pipeline_architecture_173"
  },
  {
    "text": "arse and transform\n    df_parsed = (\n        df_stream\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n        .select(\"data.*\")\n        .withColumn(\"processing_time\", F.current_timestamp())\n    )\n    \n    # Write stream to Delta\n    query = (\n        df_parsed.writeStream\n        .format(\"delta\")\n        .outputMode(\"append\")\n        .option(\"checkpointLocation\", \"/mnt/checkpoints/orders\")\n        .trigger(processingTime=\"1 minute\")\n        .start(\"/mnt/silver/orders_realtime\")\n    )\n    \n    query.awaitTermination()\n```\n\n## Error Handling & Monitoring\n\n### Robust Error Handling\n**Krishna's Pattern:**\n```python\ndef robust_pipeline(source_path: str, target_path: str):\n    \"\"\"Pipeline with comprehensive error handling\"\"\"\n    import traceback\n    \n    try:\n        # Pre-checks\n        if not path_exists(source_path):\n            raise ValueError(f\"Source path not found: {source_path}\")\n        \n        # Process\n        df = spark.read.format(\"delta\").lo",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 174
    },
    "id": "de_de_data_pipeline_architecture_174"
  },
  {
    "text": "       # Pre-checks\n        if not path_exists(source_path):\n            raise ValueError(f\"Source path not found: {source_path}\")\n        \n        # Process\n        df = spark.read.format(\"delta\").load(source_path)\n        \n        if df.count() == 0:\n            logger.warning(\"No data to process\")\n            return {\"status\": \"NO_DATA\", \"rows_processed\": 0}\n        \n        df_transformed = transform_with_quality_checks(df)\n        \n        # Validate output\n        assert df_transformed.count() > 0, \"Transformation resulted in no data\"\n        \n        # Write with retry\n        for attempt in range(3):\n            try:\n                df_transformed.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n                break\n            except Exception as e:\n                if attempt == 2:\n                    raise\n                logger.warning(f\"Write attempt {attempt + 1} failed, retrying...\")\n                time.sleep(60)\n        \n        return {\n            \"status\": ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 175
    },
    "id": "de_de_data_pipeline_architecture_175"
  },
  {
    "text": "tempt == 2:\n                    raise\n                logger.warning(f\"Write attempt {attempt + 1} failed, retrying...\")\n                time.sleep(60)\n        \n        return {\n            \"status\": \"SUCCESS\",\n            \"rows_processed\": df_transformed.count()\n        }\n        \n    except Exception as e:\n        logger.error(f\"Pipeline failed: {str(e)}\")\n        logger.error(traceback.format_exc())\n        \n        # Write to error table\n        error_record = {\n            \"pipeline_name\": \"my_pipeline\",\n            \"error_message\": str(e),\n            \"error_traceback\": traceback.format_exc(),\n            \"timestamp\": datetime.now()\n        }\n        log_error_to_table(error_record)\n        \n        # Send alert\n        send_slack_alert(f\"Pipeline failed: {str(e)}\")\n        \n        raise\n```\n\nThese architecture patterns ensure our Walgreens pipelines are reliable, scalable, and maintainable!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 176
    },
    "id": "de_de_data_pipeline_architecture_176"
  },
  {
    "text": "raise\n```\n\nThese architecture patterns ensure our Walgreens pipelines are reliable, scalable, and maintainable!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_data_pipeline_architecture.md",
      "file_name": "de_data_pipeline_architecture.md",
      "chunk_index": 177
    },
    "id": "de_de_data_pipeline_architecture_177"
  },
  {
    "text": "---\ntags: [data-engineer, azure, azure-data-factory, databricks, synapse, adls, cloud, adf]\npersona: de\n---\n\n# Azure Cloud for Data Engineering - Krishna's Experience\n\n## Introduction\n**Krishna's Azure Journey:**\nAt Walgreens, our entire data platform runs on Azure. I work daily with Azure Data Factory, Databricks, ADLS Gen2, and Synapse. Let me share everything I've learned building production pipelines in Azure.\n\n## Azure Data Factory (ADF)\n\n### Pipeline Architecture\n**Krishna's ADF Design Patterns:**\n\n**1. Metadata-Driven Pipelines:**\n```json\n{\n  \"name\": \"pl_metadata_driven_ingestion\",\n  \"description\": \"Generic pipeline that reads config from control table\",\n  \"activities\": [\n    {\n      \"name\": \"Get Pipeline Metadata\",\n      \"type\": \"Lookup\",\n      \"inputs\": [],\n      \"outputs\": [],\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"AzureSqlSource\",\n          \"sqlReaderQuery\": \"SELECT * FROM control.pipeline_config WHERE is_active = 1\"\n        },\n        \"firstRowOnly",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 178
    },
    "id": "de_de_azure_cloud_178"
  },
  {
    "text": "    \"typeProperties\": {\n        \"source\": {\n          \"type\": \"AzureSqlSource\",\n          \"sqlReaderQuery\": \"SELECT * FROM control.pipeline_config WHERE is_active = 1\"\n        },\n        \"firstRowOnly\": false\n      }\n    },\n    {\n      \"name\": \"For Each Source System\",\n      \"type\": \"ForEach\",\n      \"dependsOn\": [{\"activity\": \"Get Pipeline Metadata\", \"dependencyConditions\": [\"Succeeded\"]}],\n      \"typeProperties\": {\n        \"items\": {\n          \"value\": \"@activity('Get Pipeline Metadata').output.value\",\n          \"type\": \"Expression\"\n        },\n        \"isSequential\": false,\n        \"batchCount\": 4,\n        \"activities\": [\n          {\n            \"name\": \"Copy Data\",\n            \"type\": \"Copy\",\n            \"inputs\": [{\n              \"referenceName\": \"ds_generic_source\",\n              \"type\": \"DatasetReference\",\n              \"parameters\": {\n                \"tableName\": \"@item().source_table\",\n                \"schemaName\": \"@item().source_schema\"\n              }\n            }],\n        ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 179
    },
    "id": "de_de_azure_cloud_179"
  },
  {
    "text": "\": \"DatasetReference\",\n              \"parameters\": {\n                \"tableName\": \"@item().source_table\",\n                \"schemaName\": \"@item().source_schema\"\n              }\n            }],\n            \"outputs\": [{\n              \"referenceName\": \"ds_adls_bronze\",\n              \"type\": \"DatasetReference\",\n              \"parameters\": {\n                \"filePath\": \"@concat(item().destination_path, '/', formatDateTime(utcnow(), 'yyyy-MM-dd'))\"\n              }\n            }],\n            \"typeProperties\": {\n              \"source\": {\"type\": \"SqlServerSource\"},\n              \"sink\": {\n                \"type\": \"ParquetSink\",\n                \"storeSettings\": {\"type\": \"AzureBlobFSWriteSettings\"}\n              }\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n**Business Impact:**\nOne metadata-driven pipeline replaced 50+ individual copy pipelines. Changes now take 2 minutes (update config table) vs 2 hours (modify 50 pipelines).\n\n**2. Incremental Load Pattern:**\n```json\n{\n  \"name\":",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 180
    },
    "id": "de_de_azure_cloud_180"
  },
  {
    "text": "etadata-driven pipeline replaced 50+ individual copy pipelines. Changes now take 2 minutes (update config table) vs 2 hours (modify 50 pipelines).\n\n**2. Incremental Load Pattern:**\n```json\n{\n  \"name\": \"pl_incremental_load_with_watermark\",\n  \"activities\": [\n    {\n      \"name\": \"Get Last Watermark\",\n      \"type\": \"Lookup\",\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"AzureSqlSource\",\n          \"sqlReaderQuery\": \"SELECT MAX(updated_at) as last_watermark FROM control.watermark WHERE table_name = 'orders'\"\n        }\n      }\n    },\n    {\n      \"name\": \"Copy Incremental Data\",\n      \"type\": \"Copy\",\n      \"dependsOn\": [{\"activity\": \"Get Last Watermark\", \"dependencyConditions\": [\"Succeeded\"]}],\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"SqlServerSource\",\n          \"sqlReaderQuery\": {\n            \"value\": \"SELECT * FROM orders WHERE updated_at > '@{activity('Get Last Watermark').output.firstRow.last_watermark}' AND updated_at <= GETDATE()\",\n            \"",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 181
    },
    "id": "de_de_azure_cloud_181"
  },
  {
    "text": "        \"sqlReaderQuery\": {\n            \"value\": \"SELECT * FROM orders WHERE updated_at > '@{activity('Get Last Watermark').output.firstRow.last_watermark}' AND updated_at <= GETDATE()\",\n            \"type\": \"Expression\"\n          }\n        },\n        \"sink\": {\n          \"type\": \"ParquetSink\"\n        }\n      }\n    },\n    {\n      \"name\": \"Update Watermark\",\n      \"type\": \"SqlServerStoredProcedure\",\n      \"dependsOn\": [{\"activity\": \"Copy Incremental Data\", \"dependencyConditions\": [\"Succeeded\"]}],\n      \"typeProperties\": {\n        \"storedProcedureName\": \"control.usp_update_watermark\",\n        \"storedProcedureParameters\": {\n          \"table_name\": {\"value\": \"orders\", \"type\": \"String\"},\n          \"watermark_value\": {\n            \"value\": \"@activity('Copy Incremental Data').output.executionDetails[0].source.rowsRead\",\n            \"type\": \"DateTime\"\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n**Real Use Case - Pharmacy Orders:**\nFull load: 8 hours, 5TB data transfer\nIncremental: 15 minutes,",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 182
    },
    "id": "de_de_azure_cloud_182"
  },
  {
    "text": "s[0].source.rowsRead\",\n            \"type\": \"DateTime\"\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n**Real Use Case - Pharmacy Orders:**\nFull load: 8 hours, 5TB data transfer\nIncremental: 15 minutes, 50GB data transfer\nSaved: $200/day in data transfer costs\n\n**3. Error Handling & Retry Logic:**\n```json\n{\n  \"name\": \"pl_robust_pipeline\",\n  \"activities\": [\n    {\n      \"name\": \"Execute Databricks Notebook\",\n      \"type\": \"DatabricksNotebook\",\n      \"policy\": {\n        \"timeout\": \"0.12:00:00\",\n        \"retry\": 3,\n        \"retryIntervalInSeconds\": 300,\n        \"secureOutput\": false,\n        \"secureInput\": false\n      },\n      \"typeProperties\": {\n        \"notebookPath\": \"/Pipelines/Silver/transform_orders\",\n        \"baseParameters\": {\n          \"execution_date\": \"@pipeline().parameters.execution_date\"\n        }\n      }\n    },\n    {\n      \"name\": \"Send Success Email\",\n      \"type\": \"WebActivity\",\n      \"dependsOn\": [{\"activity\": \"Execute Databricks Notebook\", \"dependencyConditions\": [\"Succeed",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 183
    },
    "id": "de_de_azure_cloud_183"
  },
  {
    "text": "_date\"\n        }\n      }\n    },\n    {\n      \"name\": \"Send Success Email\",\n      \"type\": \"WebActivity\",\n      \"dependsOn\": [{\"activity\": \"Execute Databricks Notebook\", \"dependencyConditions\": [\"Succeeded\"]}],\n      \"typeProperties\": {\n        \"url\": \"https://prod-27.eastus.logic.azure.com/workflows/abc123/triggers/manual/paths/invoke\",\n        \"method\": \"POST\",\n        \"body\": {\n          \"pipeline\": \"@pipeline().Pipeline\",\n          \"runId\": \"@pipeline().RunId\",\n          \"status\": \"Success\"\n        }\n      }\n    },\n    {\n      \"name\": \"Send Failure Alert\",\n      \"type\": \"WebActivity\",\n      \"dependsOn\": [{\"activity\": \"Execute Databricks Notebook\", \"dependencyConditions\": [\"Failed\"]}],\n      \"typeProperties\": {\n        \"url\": \"https://hooks.slack.com/services/YOUR/WEBHOOK/URL\",\n        \"method\": \"POST\",\n        \"body\": {\n          \"text\": \"🚨 Pipeline Failed: @{pipeline().Pipeline} - Run ID: @{pipeline().RunId}\"\n        }\n      }\n    }\n  ]\n}\n```\n\n### ADF Optimization Strategies\n**Krishn",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 184
    },
    "id": "de_de_azure_cloud_184"
  },
  {
    "text": "thod\": \"POST\",\n        \"body\": {\n          \"text\": \"🚨 Pipeline Failed: @{pipeline().Pipeline} - Run ID: @{pipeline().RunId}\"\n        }\n      }\n    }\n  ]\n}\n```\n\n### ADF Optimization Strategies\n**Krishna's Performance Tips:**\n\n**1. Parallel Execution:**\n```json\n{\n  \"name\": \"For Each with Parallelism\",\n  \"type\": \"ForEach\",\n  \"typeProperties\": {\n    \"isSequential\": false,\n    \"batchCount\": 10,  // Process 10 items in parallel\n    \"activities\": [...]\n  }\n}\n```\n\n**2. Data Integration Units (DIU) Optimization:**\n```json\n{\n  \"name\": \"Copy with Optimal DIU\",\n  \"type\": \"Copy\",\n  \"typeProperties\": {\n    \"dataIntegrationUnits\": 32,  // Auto-scale up to 32 DIUs\n    \"parallelCopies\": 8,          // 8 parallel copy operations\n    \"enableStaging\": true,\n    \"stagingSettings\": {\n      \"linkedServiceName\": {\"referenceName\": \"ls_adls_staging\"}\n    }\n  }\n}\n```\n\n**3. Partitioned Copy:**\n```json\n{\n  \"name\": \"Partitioned Copy for Large Tables\",\n  \"type\": \"Copy\",\n  \"typeProperties\": {\n    \"source\": {\n      \"t",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 185
    },
    "id": "de_de_azure_cloud_185"
  },
  {
    "text": "{\"referenceName\": \"ls_adls_staging\"}\n    }\n  }\n}\n```\n\n**3. Partitioned Copy:**\n```json\n{\n  \"name\": \"Partitioned Copy for Large Tables\",\n  \"type\": \"Copy\",\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"SqlServerSource\",\n      \"partitionOption\": \"DynamicRange\",\n      \"partitionSettings\": {\n        \"partitionColumnName\": \"order_id\",\n        \"partitionUpperBound\": \"10000000\",\n        \"partitionLowerBound\": \"1\"\n      }\n    }\n  }\n}\n```\n\n## Azure Databricks\n\n### Integration with ADF\n**Krishna's Databricks-ADF Workflow:**\n\n**1. Triggered Databricks Jobs:**\n```json\n{\n  \"name\": \"Execute dbt in Databricks\",\n  \"type\": \"DatabricksNotebook\",\n  \"linkedServiceName\": {\"referenceName\": \"ls_databricks\"},\n  \"typeProperties\": {\n    \"notebookPath\": \"/Shared/dbt/run_dbt_models\",\n    \"baseParameters\": {\n      \"execution_date\": \"@formatDateTime(pipeline().parameters.date, 'yyyy-MM-dd')\",\n      \"dbt_models\": \"tag:daily\",\n      \"target\": \"prod\"\n    },\n    \"libraries\": [\n      {\"pypi\": {\"package\": \"dbt-data",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 186
    },
    "id": "de_de_azure_cloud_186"
  },
  {
    "text": "   \"execution_date\": \"@formatDateTime(pipeline().parameters.date, 'yyyy-MM-dd')\",\n      \"dbt_models\": \"tag:daily\",\n      \"target\": \"prod\"\n    },\n    \"libraries\": [\n      {\"pypi\": {\"package\": \"dbt-databricks==1.6.0\"}}\n    ]\n  },\n  \"linkedServiceName\": {\n    \"referenceName\": \"ls_databricks_job_cluster\",\n    \"type\": \"LinkedServiceReference\"\n  }\n}\n```\n\n**2. Databricks Notebook for ADF:**\n```python\n# Databricks Notebook: transform_orders.py\n# This notebook is called from ADF\n\n# Get parameters from ADF\ndbutils.widgets.text(\"execution_date\", \"\")\ndbutils.widgets.text(\"environment\", \"prod\")\n\nexecution_date = dbutils.widgets.get(\"execution_date\")\nenvironment = dbutils.widgets.get(\"environment\")\n\nprint(f\"Processing data for {execution_date} in {environment}\")\n\n# Read from Bronze (raw data)\ndf_orders = spark.read.format(\"delta\") \\\n    .load(f\"/mnt/bronze/orders/date={execution_date}\")\n\n# Transform\ndf_clean = (\n    df_orders\n    .filter(F.col(\"order_status\").isin([\"completed\", \"shipped\"]))\n    .wit",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 187
    },
    "id": "de_de_azure_cloud_187"
  },
  {
    "text": "= spark.read.format(\"delta\") \\\n    .load(f\"/mnt/bronze/orders/date={execution_date}\")\n\n# Transform\ndf_clean = (\n    df_orders\n    .filter(F.col(\"order_status\").isin([\"completed\", \"shipped\"]))\n    .withColumn(\"order_amount\", F.col(\"order_amount\").cast(\"decimal(18,2)\"))\n    .dropDuplicates([\"order_id\"])\n)\n\n# Write to Silver\ndf_clean.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"order_date\") \\\n    .save(f\"/mnt/silver/orders\")\n\n# Data quality check\nrow_count = df_clean.count()\nif row_count == 0:\n    dbutils.notebook.exit(\"FAILED: No data processed\")\n\nprint(f\"✅ Processed {row_count:,} orders\")\ndbutils.notebook.exit(f\"SUCCESS: {row_count}\")\n```\n\n### Cluster Configuration\n**Krishna's Production Cluster Setup:**\n\n**Job Cluster (Cost-Effective):**\n```python\n{\n  \"cluster_name\": \"adf-triggered-etl\",\n  \"spark_version\": \"13.3.x-scala2.12\",\n  \"node_type_id\": \"Standard_DS3_v2\",\n  \"autoscale\": {\n    \"min_workers\": 2,\n    \"max_workers\": 8\n  },\n  \"auto_termination_minutes\": 0,  // T",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 188
    },
    "id": "de_de_azure_cloud_188"
  },
  {
    "text": "\"adf-triggered-etl\",\n  \"spark_version\": \"13.3.x-scala2.12\",\n  \"node_type_id\": \"Standard_DS3_v2\",\n  \"autoscale\": {\n    \"min_workers\": 2,\n    \"max_workers\": 8\n  },\n  \"auto_termination_minutes\": 0,  // Terminate after job\n  \"spark_conf\": {\n    \"spark.databricks.delta.preview.enabled\": \"true\",\n    \"spark.sql.adaptive.enabled\": \"true\"\n  },\n  \"azure_attributes\": {\n    \"availability\": \"SPOT_WITH_FALLBACK_AZURE\",\n    \"spot_bid_max_price\": 0.5\n  }\n}\n```\n\n**All-Purpose Cluster (Development):**\n```python\n{\n  \"cluster_name\": \"dev-data-engineering\",\n  \"spark_version\": \"13.3.x-scala2.12\",\n  \"node_type_id\": \"Standard_DS3_v2\",\n  \"num_workers\": 2,\n  \"autotermination_minutes\": 30,\n  \"spark_conf\": {\n    \"spark.databricks.delta.preview.enabled\": \"true\"\n  }\n}\n```\n\n**Cost Savings:**\n- Using Spot VMs: 60-80% cheaper than on-demand\n- Job clusters vs always-on: Saved $30K/month\n- Right-sizing: Reduced over-provisioning by 40%\n\n## Azure Data Lake Storage (ADLS) Gen2\n\n### Storage Organization\n**Krishna's Data La",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 189
    },
    "id": "de_de_azure_cloud_189"
  },
  {
    "text": "aper than on-demand\n- Job clusters vs always-on: Saved $30K/month\n- Right-sizing: Reduced over-provisioning by 40%\n\n## Azure Data Lake Storage (ADLS) Gen2\n\n### Storage Organization\n**Krishna's Data Lake Structure:**\n```\nadls://walgreensdata@storageaccount.dfs.core.windows.net/\n\n├── raw/                                    # Landing zone\n│   ├── salesforce/\n│   │   └── 2024-01-15/opportunities.parquet\n│   ├── sql_server/\n│   │   └── orders/2024-01-15/orders.parquet\n│   └── api_data/\n│       └── customer_events/2024-01-15/events.json\n│\n├── bronze/                                 # Raw ingested (Delta format)\n│   ├── orders/\n│   │   └── date=2024-01-15/\n│   ├── customers/\n│   │   └── date=2024-01-15/\n│   └── products/\n│\n├── silver/                                 # Cleaned & validated\n│   ├── orders/\n│   │   └── order_date=2024-01-15/\n│   ├── dim_customer/\n│   └── dim_product/\n│\n├── gold/                                   # Business aggregations\n│   ├── daily_sales_summary/\n│   ├── custome",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 190
    },
    "id": "de_de_azure_cloud_190"
  },
  {
    "text": "orders/\n│   │   └── order_date=2024-01-15/\n│   ├── dim_customer/\n│   └── dim_product/\n│\n├── gold/                                   # Business aggregations\n│   ├── daily_sales_summary/\n│   ├── customer_ltv/\n│   └── product_performance/\n│\n└── archive/                                # Old data\n    └── raw/2023/\n```\n\n### Access Control & Security\n**Krishna's Security Implementation:**\n\n**1. Service Principal Authentication:**\n```python\n# Mount ADLS using Service Principal\nconfigs = {\n  \"fs.azure.account.auth.type\": \"OAuth\",\n  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n  \"fs.azure.account.oauth2.client.id\": dbutils.secrets.get(\"keyvault\", \"sp-client-id\"),\n  \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(\"keyvault\", \"sp-client-secret\"),\n  \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/tenant-id/oauth2/token\"\n}\n\ndbutils.fs.mount(\n  source = \"abfss://bronze@walgreensdata.dfs.core.windows",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 191
    },
    "id": "de_de_azure_cloud_191"
  },
  {
    "text": "-client-secret\"),\n  \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/tenant-id/oauth2/token\"\n}\n\ndbutils.fs.mount(\n  source = \"abfss://bronze@walgreensdata.dfs.core.windows.net/\",\n  mount_point = \"/mnt/bronze\",\n  extra_configs = configs\n)\n```\n\n**2. Azure Key Vault Integration:**\n```python\n# Store secrets in Key Vault, reference in Databricks\ndb_password = dbutils.secrets.get(scope=\"keyvault\", key=\"sql-db-password\")\napi_key = dbutils.secrets.get(scope=\"keyvault\", key=\"external-api-key\")\n\n# Use in connections\njdbc_url = f\"jdbc:sqlserver://server.database.windows.net;database=WalgreensDB;user=admin;password={db_password}\"\n```\n\n**3. RBAC (Role-Based Access Control):**\n```bash\n# Azure CLI commands for RBAC\n# Grant Data Engineer team read/write to silver layer\naz role assignment create \\\n  --assignee \"data-engineers@walgreens.com\" \\\n  --role \"Storage Blob Data Contributor\" \\\n  --scope \"/subscriptions/{sub-id}/resourceGroups/rg-data/providers/Microsoft.Storage/stor",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 192
    },
    "id": "de_de_azure_cloud_192"
  },
  {
    "text": " assignment create \\\n  --assignee \"data-engineers@walgreens.com\" \\\n  --role \"Storage Blob Data Contributor\" \\\n  --scope \"/subscriptions/{sub-id}/resourceGroups/rg-data/providers/Microsoft.Storage/storageAccounts/walgreensdata/blobServices/default/containers/silver\"\n\n# Grant Analytics team read-only to gold layer\naz role assignment create \\\n  --assignee \"analytics-team@walgreens.com\" \\\n  --role \"Storage Blob Data Reader\" \\\n  --scope \"/subscriptions/{sub-id}/resourceGroups/rg-data/providers/Microsoft.Storage/storageAccounts/walgreensdata/blobServices/default/containers/gold\"\n```\n\n## Azure Synapse Analytics\n\n### SQL Pool for Analytics\n**Krishna's Synapse Implementation:**\n\n**1. External Tables on Data Lake:**\n```sql\n-- Create external data source\nCREATE EXTERNAL DATA SOURCE adls_gold\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'abfss://gold@walgreensdata.dfs.core.windows.net/'\n);\n\n-- Create external file format\nCREATE EXTERNAL FILE FORMAT parquet_format\nWITH (\n    FORMAT_TYPE = PARQUET,\n    ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 193
    },
    "id": "de_de_azure_cloud_193"
  },
  {
    "text": "   TYPE = HADOOP,\n    LOCATION = 'abfss://gold@walgreensdata.dfs.core.windows.net/'\n);\n\n-- Create external file format\nCREATE EXTERNAL FILE FORMAT parquet_format\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\n\n-- Create external table\nCREATE EXTERNAL TABLE ext_daily_sales (\n    order_date DATE,\n    customer_segment VARCHAR(50),\n    product_category VARCHAR(100),\n    total_sales DECIMAL(18,2),\n    order_count INT\n)\nWITH (\n    LOCATION = '/daily_sales_summary/',\n    DATA_SOURCE = adls_gold,\n    FILE_FORMAT = parquet_format\n);\n\n-- Query it like a normal table\nSELECT \n    customer_segment,\n    SUM(total_sales) as total_revenue,\n    SUM(order_count) as total_orders\nFROM ext_daily_sales\nWHERE order_date >= '2024-01-01'\nGROUP BY customer_segment;\n```\n\n**2. Serverless SQL Pool:**\n```sql\n-- Query Delta tables directly (no import needed!)\nSELECT \n    order_id,\n    customer_id,\n    order_date,\n    order_amount\nFROM OPENROWSET(\n    BULK 'htt",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 194
    },
    "id": "de_de_azure_cloud_194"
  },
  {
    "text": "gment;\n```\n\n**2. Serverless SQL Pool:**\n```sql\n-- Query Delta tables directly (no import needed!)\nSELECT \n    order_id,\n    customer_id,\n    order_date,\n    order_amount\nFROM OPENROWSET(\n    BULK 'https://walgreensdata.dfs.core.windows.net/silver/orders/',\n    FORMAT = 'DELTA'\n) AS orders\nWHERE order_date >= '2024-01-01';\n```\n\n## Azure DevOps CI/CD\n\n### Data Pipeline CI/CD\n**Krishna's Deployment Pipeline:**\n\n```yaml\n# azure-pipelines.yml\ntrigger:\n  branches:\n    include:\n      - main\n      - develop\n  paths:\n    include:\n      - pipelines/\n      - databricks/\n\npool:\n  vmImage: 'ubuntu-latest'\n\nstages:\n  - stage: Build\n    jobs:\n      - job: Validate\n        steps:\n          - task: UsePythonVersion@0\n            inputs:\n              versionSpec: '3.10'\n          \n          - script: |\n              pip install databricks-cli pyspark pytest\n              pytest tests/\n            displayName: 'Run Unit Tests'\n          \n          - script: |\n              # Validate ADF pipelines\n     ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 195
    },
    "id": "de_de_azure_cloud_195"
  },
  {
    "text": "           pip install databricks-cli pyspark pytest\n              pytest tests/\n            displayName: 'Run Unit Tests'\n          \n          - script: |\n              # Validate ADF pipelines\n              az datafactory pipeline validate \\\n                --factory-name adf-walgreens \\\n                --resource-group rg-data \\\n                --name pl_daily_etl\n            displayName: 'Validate ADF Pipelines'\n\n  - stage: Deploy_Dev\n    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/develop'))\n    jobs:\n      - job: Deploy\n        steps:\n          - script: |\n              # Deploy Databricks notebooks\n              databricks workspace import_dir \\\n                ./databricks/notebooks \\\n                /Shared/Pipelines \\\n                --overwrite\n            displayName: 'Deploy to Dev Databricks'\n          \n          - task: AzureResourceManagerTemplateDeployment@3\n            inputs:\n              deploymentScope: 'Resource Group'\n            ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 196
    },
    "id": "de_de_azure_cloud_196"
  },
  {
    "text": "          displayName: 'Deploy to Dev Databricks'\n          \n          - task: AzureResourceManagerTemplateDeployment@3\n            inputs:\n              deploymentScope: 'Resource Group'\n              azureResourceManagerConnection: 'Azure-DevOps-SP'\n              subscriptionId: 'subscription-id'\n              action: 'Create Or Update Resource Group'\n              resourceGroupName: 'rg-data-dev'\n              location: 'East US'\n              templateLocation: 'Linked artifact'\n              csmFile: 'arm-templates/adf-pipelines.json'\n            displayName: 'Deploy ADF Pipelines to Dev'\n\n  - stage: Deploy_Prod\n    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n    jobs:\n      - deployment: DeployProd\n        environment: 'production'\n        strategy:\n          runOnce:\n            deploy:\n              steps:\n                - script: |\n                    databricks workspace import_dir \\\n                      ./databricks/notebooks \\\n      ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 197
    },
    "id": "de_de_azure_cloud_197"
  },
  {
    "text": "egy:\n          runOnce:\n            deploy:\n              steps:\n                - script: |\n                    databricks workspace import_dir \\\n                      ./databricks/notebooks \\\n                      /Production/Pipelines \\\n                      --overwrite\n                  displayName: 'Deploy to Prod Databricks'\n```\n\n## Cost Optimization\n\n### Krishna's Azure Cost Strategies:\n\n**1. Reserved Capacity:**\n- Committed to 1-year reserved Databricks: Saved 37%\n- Reserved Azure SQL: Saved 40%\n\n**2. Spot VMs for Databricks:**\n```python\n# Use spot instances for non-critical jobs\n\"azure_attributes\": {\n    \"availability\": \"SPOT_WITH_FALLBACK_AZURE\",\n    \"spot_bid_max_price\": 0.5  // Max 50% of on-demand price\n}\n```\n\n**3. Storage Lifecycle Policies:**\n```json\n{\n  \"rules\": [\n    {\n      \"name\": \"move-old-raw-to-cool\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"filters\": {\n          \"blobTypes\": [\"blockBlob\"],\n          \"prefixMatch\": [\"raw/\"]\n        },\n        \"act",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 198
    },
    "id": "de_de_azure_cloud_198"
  },
  {
    "text": "   \"name\": \"move-old-raw-to-cool\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"filters\": {\n          \"blobTypes\": [\"blockBlob\"],\n          \"prefixMatch\": [\"raw/\"]\n        },\n        \"actions\": {\n          \"baseBlob\": {\n            \"tierToCool\": {\"daysAfterModificationGreaterThan\": 30},\n            \"tierToArchive\": {\"daysAfterModificationGreaterThan\": 90}\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\n**4. Auto-Pause & Auto-Terminate:**\n- Auto-terminate dev clusters after 30 min idle\n- Auto-pause Synapse SQL pools overnight\n- Saved: $25K/month\n\n**Total Cost Optimization:**\n- Monthly Azure spend: $180K → $115K (36% reduction)\n- No performance impact\n- CFO was very happy!\n\nMy Azure expertise helps Walgreens run cost-effective, scalable, reliable data platforms in the cloud!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_azure_cloud.md",
      "file_name": "de_azure_cloud.md",
      "chunk_index": 199
    },
    "id": "de_de_azure_cloud_199"
  },
  {
    "text": "---\ntags: [data-engineer, pyspark, databricks, delta-lake, spark, big-data, performance-optimization]\npersona: de\n---\n\n# PySpark & Databricks - Krishna's Comprehensive Guide\n\n## Introduction\n**Krishna's PySpark Journey:**\nI use PySpark daily at Walgreens to process 10TB+ monthly data. What started with simple transformations evolved into building complex pipelines with performance optimization, Delta Lake management, and real-time streaming. Let me share everything I've learned.\n\n## PySpark Fundamentals\n\n### DataFrame API\n**Krishna's Most-Used Operations:**\n```python\nfrom pyspark.sql import SparkSession, functions as F, Window\n\n# Initialize Spark\nspark = SparkSession.builder \\\n    .appName(\"Walgreens Data Pipeline\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Read data\ndf_orders = spark.read.format(\"delta\").load(\"/mnt/raw/orders\")\n\n# Common transformations\ndf_clean = (\n    df_orders\n    # Filter\n    .filter(F.col(\"order_status\") == \"completed\")\n    .filt",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 200
    },
    "id": "de_de_pyspark_databricks_200"
  },
  {
    "text": "# Read data\ndf_orders = spark.read.format(\"delta\").load(\"/mnt/raw/orders\")\n\n# Common transformations\ndf_clean = (\n    df_orders\n    # Filter\n    .filter(F.col(\"order_status\") == \"completed\")\n    .filter(F.col(\"order_date\") >= \"2023-01-01\")\n    \n    # Select and rename\n    .select(\n        F.col(\"order_id\"),\n        F.col(\"customer_id\"),\n        F.col(\"order_date\").cast(\"date\"),\n        F.col(\"order_amount\").cast(\"decimal(18,2)\").alias(\"amount\")\n    )\n    \n    # Add derived columns\n    .withColumn(\"order_year\", F.year(\"order_date\"))\n    .withColumn(\"order_month\", F.month(\"order_date\"))\n    .withColumn(\"is_high_value\", F.when(F.col(\"amount\") > 1000, True).otherwise(False))\n    \n    # Remove duplicates\n    .dropDuplicates([\"order_id\"])\n    \n    # Handle nulls\n    .fillna({\"amount\": 0, \"customer_id\": \"UNKNOWN\"})\n)\n\n# Aggregations\ndf_summary = (\n    df_clean\n    .groupBy(\"customer_id\", \"order_year\", \"order_month\")\n    .agg(\n        F.count(\"order_id\").alias(\"order_count\"),\n        F.sum(\"am",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 201
    },
    "id": "de_de_pyspark_databricks_201"
  },
  {
    "text": "er_id\": \"UNKNOWN\"})\n)\n\n# Aggregations\ndf_summary = (\n    df_clean\n    .groupBy(\"customer_id\", \"order_year\", \"order_month\")\n    .agg(\n        F.count(\"order_id\").alias(\"order_count\"),\n        F.sum(\"amount\").alias(\"total_spent\"),\n        F.avg(\"amount\").alias(\"avg_order_value\"),\n        F.min(\"order_date\").alias(\"first_order\"),\n        F.max(\"order_date\").alias(\"last_order\")\n    )\n)\n```\n\n### Window Functions\n**Krishna's Real Use Case - Customer Rankings:**\n```python\nfrom pyspark.sql.window import Window\n\n# Calculate customer lifetime value rankings\nwindow_spec = Window.partitionBy(\"customer_segment\").orderBy(F.desc(\"lifetime_value\"))\n\ndf_customer_analysis = (\n    df_customers\n    .withColumn(\"segment_rank\", F.row_number().over(window_spec))\n    .withColumn(\"segment_percentile\", F.percent_rank().over(window_spec))\n    .withColumn(\n        \"cumulative_value\", \n        F.sum(\"lifetime_value\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.currentRow))\n    )\n    .withColumn(",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 202
    },
    "id": "de_de_pyspark_databricks_202"
  },
  {
    "text": "nk().over(window_spec))\n    .withColumn(\n        \"cumulative_value\", \n        F.sum(\"lifetime_value\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.currentRow))\n    )\n    .withColumn(\n        \"moving_avg_3months\",\n        F.avg(\"monthly_spend\").over(Window.partitionBy(\"customer_id\").orderBy(\"month\").rowsBetween(-2, 0))\n    )\n)\n\n# Lead/Lag for time-series analysis\ndf_retention = (\n    df_orders\n    .groupBy(\"customer_id\", F.date_trunc(\"month\", \"order_date\").alias(\"order_month\"))\n    .agg(F.sum(\"order_amount\").alias(\"monthly_revenue\"))\n    .withColumn(\n        \"prev_month_revenue\",\n        F.lag(\"monthly_revenue\", 1).over(Window.partitionBy(\"customer_id\").orderBy(\"order_month\"))\n    )\n    .withColumn(\n        \"next_month_revenue\",\n        F.lag(\"monthly_revenue\", -1).over(Window.partitionBy(\"customer_id\").orderBy(\"order_month\"))\n    )\n    .withColumn(\n        \"revenue_change_pct\",\n        ((F.col(\"monthly_revenue\") - F.col(\"prev_month_revenue\")) / F.col(\"prev_month_reven",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 203
    },
    "id": "de_de_pyspark_databricks_203"
  },
  {
    "text": "ndow.partitionBy(\"customer_id\").orderBy(\"order_month\"))\n    )\n    .withColumn(\n        \"revenue_change_pct\",\n        ((F.col(\"monthly_revenue\") - F.col(\"prev_month_revenue\")) / F.col(\"prev_month_revenue\") * 100)\n    )\n)\n```\n\n## Performance Optimization\n\n### 1. Partitioning Strategies\n**Krishna's Lessons Learned:**\n```python\n# BAD: No partitioning - slow queries\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/data/orders\")\n\n# GOOD: Partition by date - fast time-based queries\ndf.write.format(\"delta\") \\\n    .partitionBy(\"order_date\") \\\n    .mode(\"overwrite\") \\\n    .save(\"/mnt/data/orders\")\n\n# BETTER: Multi-level partitioning for complex queries\ndf.write.format(\"delta\") \\\n    .partitionBy(\"order_year\", \"order_month\") \\\n    .mode(\"overwrite\") \\\n    .save(\"/mnt/data/orders\")\n\n# Optimal file sizes (128MB - 1GB per file)\ndf.repartition(100).write.format(\"delta\") \\\n    .partitionBy(\"order_date\") \\\n    .mode(\"overwrite\") \\\n    .save(\"/mnt/data/orders\")\n```\n\n**Business Impact at Walgreens:*",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 204
    },
    "id": "de_de_pyspark_databricks_204"
  },
  {
    "text": "e sizes (128MB - 1GB per file)\ndf.repartition(100).write.format(\"delta\") \\\n    .partitionBy(\"order_date\") \\\n    .mode(\"overwrite\") \\\n    .save(\"/mnt/data/orders\")\n```\n\n**Business Impact at Walgreens:**\n- Pharmacy queries on daily partitions: 4 hours → 15 minutes\n- Supply chain monthly reports: 2 hours → 10 minutes\n- Cost savings: $15K/month in compute costs\n\n### 2. Broadcast Joins\n**Krishna's Join Optimization:**\n```python\n# BAD: Shuffle join for small dimension table\nlarge_sales.join(small_products, \"product_id\")\n\n# GOOD: Broadcast small table (< 10MB)\nlarge_sales.join(F.broadcast(small_products), \"product_id\")\n\n# Automatic broadcast (configure threshold)\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # 10MB\n\n# Example: Customer loyalty pipeline\ndf_transactions = spark.read.format(\"delta\").load(\"/mnt/raw/transactions\")  # 50M rows\ndf_stores = spark.read.format(\"delta\").load(\"/mnt/dim/stores\")  # 9K rows\n\n# Broadcast stores dimension\ndf_enriched = df_transactions.joi",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 205
    },
    "id": "de_de_pyspark_databricks_205"
  },
  {
    "text": "read.format(\"delta\").load(\"/mnt/raw/transactions\")  # 50M rows\ndf_stores = spark.read.format(\"delta\").load(\"/mnt/dim/stores\")  # 9K rows\n\n# Broadcast stores dimension\ndf_enriched = df_transactions.join(\n    F.broadcast(df_stores),\n    \"store_id\",\n    \"left\"\n)\n```\n\n### 3. Caching & Persistence\n**When Krishna Uses Caching:**\n```python\n# Cache frequently accessed data\ndf_customers = spark.read.format(\"delta\").load(\"/mnt/dim/customers\")\ndf_customers.cache()\n\n# Use in multiple operations\nhigh_value = df_customers.filter(F.col(\"ltv\") > 10000).count()\nchurn_risk = df_customers.filter(F.col(\"days_since_order\") > 90).count()\n\n# Different persistence levels\nfrom pyspark import StorageLevel\n\n# Memory only - fastest but risky\ndf.persist(StorageLevel.MEMORY_ONLY)\n\n# Memory + Disk - safer\ndf.persist(StorageLevel.MEMORY_AND_DISK)\n\n# Serialized - more efficient memory usage\ndf.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n# Always unpersist when done!\ndf.unpersist()\n```\n\n**Business Scenario:**\nPharmacy d",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 206
    },
    "id": "de_de_pyspark_databricks_206"
  },
  {
    "text": "rageLevel.MEMORY_AND_DISK)\n\n# Serialized - more efficient memory usage\ndf.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n# Always unpersist when done!\ndf.unpersist()\n```\n\n**Business Scenario:**\nPharmacy dashboard queries the same customer dimension 50 times. Caching reduced query time from 2 minutes to 5 seconds.\n\n### 4. Adaptive Query Execution (AQE)\n**Krishna's AQE Configuration:**\n```python\n# Enable AQE (Spark 3.0+)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n\n# Coalesce partitions after shuffle\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\", 200)\n\n# Handle skewed joins automatically\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", 5)\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n\n# Optimize join strategies\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 207
    },
    "id": "de_de_pyspark_databricks_207"
  },
  {
    "text": "Join.skewedPartitionFactor\", 5)\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n\n# Optimize join strategies\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n```\n\n**Real Impact:**\nSkewed customer data (80% orders from 20% customers) caused 3-hour jobs. AQE detected skew and reoptimized → 45 minutes.\n\n## Delta Lake\n\n### 1. ACID Transactions\n**Krishna's Reliable Pipelines:**\n```python\nfrom delta.tables import DeltaTable\n\n# Write with transactions\ndf_new_orders.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .save(\"/mnt/silver/orders\")\n\n# Concurrent writes are safe - Delta handles it!\n# Multiple jobs can write simultaneously\n\n# Read consistent snapshots\ndf = spark.read.format(\"delta\").load(\"/mnt/silver/orders\")\n# Always sees consistent state, never partial writes\n```\n\n### 2. Merge (Upsert) Operations\n**Krishna's Most Common Pattern:**\n```python\n# Load incremental data\ndf_updates = spark.read.format(\"delta\").load(\"/mnt/bronze/o",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 208
    },
    "id": "de_de_pyspark_databricks_208"
  },
  {
    "text": "stent state, never partial writes\n```\n\n### 2. Merge (Upsert) Operations\n**Krishna's Most Common Pattern:**\n```python\n# Load incremental data\ndf_updates = spark.read.format(\"delta\").load(\"/mnt/bronze/orders_incremental\")\n\n# Get existing Delta table\ndelta_table = DeltaTable.forPath(spark, \"/mnt/silver/orders\")\n\n# Upsert with merge\n(\n    delta_table.alias(\"target\")\n    .merge(\n        df_updates.alias(\"source\"),\n        \"target.order_id = source.order_id\"\n    )\n    .whenMatchedUpdate(\n        condition=\"source.updated_at > target.updated_at\",\n        set={\n            \"order_status\": \"source.order_status\",\n            \"order_amount\": \"source.order_amount\",\n            \"updated_at\": \"source.updated_at\"\n        }\n    )\n    .whenNotMatchedInsertAll()\n    .execute()\n)\n```\n\n**Business Use Case - Customer SCD Type 2:**\n```python\n# Slowly Changing Dimension for customer segments\nfrom pyspark.sql import functions as F\n\n# Read new customer data\ndf_new = spark.read.format(\"delta\").load(\"/mnt/bronze",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 209
    },
    "id": "de_de_pyspark_databricks_209"
  },
  {
    "text": "ustomer SCD Type 2:**\n```python\n# Slowly Changing Dimension for customer segments\nfrom pyspark.sql import functions as F\n\n# Read new customer data\ndf_new = spark.read.format(\"delta\").load(\"/mnt/bronze/customers_daily\")\n\n# Read existing dimension\ndelta_table = DeltaTable.forPath(spark, \"/mnt/silver/dim_customer\")\n\n# Prepare SCD Type 2 merge\n(\n    delta_table.alias(\"target\")\n    .merge(\n        df_new.alias(\"source\"),\n        \"target.customer_id = source.customer_id AND target.is_current = true\"\n    )\n    .whenMatchedUpdate(\n        condition=\"target.customer_segment != source.customer_segment\",\n        set={\n            \"is_current\": \"false\",\n            \"end_date\": F.current_date()\n        }\n    )\n    .execute()\n)\n\n# Insert new records for changed customers\ndf_changed = df_new.join(\n    delta_table.toDF().filter(\"is_current = false\"),\n    \"customer_id\"\n).select(\n    \"customer_id\",\n    \"customer_segment\",\n    F.current_date().alias(\"start_date\"),\n    F.lit(\"9999-12-31\").cast(\"date\").ali",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 210
    },
    "id": "de_de_pyspark_databricks_210"
  },
  {
    "text": "lta_table.toDF().filter(\"is_current = false\"),\n    \"customer_id\"\n).select(\n    \"customer_id\",\n    \"customer_segment\",\n    F.current_date().alias(\"start_date\"),\n    F.lit(\"9999-12-31\").cast(\"date\").alias(\"end_date\"),\n    F.lit(True).alias(\"is_current\")\n)\n\ndf_changed.write.format(\"delta\").mode(\"append\").save(\"/mnt/silver/dim_customer\")\n```\n\n### 3. Time Travel\n**Krishna's Debugging & Recovery:**\n```python\n# Read historical version\ndf_yesterday = spark.read.format(\"delta\") \\\n    .option(\"versionAsOf\", 10) \\\n    .load(\"/mnt/silver/orders\")\n\n# Read by timestamp\ndf_last_week = spark.read.format(\"delta\") \\\n    .option(\"timestampAsOf\", \"2024-01-01 00:00:00\") \\\n    .load(\"/mnt/silver/orders\")\n\n# Show table history\nhistory_df = spark.sql(\"DESCRIBE HISTORY delta.`/mnt/silver/orders`\")\nhistory_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(20, False)\n\n# Restore to previous version\nspark.sql(\"RESTORE TABLE delta.`/mnt/silver/orders` TO VERSION AS OF 10\")\n```\n\n**Real Scenario",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 211
    },
    "id": "de_de_pyspark_databricks_211"
  },
  {
    "text": "t(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(20, False)\n\n# Restore to previous version\nspark.sql(\"RESTORE TABLE delta.`/mnt/silver/orders` TO VERSION AS OF 10\")\n```\n\n**Real Scenario:**\nBad data got loaded on Friday night. Monday morning, I used time travel to compare Friday vs Monday, identified the issue, and restored Friday's version. Business users were happy!\n\n### 4. Optimize & Vacuum\n**Krishna's Maintenance Schedule:**\n```python\n# Optimize - compact small files\nspark.sql(\"OPTIMIZE delta.`/mnt/silver/orders`\")\n\n# Z-Order for faster queries\nspark.sql(\"\"\"\n    OPTIMIZE delta.`/mnt/silver/orders`\n    ZORDER BY (customer_id, order_date)\n\"\"\")\n\n# Vacuum - remove old files (run weekly)\nspark.sql(\"\"\"\n    VACUUM delta.`/mnt/silver/orders` RETAIN 168 HOURS\n\"\"\")\n\n# Check table statistics\nspark.sql(\"DESCRIBE DETAIL delta.`/mnt/silver/orders`\").show(False)\n```\n\n**Business Impact:**\n- Pre-optimize: 100K small files, 5-minute queries\n- Post-optimize: 1K optimized files, 30-secon",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 212
    },
    "id": "de_de_pyspark_databricks_212"
  },
  {
    "text": "tatistics\nspark.sql(\"DESCRIBE DETAIL delta.`/mnt/silver/orders`\").show(False)\n```\n\n**Business Impact:**\n- Pre-optimize: 100K small files, 5-minute queries\n- Post-optimize: 1K optimized files, 30-second queries\n- Storage: Reduced 2TB to 1.5TB after vacuum\n\n## Structured Streaming\n\n### Real-Time Pipelines\n**Krishna's Streaming Use Case - Click Events:**\n```python\n# Read from Kafka/Event Hub\ndf_stream = (\n    spark.readStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", \"eventhub.servicebus.windows.net:9093\")\n    .option(\"subscribe\", \"clickstream-events\")\n    .load()\n)\n\n# Parse JSON\nfrom pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n\nschema = StructType([\n    StructField(\"user_id\", StringType()),\n    StructField(\"event_type\", StringType()),\n    StructField(\"product_id\", StringType()),\n    StructField(\"timestamp\", TimestampType()),\n    StructField(\"session_id\", StringType())\n])\n\ndf_parsed = (\n    df_stream\n    .select(F.from_json(F.",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 213
    },
    "id": "de_de_pyspark_databricks_213"
  },
  {
    "text": "e()),\n    StructField(\"product_id\", StringType()),\n    StructField(\"timestamp\", TimestampType()),\n    StructField(\"session_id\", StringType())\n])\n\ndf_parsed = (\n    df_stream\n    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n    .select(\"data.*\")\n)\n\n# Windowed aggregations\ndf_aggregated = (\n    df_parsed\n    .withWatermark(\"timestamp\", \"10 minutes\")\n    .groupBy(\n        F.window(\"timestamp\", \"5 minutes\", \"1 minute\"),\n        \"product_id\"\n    )\n    .agg(\n        F.count(\"*\").alias(\"view_count\"),\n        F.countDistinct(\"user_id\").alias(\"unique_users\")\n    )\n)\n\n# Write to Delta\nquery = (\n    df_aggregated.writeStream\n    .format(\"delta\")\n    .outputMode(\"append\")\n    .option(\"checkpointLocation\", \"/mnt/checkpoints/product_views\")\n    .trigger(processingTime=\"1 minute\")\n    .start(\"/mnt/silver/product_view_metrics\")\n)\n```\n\n## Databricks Platform\n\n### 1. Cluster Configuration\n**Krishna's Production Clusters:**\n```json\n{\n  \"cluster_name\": \"production-etl-cluster\"",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 214
    },
    "id": "de_de_pyspark_databricks_214"
  },
  {
    "text": "ute\")\n    .start(\"/mnt/silver/product_view_metrics\")\n)\n```\n\n## Databricks Platform\n\n### 1. Cluster Configuration\n**Krishna's Production Clusters:**\n```json\n{\n  \"cluster_name\": \"production-etl-cluster\",\n  \"spark_version\": \"13.3.x-scala2.12\",\n  \"node_type_id\": \"Standard_DS4_v2\",\n  \"driver_node_type_id\": \"Standard_DS4_v2\",\n  \"autoscale\": {\n    \"min_workers\": 2,\n    \"max_workers\": 8\n  },\n  \"autotermination_minutes\": 30,\n  \"spark_conf\": {\n    \"spark.sql.adaptive.enabled\": \"true\",\n    \"spark.databricks.delta.preview.enabled\": \"true\",\n    \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\"\n  },\n  \"spark_env_vars\": {\n    \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\n  }\n}\n```\n\n### 2. Notebook Best Practices\n**Krishna's Notebook Structure:**\n```python\n# Cell 1: Imports & Setup\nfrom pyspark.sql import functions as F, Window\nfrom pyspark.sql.types import *\nfrom delta.tables import DeltaTable\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLog",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 215
    },
    "id": "de_de_pyspark_databricks_215"
  },
  {
    "text": "k.sql import functions as F, Window\nfrom pyspark.sql.types import *\nfrom delta.tables import DeltaTable\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Cell 2: Parameters (use Databricks widgets)\ndbutils.widgets.text(\"execution_date\", \"2024-01-01\")\ndbutils.widgets.dropdown(\"environment\", \"prod\", [\"dev\", \"staging\", \"prod\"])\n\nexecution_date = dbutils.widgets.get(\"execution_date\")\nenvironment = dbutils.widgets.get(\"environment\")\n\nlogger.info(f\"Running pipeline for {execution_date} in {environment}\")\n\n# Cell 3: Functions\ndef read_source_data(path, date):\n    \"\"\"Read source data with error handling\"\"\"\n    try:\n        return spark.read.format(\"delta\").load(f\"{path}/date={date}\")\n    except Exception as e:\n        logger.error(f\"Failed to read {path}: {e}\")\n        raise\n\n# Cell 4: Main Pipeline Logic\ndf_orders = read_source_data(\"/mnt/raw/orders\", execution_date)\ndf_clean = transform_orders(df_orders)\nwrite_to_silver(df_clean,",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 216
    },
    "id": "de_de_pyspark_databricks_216"
  },
  {
    "text": "iled to read {path}: {e}\")\n        raise\n\n# Cell 4: Main Pipeline Logic\ndf_orders = read_source_data(\"/mnt/raw/orders\", execution_date)\ndf_clean = transform_orders(df_orders)\nwrite_to_silver(df_clean, \"/mnt/silver/orders\")\n\n# Cell 5: Quality Checks\nrow_count = df_clean.count()\nassert row_count > 0, \"No data processed!\"\nlogger.info(f\"Processed {row_count:,} orders\")\n```\n\n### 3. Job Orchestration\n**Krishna's Job Configuration:**\n```python\n# Using Databricks Jobs API\njob_config = {\n    \"name\": \"Daily Orders ETL\",\n    \"tasks\": [\n        {\n            \"task_key\": \"bronze_ingestion\",\n            \"notebook_task\": {\n                \"notebook_path\": \"/Pipelines/Bronze/ingest_orders\",\n                \"base_parameters\": {\n                    \"execution_date\": \"{{job.trigger_date}}\"\n                }\n            },\n            \"new_cluster\": {\n                \"spark_version\": \"13.3.x-scala2.12\",\n                \"node_type_id\": \"Standard_DS3_v2\",\n                \"num_workers\": 4\n            }\n     ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 217
    },
    "id": "de_de_pyspark_databricks_217"
  },
  {
    "text": "            },\n            \"new_cluster\": {\n                \"spark_version\": \"13.3.x-scala2.12\",\n                \"node_type_id\": \"Standard_DS3_v2\",\n                \"num_workers\": 4\n            }\n        },\n        {\n            \"task_key\": \"silver_transformation\",\n            \"depends_on\": [{\"task_key\": \"bronze_ingestion\"}],\n            \"notebook_task\": {\n                \"notebook_path\": \"/Pipelines/Silver/transform_orders\",\n                \"base_parameters\": {\n                    \"execution_date\": \"{{job.trigger_date}}\"\n                }\n            },\n            \"existing_cluster_id\": \"1234-567890-abc123\"\n        },\n        {\n            \"task_key\": \"gold_aggregation\",\n            \"depends_on\": [{\"task_key\": \"silver_transformation\"}],\n            \"notebook_task\": {\n                \"notebook_path\": \"/Pipelines/Gold/aggregate_metrics\",\n                \"base_parameters\": {\n                    \"execution_date\": \"{{job.trigger_date}}\"\n                }\n            },\n            \"existin",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 218
    },
    "id": "de_de_pyspark_databricks_218"
  },
  {
    "text": "book_path\": \"/Pipelines/Gold/aggregate_metrics\",\n                \"base_parameters\": {\n                    \"execution_date\": \"{{job.trigger_date}}\"\n                }\n            },\n            \"existing_cluster_id\": \"1234-567890-abc123\"\n        }\n    ],\n    \"schedule\": {\n        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # 2 AM daily\n        \"timezone_id\": \"America/Chicago\"\n    },\n    \"email_notifications\": {\n        \"on_failure\": [\"data-team@walgreens.com\"]\n    },\n    \"max_concurrent_runs\": 1\n}\n```\n\n## Unity Catalog\n\n### Data Governance\n**Krishna's UC Implementation:**\n```python\n# Create catalog and schemas\nspark.sql(\"CREATE CATALOG IF NOT EXISTS walgreens_prod\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS walgreens_prod.raw\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS walgreens_prod.silver\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS walgreens_prod.gold\")\n\n# Create managed table\nspark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS walgreens_prod.silver.orders (\n        order_id STRING,\n        customer_id ",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 219
    },
    "id": "de_de_pyspark_databricks_219"
  },
  {
    "text": "sql(\"CREATE SCHEMA IF NOT EXISTS walgreens_prod.gold\")\n\n# Create managed table\nspark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS walgreens_prod.silver.orders (\n        order_id STRING,\n        customer_id STRING,\n        order_date DATE,\n        order_amount DECIMAL(18,2),\n        created_at TIMESTAMP\n    )\n    USING DELTA\n    PARTITIONED BY (order_date)\n    LOCATION '/mnt/silver/orders'\n\"\"\")\n\n# Grant permissions\nspark.sql(\"GRANT SELECT ON TABLE walgreens_prod.silver.orders TO `analytics-team@walgreens.com`\")\nspark.sql(\"GRANT ALL PRIVILEGES ON SCHEMA walgreens_prod.gold TO `data-engineers@walgreens.com`\")\n\n# Row-level security\nspark.sql(\"\"\"\n    CREATE FUNCTION walgreens_prod.mask_customer_id(customer_id STRING, user_role STRING)\n    RETURNS STRING\n    RETURN CASE \n        WHEN user_role = 'admin' THEN customer_id\n        ELSE CONCAT('***', RIGHT(customer_id, 4))\n    END\n\"\"\")\n```\n\nThis comprehensive guide covers everything I use daily at Walgreens to build reliable, performant data pipelines",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 220
    },
    "id": "de_de_pyspark_databricks_220"
  },
  {
    "text": "n' THEN customer_id\n        ELSE CONCAT('***', RIGHT(customer_id, 4))\n    END\n\"\"\")\n```\n\nThis comprehensive guide covers everything I use daily at Walgreens to build reliable, performant data pipelines with PySpark and Databricks!\n\n",
    "metadata": {
      "persona": "de",
      "file_path": "kb/data_engineering/de_pyspark_databricks.md",
      "file_name": "de_pyspark_databricks.md",
      "chunk_index": 221
    },
    "id": "de_de_pyspark_databricks_221"
  },
  {
    "text": "---\ntags: [deep-learning, nlp, tensorflow, keras, pytorch, transformers, bert, text-processing]\npersona: ml\n---\n\n# Deep Learning & NLP Projects - Krishna's Experience\n\n## Text Classification for Customer Feedback\n\n### Sentiment Analysis Pipeline\n**Krishna's Implementation:**\nSo at Walgreens, we were getting thousands of customer reviews and feedback every day. I built this NLP pipeline to automatically classify them as positive, negative, or neutral. It actually helped the customer service team prioritize issues way better.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Download required NLTK data\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef preprocess_text(text):\n    \"\"\"\n    Clean and prep",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 0
    },
    "id": "ai_deep_learning_nlp_222"
  },
  {
    "text": ".corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Download required NLTK data\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef preprocess_text(text):\n    \"\"\"\n    Clean and preprocess text data\n    This is what I used in production\n    \"\"\"\n    # Lowercase\n    text = text.lower()\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    \n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Tokenize\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Join back\n    return ' '.join(tokens)\n\n# Load and preprocess data\ndf = pd.read_csv('customer_feedback.csv')\ndf['cleaned_text'] = df['feedback_text'].apply(preprocess_text)\n\nprint(f\"Total feedback: {len(df)}\")\nprint(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n\n# Prepare data\nX = df['cleaned_text'].valu",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 1
    },
    "id": "ai_deep_learning_nlp_223"
  },
  {
    "text": "text'] = df['feedback_text'].apply(preprocess_text)\n\nprint(f\"Total feedback: {len(df)}\")\nprint(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n\n# Prepare data\nX = df['cleaned_text'].values\ny = df['sentiment'].values\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n```\n\n### Building LSTM Model\n**Krishna's Deep Learning Approach:**\n```python\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Tokenization\nmax_words = 10000\nmax_len = 200\n\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\n\n# Convert text to sequences\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_te",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 2
    },
    "id": "ai_deep_learning_nlp_224"
  },
  {
    "text": "(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\n\n# Convert text to sequences\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n\n# Pad sequences\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n\nprint(f\"Training sequences shape: {X_train_pad.shape}\")\nprint(f\"Testing sequences shape: {X_test_pad.shape}\")\n\n# Build LSTM model\ndef build_lstm_model(vocab_size, embedding_dim=128, max_len=200, num_classes=3):\n    \"\"\"\n    Build LSTM model for sentiment classification\n    This architecture worked really well for me\n    \"\"\"\n    model = keras.Sequential([\n        # Embedding layer\n        layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n        \n        # Spatial dropout to prevent overfitting\n        layers.SpatialDropout1D(0.2),\n        \n        # Bidirectional LSTM - thi",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 3
    },
    "id": "ai_deep_learning_nlp_225"
  },
  {
    "text": ".Embedding(vocab_size, embedding_dim, input_length=max_len),\n        \n        # Spatial dropout to prevent overfitting\n        layers.SpatialDropout1D(0.2),\n        \n        # Bidirectional LSTM - this helped capture context better\n        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n        layers.Dropout(0.3),\n        \n        # Another LSTM layer\n        layers.Bidirectional(layers.LSTM(32)),\n        layers.Dropout(0.3),\n        \n        # Dense layers\n        layers.Dense(64, activation='relu'),\n        layers.Dropout(0.3),\n        \n        # Output layer\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# Create model\nmodel = build_lstm_model(\n    vocab_size=max_words,\n    embedding_dim=128,\n    max_len=max_len,\n    num_classes=len(label_encoder.classes_)\n)\n\nmodel.summary()\n\n# Callbacks\nearly_stopp",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 4
    },
    "id": "ai_deep_learning_nlp_226"
  },
  {
    "text": "\n\n# Create model\nmodel = build_lstm_model(\n    vocab_size=max_words,\n    embedding_dim=128,\n    max_len=max_len,\n    num_classes=len(label_encoder.classes_)\n)\n\nmodel.summary()\n\n# Callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True\n)\n\ncheckpoint = ModelCheckpoint(\n    'best_sentiment_model.h5',\n    monitor='val_accuracy',\n    save_best_only=True,\n    verbose=1\n)\n\n# Train model\nhistory = model.fit(\n    X_train_pad, y_train,\n    validation_split=0.2,\n    epochs=20,\n    batch_size=32,\n    callbacks=[early_stopping, checkpoint],\n    verbose=1\n)\n\n# Evaluate\ntest_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\nprint(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\n```\n\n### Using Pre-trained BERT\n**Krishna's Transfer Learning Approach:**\nSo I also tried using BERT and it actually performed better than my LSTM model. The accuracy went from 85% to 91%.\n\n```python\nfrom transformers import BertTokeniz",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 5
    },
    "id": "ai_deep_learning_nlp_227"
  },
  {
    "text": "ishna's Transfer Learning Approach:**\nSo I also tried using BERT and it actually performed better than my LSTM model. The accuracy went from 85% to 91%.\n\n```python\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures\nimport tensorflow as tf\n\n# Load pre-trained BERT\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nbert_model = TFBertForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(label_encoder.classes_)\n)\n\ndef convert_data_to_examples(texts, labels):\n    \"\"\"Convert text data to BERT input format\"\"\"\n    examples = []\n    for text, label in zip(texts, labels):\n        examples.append(\n            InputExample(guid=None, text_a=text, text_b=None, label=label)\n        )\n    return examples\n\ndef convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    \"\"\"Convert examples to TensorFlow dataset\"\"\"\n    features = []\n    \n    for example in e",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 6
    },
    "id": "ai_deep_learning_nlp_228"
  },
  {
    "text": "el)\n        )\n    return examples\n\ndef convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    \"\"\"Convert examples to TensorFlow dataset\"\"\"\n    features = []\n    \n    for example in examples:\n        input_dict = tokenizer.encode_plus(\n            example.text_a,\n            add_special_tokens=True,\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='tf'\n        )\n        \n        features.append({\n            'input_ids': input_dict['input_ids'].numpy()[0],\n            'attention_mask': input_dict['attention_mask'].numpy()[0],\n            'label': example.label\n        })\n    \n    def gen():\n        for feature in features:\n            yield (\n                {\n                    'input_ids': feature['input_ids'],\n                    'attention_mask': feature['attention_mask']\n                },\n                feature['label']\n            )\n    \n    retu",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 7
    },
    "id": "ai_deep_learning_nlp_229"
  },
  {
    "text": "{\n                    'input_ids': feature['input_ids'],\n                    'attention_mask': feature['attention_mask']\n                },\n                feature['label']\n            )\n    \n    return tf.data.Dataset.from_generator(\n        gen,\n        ({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int64),\n        (\n            {\n                'input_ids': tf.TensorShape([None]),\n                'attention_mask': tf.TensorShape([None])\n            },\n            tf.TensorShape([])\n        )\n    )\n\n# Prepare data for BERT\ntrain_examples = convert_data_to_examples(X_train, y_train)\ntest_examples = convert_data_to_examples(X_test, y_test)\n\ntrain_dataset = convert_examples_to_tf_dataset(train_examples, tokenizer)\ntest_dataset = convert_examples_to_tf_dataset(test_examples, tokenizer)\n\ntrain_dataset = train_dataset.shuffle(100).batch(16).repeat(2)\ntest_dataset = test_dataset.batch(16)\n\n# Fine-tune BERT\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss = tf.keras.l",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 8
    },
    "id": "ai_deep_learning_nlp_230"
  },
  {
    "text": "izer)\n\ntrain_dataset = train_dataset.shuffle(100).batch(16).repeat(2)\ntest_dataset = test_dataset.batch(16)\n\n# Fine-tune BERT\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n# Train\nhistory = bert_model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=3,  # BERT fine-tuning usually needs fewer epochs\n    verbose=1\n)\n\nprint(f\"BERT model achieved {history.history['val_accuracy'][-1]:.4f} validation accuracy\")\n```\n\n## Named Entity Recognition (NER)\n\n### Custom NER for Medical Text\n**Krishna's Healthcare NER Project:**\n```python\nimport spacy\nfrom spacy.training import Example\nfrom spacy.util import minibatch, compounding\nimport random\n\ndef train_custom_ner(training_data, model=None, n_iter=30):\n    \"\"\"\n    Train custom NER model for medical entities\n    I used this to extract drug names and dosages from prescriptions\n ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 9
    },
    "id": "ai_deep_learning_nlp_231"
  },
  {
    "text": "mport random\n\ndef train_custom_ner(training_data, model=None, n_iter=30):\n    \"\"\"\n    Train custom NER model for medical entities\n    I used this to extract drug names and dosages from prescriptions\n    \"\"\"\n    # Create blank model or load existing\n    if model is not None:\n        nlp = spacy.load(model)\n    else:\n        nlp = spacy.blank(\"en\")\n    \n    # Add NER pipeline if it doesn't exist\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.add_pipe(\"ner\")\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # Add labels\n    for _, annotations in training_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n    \n    # Disable other pipes during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):\n        optimizer = nlp.begin_training()\n        \n        for iteration in range(n_iter):\n            random.shuffle(training_data)\n            losses = {}\n            \n            # Bat",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 10
    },
    "id": "ai_deep_learning_nlp_232"
  },
  {
    "text": "other_pipes):\n        optimizer = nlp.begin_training()\n        \n        for iteration in range(n_iter):\n            random.shuffle(training_data)\n            losses = {}\n            \n            # Batch the examples\n            batches = minibatch(training_data, size=compounding(4.0, 32.0, 1.001))\n            \n            for batch in batches:\n                examples = []\n                for text, annotations in batch:\n                    doc = nlp.make_doc(text)\n                    example = Example.from_dict(doc, annotations)\n                    examples.append(example)\n                \n                nlp.update(examples, drop=0.5, losses=losses)\n            \n            print(f\"Iteration {iteration + 1}, Losses: {losses}\")\n    \n    return nlp\n\n# Training data format\nTRAIN_DATA = [\n    (\"Patient prescribed Metformin 500mg twice daily\", {\n        \"entities\": [(19, 28, \"DRUG\"), (29, 35, \"DOSAGE\")]\n    }),\n    (\"Take Lisinopril 10mg once per day\", {\n        \"entities\": [(5, 15, \"DRUG\"",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 11
    },
    "id": "ai_deep_learning_nlp_233"
  },
  {
    "text": " (\"Patient prescribed Metformin 500mg twice daily\", {\n        \"entities\": [(19, 28, \"DRUG\"), (29, 35, \"DOSAGE\")]\n    }),\n    (\"Take Lisinopril 10mg once per day\", {\n        \"entities\": [(5, 15, \"DRUG\"), (16, 20, \"DOSAGE\")]\n    }),\n    # More training examples...\n]\n\n# Train model\nnlp_ner = train_custom_ner(TRAIN_DATA, n_iter=30)\n\n# Save model\nnlp_ner.to_disk(\"medical_ner_model\")\n\n# Use model for prediction\ndef extract_medical_entities(text):\n    \"\"\"Extract medical entities from text\"\"\"\n    doc = nlp_ner(text)\n    entities = []\n    for ent in doc.ents:\n        entities.append({\n            'text': ent.text,\n            'label': ent.label_,\n            'start': ent.start_char,\n            'end': ent.end_char\n        })\n    return entities\n\n# Test\ntest_text = \"Patient should take Aspirin 81mg daily and Atorvastatin 20mg at bedtime\"\nentities = extract_medical_entities(test_text)\nprint(f\"Extracted entities: {entities}\")\n```\n\n## Text Generation and Summarization\n\n### Document Summarization\n**",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 12
    },
    "id": "ai_deep_learning_nlp_234"
  },
  {
    "text": "daily and Atorvastatin 20mg at bedtime\"\nentities = extract_medical_entities(test_text)\nprint(f\"Extracted entities: {entities}\")\n```\n\n## Text Generation and Summarization\n\n### Document Summarization\n**Krishna's Approach:**\n```python\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n\ndef summarize_documents(documents, max_length=150, min_length=50):\n    \"\"\"\n    Summarize long documents\n    I used this to summarize customer feedback reports\n    \"\"\"\n    # Load pre-trained summarization model\n    model_name = \"facebook/bart-large-cnn\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    \n    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n    \n    summaries = []\n    for doc in documents:\n        # Truncate if too long\n        if len(doc.split()) > 1024:\n            doc = ' '.join(doc.split()[:1024])\n        \n        summary = summarizer(\n            doc,\n            max_leng",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 13
    },
    "id": "ai_deep_learning_nlp_235"
  },
  {
    "text": "ocuments:\n        # Truncate if too long\n        if len(doc.split()) > 1024:\n            doc = ' '.join(doc.split()[:1024])\n        \n        summary = summarizer(\n            doc,\n            max_length=max_length,\n            min_length=min_length,\n            do_sample=False\n        )\n        \n        summaries.append(summary[0]['summary_text'])\n    \n    return summaries\n\n# Example usage\nlong_feedback = \"\"\"\nCustomer called regarding prescription refill delay. They mentioned waiting for 3 days \nwithout receiving notification. The customer was frustrated but remained polite throughout \nthe conversation. They expressed concern about running out of medication. After checking \nthe system, we found the prescription was pending doctor approval. We expedited the \napproval process and offered same-day pickup. Customer was satisfied with the resolution \nand thanked the team for quick action.\n\"\"\"\n\nsummary = summarize_documents([long_feedback])\nprint(f\"Summary: {summary[0]}\")\n```\n\n## Production ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 14
    },
    "id": "ai_deep_learning_nlp_236"
  },
  {
    "text": "e-day pickup. Customer was satisfied with the resolution \nand thanked the team for quick action.\n\"\"\"\n\nsummary = summarize_documents([long_feedback])\nprint(f\"Summary: {summary[0]}\")\n```\n\n## Production Deployment\n\n### FastAPI Service for NLP Models\n**Krishna's Production Setup:**\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport tensorflow as tf\nfrom transformers import pipeline\nimport joblib\nimport numpy as np\n\napp = FastAPI(title=\"NLP Service API\")\n\n# Load models at startup\nsentiment_model = None\ntokenizer = None\nsummarizer = None\n\n@app.on_event(\"startup\")\nasync def load_models():\n    \"\"\"Load all models when service starts\"\"\"\n    global sentiment_model, tokenizer, summarizer\n    \n    # Load sentiment model\n    sentiment_model = tf.keras.models.load_model('best_sentiment_model.h5')\n    tokenizer = joblib.load('tokenizer.pkl')\n    \n    # Load summarization model\n    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n    \n    p",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 15
    },
    "id": "ai_deep_learning_nlp_237"
  },
  {
    "text": "model('best_sentiment_model.h5')\n    tokenizer = joblib.load('tokenizer.pkl')\n    \n    # Load summarization model\n    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n    \n    print(\"All models loaded successfully\")\n\nclass SentimentRequest(BaseModel):\n    text: str\n\nclass SentimentResponse(BaseModel):\n    text: str\n    sentiment: str\n    confidence: float\n\n@app.post(\"/predict/sentiment\", response_model=SentimentResponse)\nasync def predict_sentiment(request: SentimentRequest):\n    \"\"\"\n    Predict sentiment of text\n    This endpoint handles 1000+ requests per day\n    \"\"\"\n    try:\n        # Preprocess\n        cleaned_text = preprocess_text(request.text)\n        \n        # Tokenize and pad\n        sequence = tokenizer.texts_to_sequences([cleaned_text])\n        padded = tf.keras.preprocessing.sequence.pad_sequences(\n            sequence, maxlen=200, padding='post'\n        )\n        \n        # Predict\n        prediction = sentiment_model.predict(padded)\n        sentime",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 16
    },
    "id": "ai_deep_learning_nlp_238"
  },
  {
    "text": "f.keras.preprocessing.sequence.pad_sequences(\n            sequence, maxlen=200, padding='post'\n        )\n        \n        # Predict\n        prediction = sentiment_model.predict(padded)\n        sentiment_idx = np.argmax(prediction[0])\n        confidence = float(prediction[0][sentiment_idx])\n        \n        sentiments = ['negative', 'neutral', 'positive']\n        sentiment = sentiments[sentiment_idx]\n        \n        return SentimentResponse(\n            text=request.text,\n            sentiment=sentiment,\n            confidence=confidence\n        )\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\nclass SummarizationRequest(BaseModel):\n    text: str\n    max_length: int = 150\n    min_length: int = 50\n\nclass SummarizationResponse(BaseModel):\n    original_text: str\n    summary: str\n    compression_ratio: float\n\n@app.post(\"/summarize\", response_model=SummarizationResponse)\nasync def summarize_text(request: SummarizationRequest):\n    \"\"\"Summarize lo",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 17
    },
    "id": "ai_deep_learning_nlp_239"
  },
  {
    "text": "inal_text: str\n    summary: str\n    compression_ratio: float\n\n@app.post(\"/summarize\", response_model=SummarizationResponse)\nasync def summarize_text(request: SummarizationRequest):\n    \"\"\"Summarize long text\"\"\"\n    try:\n        summary = summarizer(\n            request.text,\n            max_length=request.max_length,\n            min_length=request.min_length,\n            do_sample=False\n        )\n        \n        summary_text = summary[0]['summary_text']\n        compression_ratio = len(summary_text) / len(request.text)\n        \n        return SummarizationResponse(\n            original_text=request.text,\n            summary=summary_text,\n            compression_ratio=compression_ratio\n        )\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"models_loaded\": sentiment_model is not None and summarizer is not None\n   ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 18
    },
    "id": "ai_deep_learning_nlp_240"
  },
  {
    "text": "p.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"models_loaded\": sentiment_model is not None and summarizer is not None\n    }\n\n# Run with: uvicorn nlp_service:app --host 0.0.0.0 --port 8000\n```\n\n## Real-World Challenges\n\n### Handling Multiple Languages\n**Krishna's Solution:**\n```python\nfrom langdetect import detect\nfrom transformers import MarianMTModel, MarianTokenizer\n\ndef detect_and_translate(text, target_lang='en'):\n    \"\"\"\n    Detect language and translate if needed\n    We had customers submitting feedback in Spanish\n    \"\"\"\n    try:\n        source_lang = detect(text)\n        \n        if source_lang == target_lang:\n            return text, source_lang\n        \n        # Load translation model\n        model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n        tokenizer = MarianTokenizer.from_pretrained(model_name)\n        model = MarianMTModel.from_pretrained(model_name)\n        \n        # Tr",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 19
    },
    "id": "ai_deep_learning_nlp_241"
  },
  {
    "text": "me = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n        tokenizer = MarianTokenizer.from_pretrained(model_name)\n        model = MarianMTModel.from_pretrained(model_name)\n        \n        # Translate\n        translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n        translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n        \n        return translated_text, source_lang\n    \n    except Exception as e:\n        print(f\"Translation error: {e}\")\n        return text, 'unknown'\n```\n\n## Interview Talking Points\n\n### Technical Achievements:\n- Built sentiment analysis model with 91% accuracy using BERT\n- Deployed NLP service handling 1000+ requests daily\n- Implemented custom NER model for medical text extraction\n- Reduced manual review time by 70% through automated classification\n- Achieved 85% accuracy on LSTM model, improved to 91% with BERT\n\n### Technologies Used:\n- **Deep Learning**: TensorFlow, Keras, PyTorch\n- **NLP**: Transfor",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 20
    },
    "id": "ai_deep_learning_nlp_242"
  },
  {
    "text": "ime by 70% through automated classification\n- Achieved 85% accuracy on LSTM model, improved to 91% with BERT\n\n### Technologies Used:\n- **Deep Learning**: TensorFlow, Keras, PyTorch\n- **NLP**: Transformers, BERT, spaCy, NLTK\n- **Deployment**: FastAPI, Docker, Kubernetes\n- **Cloud**: AWS SageMaker, Azure ML\n- **Monitoring**: Prometheus, Grafana, CloudWatch\n\n### Problem-Solving Examples:\n- Handled imbalanced text data using data augmentation\n- Optimized inference latency from 2s to 200ms\n- Implemented multilingual support for customer feedback\n- Dealt with model drift by implementing retraining pipelines\n",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/deep_learning_nlp.md",
      "file_name": "deep_learning_nlp.md",
      "chunk_index": 21
    },
    "id": "ai_deep_learning_nlp_243"
  },
  {
    "text": "---\ntags: [mlops, mlflow, deployment, monitoring]\n---\n\n# MLOps Pipeline Architecture\n\n## Overview\nMLOps (Machine Learning Operations) encompasses the practices and tools needed to deploy, monitor, and maintain machine learning models in production environments.\n\n## Core Components\n\n### 1. Model Development\n- **Experiment Tracking**: Use MLflow, Weights & Biases, or TensorBoard\n- **Version Control**: Track code, data, and model versions\n- **Reproducibility**: Ensure consistent environments and dependencies\n\n### 2. Model Training Pipeline\n- **Data Pipeline**: Automated data ingestion, validation, and preprocessing\n- **Training Orchestration**: Use Airflow, Prefect, or Kubeflow Pipelines\n- **Hyperparameter Optimization**: Automated tuning with Optuna or Ray Tune\n- **Model Validation**: Automated testing and performance evaluation\n\n### 3. Model Deployment\n- **Containerization**: Package models in Docker containers\n- **Orchestration**: Deploy on Kubernetes or cloud services\n- **API Gateway*",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 22
    },
    "id": "ai_mlops_pipeline_244"
  },
  {
    "text": "ted testing and performance evaluation\n\n### 3. Model Deployment\n- **Containerization**: Package models in Docker containers\n- **Orchestration**: Deploy on Kubernetes or cloud services\n- **API Gateway**: Expose models through REST or GraphQL APIs\n- **Load Balancing**: Distribute traffic across multiple model instances\n\n### 4. Model Monitoring\n- **Performance Monitoring**: Track accuracy, latency, and throughput\n- **Data Drift Detection**: Monitor input data distribution changes\n- **Model Drift Detection**: Track model performance degradation\n- **Alerting**: Set up notifications for critical issues\n\n## MLflow Integration\n\n### Experiment Tracking\n```python\nimport mlflow\n\n# Start experiment\nmlflow.set_experiment(\"customer-churn-prediction\")\n\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_param(\"epochs\", 100)\n    \n    # Train model\n    model = train_model(X_train, y_train)\n    \n    # Log metrics\n    accuracy = evaluate_model(model, X",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 23
    },
    "id": "ai_mlops_pipeline_245"
  },
  {
    "text": "flow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_param(\"epochs\", 100)\n    \n    # Train model\n    model = train_model(X_train, y_train)\n    \n    # Log metrics\n    accuracy = evaluate_model(model, X_test, y_test)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n```\n\n### Model Registry\n- **Model Versioning**: Track different model versions\n- **Staging Workflow**: Promote models through dev → staging → prod\n- **Model Metadata**: Store model descriptions, tags, and lineage\n- **Automated Deployment**: Trigger deployments based on model performance\n\n## Kubernetes Deployment\n\n### Model Serving with KServe\n```yaml\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: customer-churn-model\nspec:\n  predictor:\n    sklearn:\n      storageUri: gs://model-registry/churn-model/1\n      resources:\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        limits:\n          cpu: \"2\"\n          memory: \"4Gi\"\n`",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 24
    },
    "id": "ai_mlops_pipeline_246"
  },
  {
    "text": " sklearn:\n      storageUri: gs://model-registry/churn-model/1\n      resources:\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        limits:\n          cpu: \"2\"\n          memory: \"4Gi\"\n```\n\n### Auto-scaling Configuration\n- **Horizontal Pod Autoscaler**: Scale based on CPU/memory usage\n- **Vertical Pod Autoscaler**: Optimize resource allocation\n- **Custom Metrics**: Scale based on request latency or queue depth\n\n## Data Pipeline Integration\n\n### Feature Engineering Pipeline\n- **Feature Store**: Centralized feature management (Feast, Tecton)\n- **Feature Validation**: Ensure feature quality and consistency\n- **Feature Serving**: Low-latency feature retrieval for inference\n- **Feature Lineage**: Track feature dependencies and transformations\n\n### Data Validation\n- **Schema Validation**: Ensure data conforms to expected schema\n- **Statistical Validation**: Check data distributions and anomalies\n- **Temporal Validation**: Monitor data freshness and completeness\n- **Quality Metr",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 25
    },
    "id": "ai_mlops_pipeline_247"
  },
  {
    "text": "tion**: Ensure data conforms to expected schema\n- **Statistical Validation**: Check data distributions and anomalies\n- **Temporal Validation**: Monitor data freshness and completeness\n- **Quality Metrics**: Track data quality scores over time\n\n## Monitoring and Observability\n\n### Performance Metrics\n- **Latency**: Track prediction response times\n- **Throughput**: Monitor requests per second\n- **Error Rates**: Track prediction failures and exceptions\n- **Resource Usage**: Monitor CPU, memory, and GPU utilization\n\n### Model Quality Metrics\n- **Accuracy**: Track model performance on validation data\n- **Precision/Recall**: Monitor classification metrics\n- **A/B Testing**: Compare model versions in production\n- **Statistical Significance**: Ensure metric differences are meaningful\n\n### Data Drift Detection\n- **Statistical Tests**: Use KS test, PSI, or KL divergence\n- **Feature Importance**: Track changes in feature importance\n- **Distribution Monitoring**: Visualize feature distributions ov",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 26
    },
    "id": "ai_mlops_pipeline_248"
  },
  {
    "text": "ft Detection\n- **Statistical Tests**: Use KS test, PSI, or KL divergence\n- **Feature Importance**: Track changes in feature importance\n- **Distribution Monitoring**: Visualize feature distributions over time\n- **Alerting Thresholds**: Set up alerts for significant drift\n\n## Best Practices\n\n### Model Lifecycle Management\n- **Automated Retraining**: Trigger retraining based on performance degradation\n- **Gradual Rollout**: Deploy new models to small traffic percentages first\n- **Rollback Strategy**: Quick rollback to previous model version\n- **Blue-Green Deployment**: Zero-downtime model updates\n\n### Security and Compliance\n- **Model Encryption**: Encrypt models at rest and in transit\n- **Access Control**: Implement role-based access to models and data\n- **Audit Logging**: Track all model access and predictions\n- **GDPR Compliance**: Handle data privacy and right to be forgotten\n\n### Cost Optimization\n- **Resource Right-sizing**: Optimize CPU/memory allocation\n- **Spot Instances**: Use s",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 27
    },
    "id": "ai_mlops_pipeline_249"
  },
  {
    "text": "cess and predictions\n- **GDPR Compliance**: Handle data privacy and right to be forgotten\n\n### Cost Optimization\n- **Resource Right-sizing**: Optimize CPU/memory allocation\n- **Spot Instances**: Use spot instances for training workloads\n- **Model Compression**: Reduce model size with quantization or pruning\n- **Caching**: Cache predictions for repeated queries\n\n## Tools and Platforms\n\n### Open Source\n- **MLflow**: Experiment tracking and model registry\n- **Kubeflow**: End-to-end ML workflow orchestration\n- **Seldon Core**: Model serving and deployment\n- **Evidently**: Model monitoring and drift detection\n\n### Cloud Platforms\n- **Azure ML**: Managed ML platform with MLOps capabilities\n- **AWS SageMaker**: End-to-end ML platform\n- **Google Vertex AI**: Managed ML platform with AutoML\n- **Databricks**: Unified analytics platform for ML\n\n## Implementation Strategy\n\n### Phase 1: Foundation\n- Set up experiment tracking and model registry\n- Implement basic CI/CD pipeline\n- Deploy models as RE",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 28
    },
    "id": "ai_mlops_pipeline_250"
  },
  {
    "text": "abricks**: Unified analytics platform for ML\n\n## Implementation Strategy\n\n### Phase 1: Foundation\n- Set up experiment tracking and model registry\n- Implement basic CI/CD pipeline\n- Deploy models as REST APIs\n\n### Phase 2: Automation\n- Automate model training and validation\n- Implement automated deployment pipelines\n- Add basic monitoring and alerting\n\n### Phase 3: Advanced Features\n- Implement advanced monitoring and drift detection\n- Add A/B testing and gradual rollout capabilities\n- Optimize for cost and performance\n",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md",
      "chunk_index": 29
    },
    "id": "ai_mlops_pipeline_251"
  },
  {
    "text": "---\npersona: ai\n---\n\n# Krishna's AI/ML and Generative AI Experience\n\n## Building Production RAG System\n**Krishna's Implementation:**\nSo I built this RAG chatbot that you're actually using right now. The idea was to create an AI assistant that could answer interview questions based on my actual project experience. It's been pretty effective for interview prep.\n\nKrishna built a production RAG system that powers the interview chatbot. The system uses OpenAI embeddings for semantic search and retrieves relevant context from his project experience to answer questions naturally and accurately.\n\n## Krishna's AI/ML and Generative AI Experience\n\n### Core AI/ML Expertise\n\nKrishna has extensive experience in machine learning, deep learning, and generative AI technologies. His expertise spans across multiple domains including natural language processing, computer vision, and large language models.\n\n### Machine Learning Projects\n\nKrishna has worked on various ML projects including:\n- **Predictive A",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 30
    },
    "id": "ai_generative_ai_llm_252"
  },
  {
    "text": "ultiple domains including natural language processing, computer vision, and large language models.\n\n### Machine Learning Projects\n\nKrishna has worked on various ML projects including:\n- **Predictive Analytics**: Built models for customer churn prediction and sales forecasting\n- **Computer Vision**: Developed image classification and object detection systems\n- **NLP Applications**: Created text classification and sentiment analysis models\n- **Recommendation Systems**: Designed collaborative filtering and content-based recommendation engines\n\n### Deep Learning Experience\n\nKrishna's deep learning expertise includes:\n- **Neural Networks**: Experience with CNNs, RNNs, LSTMs, and Transformers\n- **Frameworks**: Proficient in TensorFlow, PyTorch, and Keras\n- **Model Optimization**: Skilled in hyperparameter tuning, regularization, and model compression\n- **Transfer Learning**: Applied pre-trained models for various downstream tasks\n\n### Generative AI and LLMs\n\nKrishna has hands-on experience w",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 31
    },
    "id": "ai_generative_ai_llm_253"
  },
  {
    "text": "rparameter tuning, regularization, and model compression\n- **Transfer Learning**: Applied pre-trained models for various downstream tasks\n\n### Generative AI and LLMs\n\nKrishna has hands-on experience with:\n- **Large Language Models**: Worked with GPT, BERT, and other transformer architectures\n- **RAG Systems**: Built retrieval-augmented generation pipelines for knowledge management\n- **Prompt Engineering**: Designed effective prompts for various AI applications\n- **Fine-tuning**: Customized pre-trained models for specific use cases\n- **AI Applications**: Developed chatbots, content generation systems, and AI-powered tools\n\n### Cloud AI Services\n\nKrishna is experienced with cloud-based AI services:\n- **AWS**: SageMaker, Bedrock, Comprehend, Rekognition\n- **Azure**: Cognitive Services, Machine Learning Studio\n- **Google Cloud**: Vertex AI, AutoML, Natural Language API\n- **OpenAI**: GPT API integration and optimization\n\n### MLOps and Production\n\nKrishna understands the full ML lifecycle:\n-",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 32
    },
    "id": "ai_generative_ai_llm_254"
  },
  {
    "text": "e Learning Studio\n- **Google Cloud**: Vertex AI, AutoML, Natural Language API\n- **OpenAI**: GPT API integration and optimization\n\n### MLOps and Production\n\nKrishna understands the full ML lifecycle:\n- **Model Deployment**: Containerized ML models using Docker and Kubernetes\n- **Monitoring**: Implemented model performance tracking and drift detection\n- **CI/CD**: Automated ML pipeline deployment and testing\n- **Data Pipeline**: Built end-to-end data processing workflows for ML\n\n### Recent Achievements\n\n- Successfully deployed a production RAG system that improved customer support efficiency by 40%\n- Built a multi-modal AI application combining computer vision and NLP\n- Led the migration of ML models from on-premise to cloud infrastructure\n- Developed automated model retraining pipelines that reduced manual effort by 60%\n\n## Prompt Engineering\n\n### Advanced Prompt Techniques\n**Krishna's Learnings:**\nSo one thing I learned is that prompt engineering is actually super important. The way yo",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 33
    },
    "id": "ai_generative_ai_llm_255"
  },
  {
    "text": "t reduced manual effort by 60%\n\n## Prompt Engineering\n\n### Advanced Prompt Techniques\n**Krishna's Learnings:**\nSo one thing I learned is that prompt engineering is actually super important. The way you structure your prompts can make a huge difference in the quality of responses.\n\nKrishna has developed expertise in various prompt engineering techniques:\n- **Chain of Thought**: Breaking down complex problems into logical steps\n- **Few-shot Learning**: Providing examples to guide model behavior\n- **Role-based Prompting**: Assigning specific personas to improve response quality\n- **Context Management**: Effectively managing conversation history and context\n\n### Prompt Optimization Strategies\n\nKrishna uses several strategies to optimize prompts:\n- **Iterative Refinement**: Testing and improving prompts based on results\n- **A/B Testing**: Comparing different prompt variations\n- **Context Compression**: Reducing token usage while maintaining quality\n- **Error Handling**: Building robust prom",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 34
    },
    "id": "ai_generative_ai_llm_256"
  },
  {
    "text": "g prompts based on results\n- **A/B Testing**: Comparing different prompt variations\n- **Context Compression**: Reducing token usage while maintaining quality\n- **Error Handling**: Building robust prompts that handle edge cases\n\n## LLM Fine-Tuning\n\n### Fine-Tuning GPT for Domain-Specific Tasks\n**Krishna's Experience:**\nKrishna has experience fine-tuning large language models for specific business use cases. He understands the process of preparing training data, selecting appropriate base models, and optimizing for specific domains.\n\n### Fine-Tuning Process\n\nKrishna follows a systematic approach to fine-tuning:\n- **Data Preparation**: Cleaning and formatting training data\n- **Model Selection**: Choosing the right base model for the task\n- **Training Configuration**: Setting appropriate hyperparameters\n- **Evaluation**: Testing model performance on validation sets\n- **Deployment**: Integrating fine-tuned models into production systems\n\n## LangChain for Complex Workflows\n\n### Building Mult",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 35
    },
    "id": "ai_generative_ai_llm_257"
  },
  {
    "text": "parameters\n- **Evaluation**: Testing model performance on validation sets\n- **Deployment**: Integrating fine-tuned models into production systems\n\n## LangChain for Complex Workflows\n\n### Building Multi-Step AI Workflows\n**Krishna's LangChain Implementation:**\nKrishna uses LangChain to build complex AI workflows that combine multiple models and tools. He's built systems that can handle multi-step reasoning, tool usage, and memory management.\n\n### Workflow Components\n\nKrishna's LangChain implementations include:\n- **Sequential Chains**: Multi-step processing pipelines\n- **Agent Systems**: AI agents that can use tools and make decisions\n- **Memory Management**: Conversation history and context preservation\n- **Tool Integration**: Connecting AI models with external APIs and databases\n\n### Conversational AI with Memory\n**Krishna's Chatbot Implementation:**\nKrishna has built conversational AI systems with memory capabilities. These systems can maintain context across conversations and provid",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 36
    },
    "id": "ai_generative_ai_llm_258"
  },
  {
    "text": "ersational AI with Memory\n**Krishna's Chatbot Implementation:**\nKrishna has built conversational AI systems with memory capabilities. These systems can maintain context across conversations and provide personalized responses.\n\n## Production Deployment\n\n### FastAPI Service for LLM Applications\n**Krishna's Production Setup:**\nKrishna has deployed LLM applications using FastAPI, creating scalable and reliable services. He understands the challenges of production deployment including rate limiting, error handling, and monitoring.\n\n### Production Considerations\n\nKrishna's production deployments include:\n- **API Design**: RESTful APIs with proper error handling\n- **Rate Limiting**: Managing API usage and costs\n- **Monitoring**: Tracking performance and usage metrics\n- **Scaling**: Handling increased load and traffic\n- **Security**: Implementing proper authentication and authorization\n\n## Cost Optimization\n\n### Strategies Krishna Used\nKrishna has implemented several cost optimization strategi",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 37
    },
    "id": "ai_generative_ai_llm_259"
  },
  {
    "text": "ased load and traffic\n- **Security**: Implementing proper authentication and authorization\n\n## Cost Optimization\n\n### Strategies Krishna Used\nKrishna has implemented several cost optimization strategies for LLM applications:\n- **Token Management**: Optimizing prompt length and response size\n- **Caching**: Storing frequently used responses\n- **Model Selection**: Choosing cost-effective models for different tasks\n- **Batch Processing**: Processing multiple requests efficiently\n\n### Cost Reduction Techniques\n\nKrishna's cost optimization techniques include:\n- **Prompt Compression**: Reducing token usage without losing quality\n- **Response Caching**: Avoiding redundant API calls\n- **Smart Routing**: Using different models for different complexity levels\n- **Usage Monitoring**: Tracking and analyzing API usage patterns\n\n## Interview Talking Points\n\n### Key Achievements to Highlight\n\nWhen discussing AI/ML experience in interviews, Krishna focuses on:\n- **Production Impact**: Real business val",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 38
    },
    "id": "ai_generative_ai_llm_260"
  },
  {
    "text": "yzing API usage patterns\n\n## Interview Talking Points\n\n### Key Achievements to Highlight\n\nWhen discussing AI/ML experience in interviews, Krishna focuses on:\n- **Production Impact**: Real business value delivered through AI solutions\n- **Technical Depth**: Understanding of both theoretical concepts and practical implementation\n- **Problem-Solving**: Ability to identify and solve complex AI challenges\n- **Collaboration**: Working with cross-functional teams to deliver AI solutions\n\n### Technical Skills to Emphasize\n\nKrishna's technical expertise includes:\n- **Machine Learning**: End-to-end ML pipeline development\n- **Deep Learning**: Neural network architecture design and optimization\n- **Generative AI**: LLM integration and prompt engineering\n- **MLOps**: Production deployment and monitoring\n- **Cloud Platforms**: AWS, Azure, and Google Cloud AI services\n\n### Project Examples\n\nKrishna can discuss specific projects including:\n- **RAG System**: Built a production RAG system for knowledge",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 39
    },
    "id": "ai_generative_ai_llm_261"
  },
  {
    "text": "ng\n- **Cloud Platforms**: AWS, Azure, and Google Cloud AI services\n\n### Project Examples\n\nKrishna can discuss specific projects including:\n- **RAG System**: Built a production RAG system for knowledge management\n- **Computer Vision**: Developed image classification systems\n- **NLP Applications**: Created text processing and analysis tools\n- **Recommendation Systems**: Built personalized recommendation engines\n- **Chatbot Development**: Created conversational AI systems with memory\n\n### Challenges and Solutions\n\nKrishna can discuss challenges he's faced and how he solved them:\n- **Model Performance**: Optimizing models for production requirements\n- **Data Quality**: Handling noisy and incomplete datasets\n- **Scalability**: Building systems that can handle increased load\n- **Cost Management**: Optimizing AI system costs while maintaining quality\n- **Integration**: Connecting AI systems with existing infrastructure",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 40
    },
    "id": "ai_generative_ai_llm_262"
  },
  {
    "text": "*: Optimizing AI system costs while maintaining quality\n- **Integration**: Connecting AI systems with existing infrastructure",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/generative_ai_llm.md",
      "file_name": "generative_ai_llm.md",
      "chunk_index": 41
    },
    "id": "ai_generative_ai_llm_263"
  },
  {
    "text": "---\ntags: [machine-learning, ml, scikit-learn, tensorflow, keras, model-training, feature-engineering]\npersona: ml\n---\n\n# Machine Learning Projects & Krishna's Real-World Experience\n\n## Predictive Analytics at Walgreens\n\n### Customer Churn Prediction Model\n**Krishna's Implementation:**\nSo at Walgreens, I built this customer churn prediction model that actually helped reduce churn by about 15%. What I did was use historical transaction data - we're talking like 2 years of customer purchases, prescription refills, and loyalty program activity.\n\n**Feature Engineering:**\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef engineer_customer_features(df):\n    \"\"\"\n    Create features for churn prediction\n    This is what I used in production at Walgreens\n    \"\"\"\n    # Recency, Frequency, Monetary features\n    customer_features = df.groupby('customer_id').agg({\n        'order_date': lambda x",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 42
    },
    "id": "ai_machine_learning_projects_264"
  },
  {
    "text": "prediction\n    This is what I used in production at Walgreens\n    \"\"\"\n    # Recency, Frequency, Monetary features\n    customer_features = df.groupby('customer_id').agg({\n        'order_date': lambda x: (pd.Timestamp.now() - x.max()).days,  # Recency\n        'order_id': 'count',  # Frequency\n        'amount': ['sum', 'mean', 'std']  # Monetary\n    })\n    \n    customer_features.columns = ['recency', 'frequency', 'total_spent', 'avg_order', 'order_std']\n    \n    # Engagement features - these actually helped a lot\n    customer_features['days_since_first_order'] = df.groupby('customer_id')['order_date'].apply(\n        lambda x: (x.max() - x.min()).days\n    )\n    \n    # Purchase patterns\n    customer_features['purchase_frequency'] = customer_features['frequency'] / (\n        customer_features['days_since_first_order'] + 1\n    )\n    \n    # Trend features - is customer spending increasing or decreasing?\n    recent_30_days = df[df['order_date'] >= pd.Timestamp.now() - pd.Timedelta(days=30)]\n   ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 43
    },
    "id": "ai_machine_learning_projects_265"
  },
  {
    "text": "ays_since_first_order'] + 1\n    )\n    \n    # Trend features - is customer spending increasing or decreasing?\n    recent_30_days = df[df['order_date'] >= pd.Timestamp.now() - pd.Timedelta(days=30)]\n    customer_features['recent_orders'] = recent_30_days.groupby('customer_id').size()\n    customer_features['recent_orders'] = customer_features['recent_orders'].fillna(0)\n    \n    return customer_features\n\n# Load and prepare data\ntransactions = pd.read_csv('customer_transactions.csv', parse_dates=['order_date'])\nfeatures = engineer_customer_features(transactions)\n\n# Create target variable - churned if no purchase in 90 days\nfeatures['churned'] = (features['recency'] > 90).astype(int)\n\n# Handle missing values\nfeatures = features.fillna(0)\n\nprint(f\"Total customers: {len(features)}\")\nprint(f\"Churn rate: {features['churned'].mean():.2%}\")\n```\n\n**Model Training:**\n```python\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticReg",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 44
    },
    "id": "ai_machine_learning_projects_266"
  },
  {
    "text": "rate: {features['churned'].mean():.2%}\")\n```\n\n**Model Training:**\n```python\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Split data\nX = features.drop(['churned'], axis=1)\ny = features['churned']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Try multiple models - I found Random Forest worked best\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n}\n\nresults = {}\nfor name, mod",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 45
    },
    "id": "ai_machine_learning_projects_267"
  },
  {
    "text": "m Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n}\n\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    \n    results[name] = {\n        'model': model,\n        'accuracy': model.score(X_test_scaled, y_test),\n        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n    }\n    \n    print(f\"\\n{name} Results:\")\n    print(f\"Accuracy: {results[name]['accuracy']:.3f}\")\n    print(f\"ROC-AUC: {results[name]['roc_auc']:.3f}\")\n    print(classification_report(y_test, y_pred))\n\n# Random Forest won with 0.87 AUC\nbest_model = results['Random Forest']['model']\n```\n\n**Feature Importance Analysis:**\n```python\nimport matplotlib.pyplot as plt\n\n# What I learned was that recency and purchase frequency were the top predictors\nfeature_importance = pd.DataF",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 46
    },
    "id": "ai_machine_learning_projects_268"
  },
  {
    "text": "'model']\n```\n\n**Feature Importance Analysis:**\n```python\nimport matplotlib.pyplot as plt\n\n# What I learned was that recency and purchase frequency were the top predictors\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': best_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(feature_importance.head(10))\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\nplt.xlabel('Importance')\nplt.title('Top 10 Feature Importance')\nplt.tight_layout()\nplt.savefig('feature_importance.png')\n```\n\n## Demand Forecasting for Pharmacy Inventory\n\n### Time Series Forecasting\n**Krishna's Approach:**\nSo we had this problem where some pharmacies were running out of stock while others had too much inventory. I built a forecasting model using historical sales data and it actually reduced stockouts by 30%.\n\n```python\nimport panda",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 47
    },
    "id": "ai_machine_learning_projects_269"
  },
  {
    "text": "some pharmacies were running out of stock while others had too much inventory. I built a forecasting model using historical sales data and it actually reduced stockouts by 30%.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef forecast_product_demand(product_id, store_id, historical_data):\n    \"\"\"\n    Forecast demand for next 30 days\n    Used this in production for 200+ stores\n    \"\"\"\n    # Filter data for specific product and store\n    df = historical_data[\n        (historical_data['product_id'] == product_id) & \n        (historical_data['store_id'] == store_id)\n    ].copy()\n    \n    # Create daily time series\n    df = df.set_index('date').resample('D')['quantity_sold'].sum()\n    df = df.fillna(0)  # Days with no sales\n    \n    # Handle outliers - sometimes we had data entry errors\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    df",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 48
    },
    "id": "ai_machine_learning_projects_270"
  },
  {
    "text": "_sold'].sum()\n    df = df.fillna(0)  # Days with no sales\n    \n    # Handle outliers - sometimes we had data entry errors\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    df = df.clip(lower=Q1 - 1.5*IQR, upper=Q3 + 1.5*IQR)\n    \n    # Split train/test\n    train_size = int(len(df) * 0.8)\n    train, test = df[:train_size], df[train_size:]\n    \n    # SARIMA model - seasonal pattern was weekly\n    try:\n        model = SARIMAX(\n            train,\n            order=(1, 1, 1),  # ARIMA parameters\n            seasonal_order=(1, 1, 1, 7),  # Weekly seasonality\n            enforce_stationarity=False,\n            enforce_invertibility=False\n        )\n        \n        fitted_model = model.fit(disp=False)\n        \n        # Forecast\n        forecast = fitted_model.forecast(steps=len(test))\n        \n        # Calculate metrics\n        mae = mean_absolute_error(test, forecast)\n        rmse = np.sqrt(mean_squared_error(test, forecast))\n        \n        # Forecast next 30 day",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 49
    },
    "id": "ai_machine_learning_projects_271"
  },
  {
    "text": "eps=len(test))\n        \n        # Calculate metrics\n        mae = mean_absolute_error(test, forecast)\n        rmse = np.sqrt(mean_squared_error(test, forecast))\n        \n        # Forecast next 30 days\n        future_forecast = fitted_model.forecast(steps=30)\n        \n        return {\n            'forecast': future_forecast,\n            'mae': mae,\n            'rmse': rmse,\n            'model': fitted_model\n        }\n    \n    except Exception as e:\n        print(f\"Error forecasting for product {product_id}, store {store_id}: {e}\")\n        # Fallback to simple moving average\n        return {\n            'forecast': train.rolling(window=7).mean().iloc[-1],\n            'mae': None,\n            'rmse': None,\n            'model': None\n        }\n\n# Run forecasting for all products\nproducts = historical_data[['product_id', 'store_id']].drop_duplicates()\n\nforecasts = []\nfor _, row in products.iterrows():\n    result = forecast_product_demand(row['product_id'], row['store_id'], historical_data)\n",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 50
    },
    "id": "ai_machine_learning_projects_272"
  },
  {
    "text": "torical_data[['product_id', 'store_id']].drop_duplicates()\n\nforecasts = []\nfor _, row in products.iterrows():\n    result = forecast_product_demand(row['product_id'], row['store_id'], historical_data)\n    forecasts.append({\n        'product_id': row['product_id'],\n        'store_id': row['store_id'],\n        'forecast': result['forecast'],\n        'mae': result['mae']\n    })\n\nforecasts_df = pd.DataFrame(forecasts)\nprint(f\"Generated forecasts for {len(forecasts_df)} product-store combinations\")\n```\n\n## Recommendation System\n\n### Collaborative Filtering for Product Recommendations\n**Krishna's Implementation:**\nI built a recommendation system that suggested products to customers based on their purchase history and similar customers. It increased cross-sell by about 12%.\n\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse import csr_matrix\n\ndef build_recommendation_system(transactions):\n    \"\"\"\n    Build collaborative filtering recommendation system\n    This ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 51
    },
    "id": "ai_machine_learning_projects_273"
  },
  {
    "text": ".metrics.pairwise import cosine_similarity\nfrom scipy.sparse import csr_matrix\n\ndef build_recommendation_system(transactions):\n    \"\"\"\n    Build collaborative filtering recommendation system\n    This ran daily in production\n    \"\"\"\n    # Create user-item matrix\n    user_item_matrix = transactions.pivot_table(\n        index='customer_id',\n        columns='product_id',\n        values='quantity',\n        aggfunc='sum',\n        fill_value=0\n    )\n    \n    # Convert to sparse matrix for memory efficiency\n    sparse_matrix = csr_matrix(user_item_matrix.values)\n    \n    # Calculate item-item similarity\n    item_similarity = cosine_similarity(sparse_matrix.T)\n    item_similarity_df = pd.DataFrame(\n        item_similarity,\n        index=user_item_matrix.columns,\n        columns=user_item_matrix.columns\n    )\n    \n    return user_item_matrix, item_similarity_df\n\ndef get_recommendations(customer_id, user_item_matrix, item_similarity_df, n_recommendations=5):\n    \"\"\"\n    Get top N product recommen",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 52
    },
    "id": "ai_machine_learning_projects_274"
  },
  {
    "text": "umns\n    )\n    \n    return user_item_matrix, item_similarity_df\n\ndef get_recommendations(customer_id, user_item_matrix, item_similarity_df, n_recommendations=5):\n    \"\"\"\n    Get top N product recommendations for a customer\n    \"\"\"\n    # Get products customer has already purchased\n    customer_purchases = user_item_matrix.loc[customer_id]\n    purchased_products = customer_purchases[customer_purchases > 0].index.tolist()\n    \n    # Calculate recommendation scores\n    recommendation_scores = {}\n    for product in purchased_products:\n        similar_products = item_similarity_df[product].sort_values(ascending=False)[1:11]\n        for similar_product, similarity in similar_products.items():\n            if similar_product not in purchased_products:\n                if similar_product not in recommendation_scores:\n                    recommendation_scores[similar_product] = 0\n                recommendation_scores[similar_product] += similarity\n    \n    # Sort and return top N\n    recommendatio",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 53
    },
    "id": "ai_machine_learning_projects_275"
  },
  {
    "text": "mendation_scores:\n                    recommendation_scores[similar_product] = 0\n                recommendation_scores[similar_product] += similarity\n    \n    # Sort and return top N\n    recommendations = sorted(\n        recommendation_scores.items(),\n        key=lambda x: x[1],\n        reverse=True\n    )[:n_recommendations]\n    \n    return [product for product, score in recommendations]\n\n# Build system\nuser_item_matrix, item_similarity_df = build_recommendation_system(transactions)\n\n# Get recommendations for a customer\ncustomer_id = 12345\nrecommendations = get_recommendations(customer_id, user_item_matrix, item_similarity_df)\nprint(f\"Recommendations for customer {customer_id}: {recommendations}\")\n```\n\n## Model Deployment and Monitoring\n\n### Production ML Pipeline\n**Krishna's MLOps Setup:**\nSo one thing I learned was that training the model is just 20% of the work. The real challenge was deploying it and monitoring it in production.\n\n```python\nimport joblib\nimport json\nfrom datetime im",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 54
    },
    "id": "ai_machine_learning_projects_276"
  },
  {
    "text": "*\nSo one thing I learned was that training the model is just 20% of the work. The real challenge was deploying it and monitoring it in production.\n\n```python\nimport joblib\nimport json\nfrom datetime import datetime\nimport logging\n\nclass ModelMonitor:\n    \"\"\"\n    Monitor model performance in production\n    I set this up to track model drift\n    \"\"\"\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.predictions_log = []\n        self.performance_log = []\n        \n        logging.basicConfig(\n            filename=f'{model_name}_monitor.log',\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n    \n    def log_prediction(self, features, prediction, actual=None):\n        \"\"\"Log each prediction for monitoring\"\"\"\n        log_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'features': features.tolist() if hasattr(features, 'tolist') else features,\n            'prediction': float(pre",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 55
    },
    "id": "ai_machine_learning_projects_277"
  },
  {
    "text": "\"\n        log_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'features': features.tolist() if hasattr(features, 'tolist') else features,\n            'prediction': float(prediction),\n            'actual': float(actual) if actual is not None else None\n        }\n        \n        self.predictions_log.append(log_entry)\n        logging.info(f\"Prediction: {json.dumps(log_entry)}\")\n        \n        # Check for data drift\n        if len(self.predictions_log) % 1000 == 0:\n            self.check_data_drift()\n    \n    def check_data_drift(self):\n        \"\"\"Check if feature distributions are changing\"\"\"\n        recent_predictions = self.predictions_log[-1000:]\n        \n        # Simple check - you'd want something more sophisticated in production\n        recent_features = [p['features'] for p in recent_predictions]\n        feature_means = np.mean(recent_features, axis=0)\n        \n        logging.info(f\"Recent feature means: {feature_means}\")\n        \n        # Alert if s",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 56
    },
    "id": "ai_machine_learning_projects_278"
  },
  {
    "text": "['features'] for p in recent_predictions]\n        feature_means = np.mean(recent_features, axis=0)\n        \n        logging.info(f\"Recent feature means: {feature_means}\")\n        \n        # Alert if significant drift detected\n        # In production, I used more sophisticated drift detection\n        \n    def calculate_performance(self):\n        \"\"\"Calculate model performance on recent predictions\"\"\"\n        recent_with_actuals = [\n            p for p in self.predictions_log[-1000:]\n            if p['actual'] is not None\n        ]\n        \n        if len(recent_with_actuals) > 0:\n            predictions = [p['prediction'] for p in recent_with_actuals]\n            actuals = [p['actual'] for p in recent_with_actuals]\n            \n            mae = mean_absolute_error(actuals, predictions)\n            rmse = np.sqrt(mean_squared_error(actuals, predictions))\n            \n            logging.info(f\"Recent performance - MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n            \n            return {'mae'",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 57
    },
    "id": "ai_machine_learning_projects_279"
  },
  {
    "text": "         rmse = np.sqrt(mean_squared_error(actuals, predictions))\n            \n            logging.info(f\"Recent performance - MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n            \n            return {'mae': mae, 'rmse': rmse}\n        \n        return None\n\n# Save model for production\ndef save_model_for_production(model, scaler, feature_names, model_name):\n    \"\"\"\n    Save model with all necessary artifacts\n    \"\"\"\n    model_artifacts = {\n        'model': model,\n        'scaler': scaler,\n        'feature_names': feature_names,\n        'model_version': '1.0',\n        'training_date': datetime.now().isoformat(),\n        'model_type': type(model).__name__\n    }\n    \n    joblib.dump(model_artifacts, f'{model_name}_v1.0.pkl')\n    print(f\"Model saved: {model_name}_v1.0.pkl\")\n\n# Load model in production\ndef load_model_for_inference(model_path):\n    \"\"\"Load model and all artifacts\"\"\"\n    artifacts = joblib.load(model_path)\n    return artifacts\n\n# Example usage\nmonitor = ModelMonitor('churn_prediction",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 58
    },
    "id": "ai_machine_learning_projects_280"
  },
  {
    "text": "\ndef load_model_for_inference(model_path):\n    \"\"\"Load model and all artifacts\"\"\"\n    artifacts = joblib.load(model_path)\n    return artifacts\n\n# Example usage\nmonitor = ModelMonitor('churn_prediction')\n\n# In production API\ndef predict_churn(customer_features):\n    \"\"\"Production prediction endpoint\"\"\"\n    # Load model\n    artifacts = load_model_for_inference('churn_prediction_v1.0.pkl')\n    model = artifacts['model']\n    scaler = artifacts['scaler']\n    \n    # Prepare features\n    features_scaled = scaler.transform([customer_features])\n    \n    # Predict\n    prediction = model.predict_proba(features_scaled)[0][1]\n    \n    # Log prediction\n    monitor.log_prediction(customer_features, prediction)\n    \n    return {\n        'customer_id': customer_features[0],\n        'churn_probability': float(prediction),\n        'risk_level': 'high' if prediction > 0.7 else 'medium' if prediction > 0.4 else 'low'\n    }\n```\n\n## Real-World Challenges and Solutions\n\n### Handling Imbalanced Data\n**Krishna'",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 59
    },
    "id": "ai_machine_learning_projects_281"
  },
  {
    "text": "at(prediction),\n        'risk_level': 'high' if prediction > 0.7 else 'medium' if prediction > 0.4 else 'low'\n    }\n```\n\n## Real-World Challenges and Solutions\n\n### Handling Imbalanced Data\n**Krishna's Experience:**\nSo one big challenge I faced was imbalanced data. Like in churn prediction, only 5% of customers actually churned. Here's what worked for me:\n\n```python\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# What I found worked best was a combination of SMOTE and undersampling\ndef handle_imbalanced_data(X_train, y_train):\n    \"\"\"\n    Handle imbalanced dataset\n    I tried multiple approaches and this worked best\n    \"\"\"\n    # Check class distribution\n    print(f\"Original class distribution: {np.bincount(y_train)}\")\n    \n    # SMOTE + Random Undersampling\n    over = SMOTE(sampling_strategy=0.5, random_state=42)  # Oversample minority to 50%\n    under = RandomUnderSampler(sampling",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 60
    },
    "id": "ai_machine_learning_projects_282"
  },
  {
    "text": "ibution: {np.bincount(y_train)}\")\n    \n    # SMOTE + Random Undersampling\n    over = SMOTE(sampling_strategy=0.5, random_state=42)  # Oversample minority to 50%\n    under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)  # Undersample majority\n    \n    steps = [('over', over), ('under', under)]\n    pipeline = ImbPipeline(steps=steps)\n    \n    X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)\n    \n    print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n    \n    return X_resampled, y_resampled\n\n# Use class weights as alternative\nmodel = RandomForestClassifier(\n    n_estimators=100,\n    class_weight='balanced',  # This helped a lot\n    random_state=42\n)\n```\n\n### Feature Selection\n**Krishna's Approach:**\n```python\nfrom sklearn.feature_selection import SelectKBest, f_classif, RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef select_best_features(X, y, n_features=20):\n    \"\"\"\n    Select most important features\n    This reduced training t",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 61
    },
    "id": "ai_machine_learning_projects_283"
  },
  {
    "text": "t SelectKBest, f_classif, RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef select_best_features(X, y, n_features=20):\n    \"\"\"\n    Select most important features\n    This reduced training time by 40% without hurting accuracy\n    \"\"\"\n    # Method 1: Statistical test\n    selector_stats = SelectKBest(score_func=f_classif, k=n_features)\n    selector_stats.fit(X, y)\n    \n    # Method 2: Recursive Feature Elimination\n    estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n    selector_rfe = RFE(estimator, n_features_to_select=n_features, step=1)\n    selector_rfe.fit(X, y)\n    \n    # Get feature rankings\n    feature_scores = pd.DataFrame({\n        'feature': X.columns,\n        'stats_score': selector_stats.scores_,\n        'rfe_ranking': selector_rfe.ranking_\n    })\n    \n    # Select features that rank high in both methods\n    top_features = feature_scores.nsmallest(n_features, 'rfe_ranking')['feature'].tolist()\n    \n    return top_features, feature_scores\n\n# Apply",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 62
    },
    "id": "ai_machine_learning_projects_284"
  },
  {
    "text": "  \n    # Select features that rank high in both methods\n    top_features = feature_scores.nsmallest(n_features, 'rfe_ranking')['feature'].tolist()\n    \n    return top_features, feature_scores\n\n# Apply feature selection\ntop_features, feature_scores = select_best_features(X_train, y_train, n_features=20)\nX_train_selected = X_train[top_features]\nX_test_selected = X_test[top_features]\n\nprint(f\"Reduced features from {X_train.shape[1]} to {len(top_features)}\")\n```\n\n## Interview Talking Points\n\n### Technical Achievements:\n- Built churn prediction model reducing customer churn by 15%\n- Developed demand forecasting system reducing stockouts by 30%\n- Implemented recommendation system increasing cross-sell by 12%\n- Achieved 0.87 ROC-AUC score on production churn model\n- Reduced model training time by 40% through feature selection\n\n### Technologies Used:\n- **ML Libraries**: Scikit-learn, XGBoost, LightGBM, Statsmodels\n- **Data Processing**: Pandas, NumPy, SciPy\n- **Model Deployment**: Joblib, Flas",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 63
    },
    "id": "ai_machine_learning_projects_285"
  },
  {
    "text": " 40% through feature selection\n\n### Technologies Used:\n- **ML Libraries**: Scikit-learn, XGBoost, LightGBM, Statsmodels\n- **Data Processing**: Pandas, NumPy, SciPy\n- **Model Deployment**: Joblib, Flask, Docker\n- **Monitoring**: Custom logging, Prometheus, Grafana\n- **Cloud**: AWS SageMaker, Azure ML\n\n### Problem-Solving Examples:\n- Handled imbalanced data using SMOTE and class weights\n- Dealt with data drift by implementing monitoring and retraining pipelines\n- Optimized model performance through hyperparameter tuning\n- Reduced inference latency from 500ms to 50ms through optimization\n",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/machine_learning_projects.md",
      "file_name": "machine_learning_projects.md",
      "chunk_index": 64
    },
    "id": "ai_machine_learning_projects_286"
  },
  {
    "text": "---\ntags: [krishna, experience, resume, ai-ml-engineer, genai-engineer, walgreens, cvs, mckesson, indietek]\npersona: krishna\n---\n\n# Krishna's AI/ML/GenAI Professional Experience & Background\n\n## Professional Summary\n\nI'm Krishna, an AI/ML Engineer with 5+ years of experience building end-to-end machine learning and generative AI solutions. I'm skilled in developing data pipelines, training and deploying ML models, and implementing RAG/LLM systems with embeddings and vector databases. I have a proven ability to deliver cost-efficient, production-ready AI platforms that scale across domains including healthcare, retail, and finance.\n\n## Professional Experience\n\n### AI/ML Engineer — TCS (Walgreens, USA)\n**Duration:** Feb 2022 – Present\n\n**Key Responsibilities and Achievements:**\n\nSo at Walgreens through TCS, I designed and deployed this RAG pipeline with Databricks, Pinecone, and OpenAI. The goal was to enable pharmacists to query compliance documents in natural language with source-cited",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 65
    },
    "id": "ai_krishna_ai_experience_287"
  },
  {
    "text": "ens through TCS, I designed and deployed this RAG pipeline with Databricks, Pinecone, and OpenAI. The goal was to enable pharmacists to query compliance documents in natural language with source-cited answers. This was a game-changer because pharmacists could now get instant answers instead of spending hours searching through documents.\n\nI engineered ingestion of structured and unstructured data (PDFs, transcripts) via PySpark pipelines, transforming data into embeddings and cutting manual search effort by 60%. The pharmacists were really happy with this improvement.\n\nI automated retraining and re-indexing workflows using Airflow, which improved model freshness and reduced knowledge gaps across pharmacy operations. This was important because compliance documents change frequently, and we needed to keep the system up-to-date.\n\nI deployed scalable inference APIs on AKS with CI/CD in Azure DevOps, ensuring zero-downtime rollouts and 35% latency reduction with optimized caching and prompts",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 66
    },
    "id": "ai_krishna_ai_experience_288"
  },
  {
    "text": "eeded to keep the system up-to-date.\n\nI deployed scalable inference APIs on AKS with CI/CD in Azure DevOps, ensuring zero-downtime rollouts and 35% latency reduction with optimized caching and prompts. The performance improvement was significant, and the system was much more reliable.\n\nI implemented governance with Unity Catalog and PII scrubbing, securing sensitive patient data while maintaining HIPAA compliance in embeddings and responses. This was crucial because we were dealing with healthcare data and needed to ensure patient privacy.\n\n**Technologies Used:**\n- Databricks, PySpark, Pinecone, OpenAI API\n- LangChain, RAG pipelines, Vector DBs\n- Azure ML, AKS, Azure DevOps\n- Unity Catalog, PII masking, HIPAA compliance\n\n### ML Engineer — CVS Health (USA)\n**Duration:** Jan 2021 – Jan 2022\n\n**Key Responsibilities and Achievements:**\n\nAt CVS Health, I built demand forecasting models in PySpark and TensorFlow that predicted sales and supply trends. This was really exciting because I got t",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 67
    },
    "id": "ai_krishna_ai_experience_289"
  },
  {
    "text": "\n**Key Responsibilities and Achievements:**\n\nAt CVS Health, I built demand forecasting models in PySpark and TensorFlow that predicted sales and supply trends. This was really exciting because I got to work with both data engineering and machine learning. I improved procurement accuracy and saved $15M annually, which was a huge win for the business.\n\nI engineered feature pipelines with dbt and Databricks for model-ready datasets, cutting preparation time by 40% and ensuring consistency across teams. This was important because data scientists were spending too much time on data preparation instead of building models.\n\nI tracked experiments with MLflow and automated retraining pipelines, which boosted model performance by 18% over baseline. The automated retraining was crucial because it kept the models fresh and accurate.\n\nI deployed models to production via Azure ML, implementing monitoring for drift and automating retraining triggers. This was my first experience with MLOps, and I lea",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 68
    },
    "id": "ai_krishna_ai_experience_290"
  },
  {
    "text": "t the models fresh and accurate.\n\nI deployed models to production via Azure ML, implementing monitoring for drift and automating retraining triggers. This was my first experience with MLOps, and I learned a lot about production ML systems.\n\n**Technologies Used:**\n- PySpark, TensorFlow, MLflow\n- dbt, Databricks, Azure ML\n- Python, Pandas, Scikit-learn\n- SQL Server, Power BI\n\n**Challenges Overcome:**\n- Building scalable forecasting models for retail operations\n- Implementing MLOps practices for model lifecycle management\n- Ensuring model reliability in production environments\n- Integrating ML models with business processes\n\n### Data Science Intern — McKesson (USA)\n**Duration:** May 2020 – Dec 2020\n\n**Key Responsibilities and Achievements:**\n\nAt McKesson, I developed ETL and ML scripts in Python that reduced ingestion latency by 50%. This was my first real experience with ETL and ML, and it was really exciting to see the impact. The scripts powered dashboards for executive decisions on fi",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 69
    },
    "id": "ai_krishna_ai_experience_291"
  },
  {
    "text": "hon that reduced ingestion latency by 50%. This was my first real experience with ETL and ML, and it was really exciting to see the impact. The scripts powered dashboards for executive decisions on financial integrity, which was really important for the business.\n\nI built regression and time series models that forecasted patient demand, preventing supply mismatches and reducing stockouts by 22%. This was really satisfying because it directly improved patient care by ensuring medications were available when needed.\n\nI produced compliance-focused ML insights that aligned with audit requirements and informed leadership's strategic reviews. This was important because healthcare companies need to maintain strict compliance standards.\n\n**Technologies Used:**\n- Python, SQL, Pandas, NumPy\n- Scikit-learn, TensorFlow\n- SQL Server, Oracle\n- Power BI, Tableau\n\n**Key Learnings:**\n- Building ML models for healthcare compliance\n- Time series forecasting for supply chain optimization\n- Integrating ML ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 70
    },
    "id": "ai_krishna_ai_experience_292"
  },
  {
    "text": "-learn, TensorFlow\n- SQL Server, Oracle\n- Power BI, Tableau\n\n**Key Learnings:**\n- Building ML models for healthcare compliance\n- Time series forecasting for supply chain optimization\n- Integrating ML insights with business processes\n- Ensuring model reliability in regulated environments\n\n### Software Developer — Inditek Pioneer Solutions (India)\n**Duration:** 2017 – 2019\n\n**Key Responsibilities and Achievements:**\n\nAt Indietek, I built backend APIs and optimized SQL queries for ERP modules, which strengthened transactional accuracy and improved response times by 35%. This was where I started my career, and it was a great foundation for learning about system performance and optimization.\n\nI designed reporting modules that surfaced anomalies in contracts and payments, reducing manual reconciliation workload. This was really satisfying because it helped the business team work more efficiently.\n\n**Technologies Used:**\n- SQL Server, Oracle\n- C#, .NET Framework\n- JavaScript, HTML, CSS\n- ERP ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 71
    },
    "id": "ai_krishna_ai_experience_293"
  },
  {
    "text": "liation workload. This was really satisfying because it helped the business team work more efficiently.\n\n**Technologies Used:**\n- SQL Server, Oracle\n- C#, .NET Framework\n- JavaScript, HTML, CSS\n- ERP systems, API development\n\n**Skills Developed:**\n- Backend API development and optimization\n- SQL performance tuning and query optimization\n- ERP system integration and customization\n- Business process automation\n\n## Skills\n\n- **AI/ML & GenAI:** PyTorch, TensorFlow, Hugging Face, OpenAI API, LangChain, MLflow, RAG pipelines, Vector DBs (Pinecone, FAISS, Weaviate)\n- **Data Engineering & ELT:** PySpark, Databricks, Airflow, dbt\n- **Cloud Platforms:** Azure (ML, Data Factory, AKS, DevOps), AWS (SageMaker, Lambda, S3)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle, Vector DBs\n- **Analytics & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Governance:** Azure DevOps, GitHub Actions, Docker, Kubernetes, Unity Catalog, PII Masking, Model Monitoring\n\n## Key Achievements ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 72
    },
    "id": "ai_krishna_ai_experience_294"
  },
  {
    "text": "\n- **Analytics & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Governance:** Azure DevOps, GitHub Actions, Docker, Kubernetes, Unity Catalog, PII Masking, Model Monitoring\n\n## Key Achievements & Metrics\n\n### AI/ML Impact\n- Built RAG pipeline reducing manual search by 60%\n- Developed forecasting models saving $15M+ annually\n- Improved model performance by 18% over baseline\n- Reduced stockouts by 22% through demand forecasting\n- Achieved 35% latency reduction in inference APIs\n\n### Scale & Performance\n- Processed 10TB+ monthly data for ML pipelines\n- Built models serving 1000+ concurrent users\n- Implemented automated retraining for 20+ models\n- Achieved 99.9% uptime for production ML services\n\n### Business Impact\n- Enabled natural language querying of compliance documents\n- Improved procurement accuracy through demand forecasting\n- Reduced manual reconciliation by 40%\n- Uncovered $20M+ in procurement savings through ML insights\n\n### Technical Leadership\n- Led end-to-end RAG system",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 73
    },
    "id": "ai_krishna_ai_experience_295"
  },
  {
    "text": "rocurement accuracy through demand forecasting\n- Reduced manual reconciliation by 40%\n- Uncovered $20M+ in procurement savings through ML insights\n\n### Technical Leadership\n- Led end-to-end RAG system implementation\n- Mentored teams on MLOps best practices\n- Established model governance and monitoring frameworks\n- Implemented HIPAA-compliant AI systems\n\n## Interview Talking Points\n\n### Problem-Solving Examples\n\n**Challenge: Building Production RAG System**\nAt Walgreens, I had to build a RAG pipeline for pharmacists to query compliance documents, but getting it production-ready was really challenging. We had to handle HIPAA compliance, ensure low latency, and maintain high availability. What I did was implement PII scrubbing in the embedding pipeline, deploy scalable inference APIs on AKS with CI/CD, and optimize caching and prompts. The result was a 35% latency reduction with zero-downtime deployments and full HIPAA compliance. The system now serves 1000+ users daily.\n\n**Challenge: Lar",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 74
    },
    "id": "ai_krishna_ai_experience_296"
  },
  {
    "text": "ith CI/CD, and optimize caching and prompts. The result was a 35% latency reduction with zero-downtime deployments and full HIPAA compliance. The system now serves 1000+ users daily.\n\n**Challenge: Large-Scale Forecasting Model Deployment**\nAt CVS, I built demand forecasting models that were performing well in development, but deploying them to production at scale was tricky. We had to handle real-time inference, manage model drift, and ensure reliability. What I did was implement MLflow for experiment tracking, build automated retraining pipelines, and deploy models via Azure ML with comprehensive monitoring. The result was models that improved performance by 18% over baseline and saved the company over $15 million annually.\n\n**Challenge: ML Pipeline Optimization**\nAt McKesson, we had ML pipelines that were taking too long to run and costing too much. The data preparation was manual and error-prone. What I did was build automated ETL scripts using Python that reduced ingestion latency ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 75
    },
    "id": "ai_krishna_ai_experience_297"
  },
  {
    "text": "ipelines that were taking too long to run and costing too much. The data preparation was manual and error-prone. What I did was build automated ETL scripts using Python that reduced ingestion latency by 50% and implemented proper data validation. I also built feature pipelines that standardized data preparation across teams. The result was much more reliable and efficient ML operations.\n\n### Technical Strengths\n\n**RAG & GenAI Systems**\nI'm really strong in building RAG systems and working with LLMs. I've built production RAG pipelines using Pinecone, OpenAI API, and LangChain. I know how to handle embeddings, implement semantic search, and optimize prompts for better responses. I understand vector databases and how to scale them for production use.\n\n**MLOps & Production Deployment**\nI'm experienced with the full ML lifecycle from development to production. I know how to use MLflow for experiment tracking, implement automated retraining, and deploy models at scale. I understand model mo",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 76
    },
    "id": "ai_krishna_ai_experience_298"
  },
  {
    "text": "perienced with the full ML lifecycle from development to production. I know how to use MLflow for experiment tracking, implement automated retraining, and deploy models at scale. I understand model monitoring, drift detection, and how to ensure model reliability in production.\n\n**Machine Learning & Deep Learning**\nI've built models across different domains including forecasting, NLP, and computer vision. I'm experienced with TensorFlow, PyTorch, and Scikit-learn. I know how to optimize models for performance and handle challenges like overfitting, data imbalance, and model interpretability.\n\n**Cloud ML Services**\nI'm experienced with cloud ML platforms including Azure ML, AWS SageMaker, and Google Vertex AI. I know how to leverage managed services for model training and deployment while maintaining control over the ML pipeline.\n\n## Work Style & Approach\n\n### My Philosophy\nI believe in building AI systems that are not just technically sound but also reliable and maintainable. When I bui",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 77
    },
    "id": "ai_krishna_ai_experience_299"
  },
  {
    "text": "intaining control over the ML pipeline.\n\n## Work Style & Approach\n\n### My Philosophy\nI believe in building AI systems that are not just technically sound but also reliable and maintainable. When I build an ML model or RAG system, I think about the entire lifecycle - from data preparation to production deployment and monitoring. I don't just deliver a model - I make sure it's production-ready and can scale with business needs.\n\n### Collaboration\nI work well in cross-functional teams. I've collaborated with data scientists, data engineers, product managers, and business stakeholders. I believe in clear communication, documentation, and knowledge sharing. I've mentored teams on ML best practices and helped them understand complex AI concepts.\n\n### Continuous Learning\nI'm always learning new AI/ML technologies and techniques. I recently got deep into RAG systems, vector databases, and advanced prompt engineering. I'm exploring newer LLM architectures and MLOps practices. I believe in stayi",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 78
    },
    "id": "ai_krishna_ai_experience_300"
  },
  {
    "text": "I/ML technologies and techniques. I recently got deep into RAG systems, vector databases, and advanced prompt engineering. I'm exploring newer LLM architectures and MLOps practices. I believe in staying current with AI research and applying new techniques to solve business problems.\n\n### Problem-Solving Approach\nWhen I face a complex AI/ML problem, I start by understanding the business context and data constraints. I prototype quickly, validate assumptions, and build iteratively. I believe in thorough testing, monitoring, and continuous optimization. I also believe in documenting everything so the team can understand and maintain the AI system.\n",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/krishna_ai_experience.md",
      "file_name": "krishna_ai_experience.md",
      "chunk_index": 79
    },
    "id": "ai_krishna_ai_experience_301"
  },
  {
    "text": "---\ntags: [rag, retrieval, embeddings, vector-db]\n---\n\n# RAG Systems Architecture\n\n## Overview\nRetrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to provide more accurate and up-to-date responses.\n\n## Key Components\n\n### 1. Document Ingestion Pipeline\n- **Text Chunking**: Split documents into overlapping chunks (typically 512-1024 tokens)\n- **Embedding Generation**: Use models like `text-embedding-3-small` or `text-embedding-ada-002`\n- **Vector Storage**: Store embeddings in vector databases (Pinecone, Weaviate, FAISS)\n\n### 2. Retrieval Strategy\n- **Similarity Search**: Use cosine similarity or dot product for semantic matching\n- **Hybrid Search**: Combine semantic search with keyword-based BM25\n- **Re-ranking**: Apply cross-encoder models to improve relevance\n\n### 3. Generation Process\n- **Context Injection**: Include retrieved chunks in the prompt\n- **Prompt Engineering**: Structure prompts to leverage retrieved informat",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/rag_systems.md",
      "file_name": "rag_systems.md",
      "chunk_index": 80
    },
    "id": "ai_rag_systems_302"
  },
  {
    "text": "ncoder models to improve relevance\n\n### 3. Generation Process\n- **Context Injection**: Include retrieved chunks in the prompt\n- **Prompt Engineering**: Structure prompts to leverage retrieved information\n- **Response Synthesis**: Generate answers grounded in retrieved context\n\n## Implementation Best Practices\n\n### Chunking Strategies\n- **Fixed-size chunks**: Simple but may split important information\n- **Semantic chunking**: Use sentence transformers to find natural boundaries\n- **Hierarchical chunking**: Store both fine and coarse-grained representations\n\n### Embedding Models\n- **General purpose**: OpenAI embeddings work well for most use cases\n- **Domain-specific**: Fine-tune embeddings on your specific domain\n- **Multilingual**: Consider models like `paraphrase-multilingual-MiniLM-L12-v2`\n\n### Vector Database Selection\n- **Pinecone**: Managed service, good for production\n- **Weaviate**: Open source, supports hybrid search\n- **FAISS**: Facebook's library, good for research and protot",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/rag_systems.md",
      "file_name": "rag_systems.md",
      "chunk_index": 81
    },
    "id": "ai_rag_systems_303"
  },
  {
    "text": "2`\n\n### Vector Database Selection\n- **Pinecone**: Managed service, good for production\n- **Weaviate**: Open source, supports hybrid search\n- **FAISS**: Facebook's library, good for research and prototyping\n- **Chroma**: Lightweight, easy to get started\n\n## Performance Optimization\n\n### Query Processing\n- **Query expansion**: Generate multiple query variations\n- **Query routing**: Direct queries to specialized indexes\n- **Caching**: Cache frequent queries and their results\n\n### Retrieval Tuning\n- **Top-k selection**: Experiment with different numbers of retrieved chunks\n- **Score thresholds**: Filter out low-relevance results\n- **Diversity sampling**: Ensure retrieved chunks cover different aspects\n\n## Common Challenges\n\n### Information Retrieval Issues\n- **Semantic mismatch**: Query and documents use different terminology\n- **Context loss**: Important information split across chunks\n- **Hallucination**: Model generates information not in retrieved context\n\n### Solutions\n- **Query prepr",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/rag_systems.md",
      "file_name": "rag_systems.md",
      "chunk_index": 82
    },
    "id": "ai_rag_systems_304"
  },
  {
    "text": "cuments use different terminology\n- **Context loss**: Important information split across chunks\n- **Hallucination**: Model generates information not in retrieved context\n\n### Solutions\n- **Query preprocessing**: Expand queries with synonyms and related terms\n- **Overlapping chunks**: Ensure important information appears in multiple chunks\n- **Grounding verification**: Check if generated content is supported by retrieved chunks\n\n## Evaluation Metrics\n\n### Retrieval Quality\n- **Precision@K**: Fraction of retrieved items that are relevant\n- **Recall@K**: Fraction of relevant items that are retrieved\n- **MRR**: Mean reciprocal rank of first relevant item\n\n### Generation Quality\n- **Faithfulness**: Generated content is supported by retrieved context\n- **Relevance**: Generated content answers the user's question\n- **Completeness**: Generated content covers all aspects of the question\n\n## Production Considerations\n\n### Scalability\n- **Batch processing**: Process large document collections eff",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/rag_systems.md",
      "file_name": "rag_systems.md",
      "chunk_index": 83
    },
    "id": "ai_rag_systems_305"
  },
  {
    "text": "e user's question\n- **Completeness**: Generated content covers all aspects of the question\n\n## Production Considerations\n\n### Scalability\n- **Batch processing**: Process large document collections efficiently\n- **Incremental updates**: Add new documents without rebuilding entire index\n- **Load balancing**: Distribute queries across multiple instances\n\n### Monitoring\n- **Query latency**: Track response times for different query types\n- **Retrieval quality**: Monitor precision and recall metrics\n- **Generation quality**: Track user satisfaction and feedback\n\n### Security\n- **Access control**: Restrict access to sensitive documents\n- **Query sanitization**: Prevent injection attacks through user queries\n- **Audit logging**: Track all queries and responses for compliance\n",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/rag_systems.md",
      "file_name": "rag_systems.md",
      "chunk_index": 84
    },
    "id": "ai_rag_systems_306"
  },
  {
    "text": "# 📄 OG Resume – Krishna Sathvik (AI/ML & GenAI Version)\n\n---\n\n## Project Intro (Real-World AI/ML/GenAI Example)\n\nIn my recent AI/ML project at Walgreens, I worked as an AI/ML Engineer to design and deploy a **GenAI-powered knowledge assistant** for pharmacy operations. The solution integrated a **Retrieval-Augmented Generation (RAG) pipeline** where customer FAQs and policy documents were ingested, chunked, and stored in a **vector database (Pinecone)**. We built embeddings using **OpenAI + Hugging Face models**, enabling pharmacists to query compliance or medication rules in natural language with citations. The ingestion pipelines were built in **Databricks + PySpark**, transforming structured and unstructured data (text, PDFs, call transcripts) into embedding-friendly formats.\n\nFor orchestration, we leveraged **Airflow** to automate nightly ingestion, retraining, and re-indexing jobs. The inference pipeline was deployed via **Azure Kubernetes Service (AKS)** with scalable APIs. We us",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 85
    },
    "id": "ai_ai_ml_gen_ai_307"
  },
  {
    "text": "stration, we leveraged **Airflow** to automate nightly ingestion, retraining, and re-indexing jobs. The inference pipeline was deployed via **Azure Kubernetes Service (AKS)** with scalable APIs. We used **MLflow** for experiment tracking and **Azure DevOps CI/CD** to deploy models into production with rollback safety. Governance was enforced with **Unity Catalog + custom PII scrubbing** before embedding sensitive data. Performance tuning included prompt optimization, few-shot examples, and caching embeddings for high-frequency queries, which reduced inference latency by 40%.\n\nThis project gave me **end-to-end GenAI + ML Ops experience**: ingestion, embeddings, vector search, model deployment, monitoring, and governance. It also showcased the ability to bridge **data engineering foundations with AI/ML innovation**, aligning with enterprise needs for compliance, scalability, and cost efficiency.\n\n---\n\n### 📄 OG Resume Summary – AI/ML & GenAI Version\n\nAI/ML Engineer with **5+ years of expe",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 86
    },
    "id": "ai_ai_ml_gen_ai_308"
  },
  {
    "text": " with AI/ML innovation**, aligning with enterprise needs for compliance, scalability, and cost efficiency.\n\n---\n\n### 📄 OG Resume Summary – AI/ML & GenAI Version\n\nAI/ML Engineer with **5+ years of experience** building end-to-end machine learning and generative AI solutions. Skilled in developing data pipelines, training and deploying ML models, and implementing RAG/LLM systems with embeddings and vector databases. Proven ability to deliver cost-efficient, production-ready AI platforms that scale across domains including healthcare, retail, and finance.\n\n## Experience\n\n**AI/ML Engineer | TCS (Walgreens, USA) | Feb 2022 – Present**\n\nDesigned and deployed RAG pipeline with Databricks + Pinecone + OpenAI, enabling pharmacists to query compliance docs in natural language with source-cited answers\n\nEngineered ingestion of structured/unstructured data (PDFs, transcripts) via PySpark pipelines, transforming data into embeddings and cutting manual search effort by 60%\n\nAutomated retraining and ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 87
    },
    "id": "ai_ai_ml_gen_ai_309"
  },
  {
    "text": "rs\n\nEngineered ingestion of structured/unstructured data (PDFs, transcripts) via PySpark pipelines, transforming data into embeddings and cutting manual search effort by 60%\n\nAutomated retraining and re-indexing workflows using Airflow, improving model freshness and reducing knowledge gaps across pharmacy operations\n\nDeployed scalable inference APIs on AKS with CI/CD in Azure DevOps, ensuring zero-downtime rollouts and 35% latency reduction with optimized caching + prompts\n\nImplemented governance with Unity Catalog and PII scrubbing, securing sensitive patient data while maintaining HIPAA compliance in embeddings and responses\n\n**ML Engineer | CVS Health (USA) | Jan 2021 – Jan 2022**\n\nBuilt demand forecasting models in PySpark + TensorFlow predicting sales and supply trends, improving procurement accuracy and saving \\$15M annually\n\nEngineered feature pipelines with dbt + Databricks for model-ready datasets, cutting preparation time by 40% and ensuring consistency across teams\n\nTracked ",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 88
    },
    "id": "ai_ai_ml_gen_ai_310"
  },
  {
    "text": "urement accuracy and saving \\$15M annually\n\nEngineered feature pipelines with dbt + Databricks for model-ready datasets, cutting preparation time by 40% and ensuring consistency across teams\n\nTracked experiments with MLflow and automated retraining pipelines, boosting model performance by 18% over baseline\n\nDeployed models to production via Azure ML, implementing monitoring for drift and automating retraining triggers\n\n**Data Science Intern | McKesson (USA) | May 2020 – Dec 2020**\n\nDeveloped ETL + ML scripts in Python reducing ingestion latency 50%, powering dashboards for executive decisions on financial integrity\n\nBuilt regression + time series models forecasting patient demand, preventing supply mismatches and reducing stockouts by 22%\n\nProduced compliance-focused ML insights aligning with audit requirements and informing leadership’s strategic reviews\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nBuilt backend APIs and optimized SQL queries for ERP modu",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 89
    },
    "id": "ai_ai_ml_gen_ai_311"
  },
  {
    "text": "ith audit requirements and informing leadership’s strategic reviews\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nBuilt backend APIs and optimized SQL queries for ERP modules, strengthening transactional accuracy and improving response times by 35%\n\nDesigned reporting modules surfacing anomalies in contracts and payments, reducing manual reconciliation workload\n\n---\n\n## Skills\n\n- **AI/ML & GenAI:** PyTorch, TensorFlow, Hugging Face, OpenAI API, LangChain, MLflow, RAG pipelines, Vector DBs (Pinecone, FAISS, Weaviate)\n- **Data Engineering & ELT:** PySpark, Databricks, Airflow, dbt\n- **Cloud Platforms:** Azure (ML, Data Factory, AKS, DevOps), AWS (SageMaker, Lambda, S3)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle, Vector DBs\n- **Analytics & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Governance:** Azure DevOps, GitHub Actions, Docker, Kubernetes, Unity Catalog, PII Masking, Model Monitoring\n\n---\n\n# 📑 Resume Optimization Frame",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 90
    },
    "id": "ai_ai_ml_gen_ai_312"
  },
  {
    "text": " & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Governance:** Azure DevOps, GitHub Actions, Docker, Kubernetes, Unity Catalog, PII Masking, Model Monitoring\n\n---\n\n# 📑 Resume Optimization Framework – AI/ML/GenAI Version\n\n## 1. Structure & Bullet Rules\n\n**5-4-3-2 Rule**\n\n- Walgreens → 5 bullets | Present tense | Outcome + metrics | Core tech (RAG, GenAI, PySpark, Databricks)\n- CVS → 4 bullets | Past tense | ML pipelines + forecasting | TensorFlow, Databricks, dbt\n- McKesson → 3 bullets | Past tense | Forecasting + compliance outcomes | Python + ML models\n- Inditek → 2 bullets | Past tense | Developer foundation | APIs + SQL\n\n**Character Constraint Rule**\n\n- Each bullet = **220–240 characters**\n- Never <215, never >240\n\n---\n\n## 2. Domain Adaptation Layer\n\nAdapt **vocabulary + emphasis** per JD:\n\n- GenAI/LLMs → RAG, vector DBs, embeddings, LangChain, prompt optimization\n- ML Engineering → model training, feature pipelines, MLflow, deployment\n- Data + AI → hybrid roles bridging inge",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 91
    },
    "id": "ai_ai_ml_gen_ai_313"
  },
  {
    "text": "** per JD:\n\n- GenAI/LLMs → RAG, vector DBs, embeddings, LangChain, prompt optimization\n- ML Engineering → model training, feature pipelines, MLflow, deployment\n- Data + AI → hybrid roles bridging ingestion, ELT, and ML ops\n- Compliance/Healthcare → HIPAA, lineage, governance for AI outputs\n\n---\n\n## 3. Skills Optimization Layer\n\n**Always keep 6 categories:**\n\n1. AI/ML & GenAI\n2. Data Engineering & ELT\n3. Cloud Platforms\n4. Databases & Storage\n5. Analytics & BI\n6. DevOps & Governance\n\n**JD Alignment:**\n\n- Re-order tools inside categories (AWS ML before Azure ML if JD says AWS)\n- Highlight GenAI (LangChain, OpenAI, vector DBs) when relevant\n- Strictly ATS-optimized, no jargon\n\n---\n\n## 4. Verb Rotation Rule\n\nRotate verbs: **Engineer, Optimize, Automate, Deploy, Develop, Train, Fine-tune, Implement, Orchestrate, Scale, Monitor**\n\n---\n\n## 5. Metrics Bank Rule\n\nEvery bullet ties to measurable outcome:\n\n- % improvements (latency -40%, accuracy +18%)\n- Scale (10TB+, 200+ models tracked, 1M+ emb",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 92
    },
    "id": "ai_ai_ml_gen_ai_314"
  },
  {
    "text": "ment, Orchestrate, Scale, Monitor**\n\n---\n\n## 5. Metrics Bank Rule\n\nEvery bullet ties to measurable outcome:\n\n- % improvements (latency -40%, accuracy +18%)\n- Scale (10TB+, 200+ models tracked, 1M+ embeddings)\n- Cost savings (\\$15M+ savings, 30% infra cost cut)\n- Risk/Compliance outcomes (HIPAA alignment, audit-ready insights)\n- User impact (pharmacists saved 60% manual search time)\n\n---\n\n# 📋 JD → Resume Adaptation Checklist\n\n1. **Read JD closely** → Look for AI/ML vs GenAI emphasis (RAG, embeddings, ML pipelines).\n2. **Apply Domain Layer** → Mirror vocabulary (e.g., LLM + LangChain vs forecasting + TensorFlow).\n3. **Reorder Skills** → Keep 6 categories, move JD-priority tools up front.\n4. **Rewrite Bullets** → Use 5-4-3-2, adjust tech names, keep measurable outcomes.\n5. **Rotate Verbs** → Ensure diversity, no repetition.\n6. **Check Character Count** → 220–240 characters each.\n7. **Final Scan** → ATS keywords aligned, outcomes present, tense correct.\n\n---\n\n# 🎯 JD-Specific Summary Templa",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 93
    },
    "id": "ai_ai_ml_gen_ai_315"
  },
  {
    "text": "Ensure diversity, no repetition.\n6. **Check Character Count** → 220–240 characters each.\n7. **Final Scan** → ATS keywords aligned, outcomes present, tense correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**GenAI/LLM-Heavy JD**\\\nAI/ML Engineer with 6+ years’ experience delivering RAG pipelines, vector database search, and LLM-powered assistants. Skilled in Databricks, PySpark, OpenAI, and Hugging Face with proven impact on compliance, scalability, and latency reduction.\n\n**ML Engineering JD**\\\nMachine Learning Engineer with 6+ years’ experience designing feature pipelines, training models in TensorFlow/PyTorch, and deploying production systems via Azure ML & SageMaker. Strong background in Databricks, MLflow, and cost-optimized ML Ops.\n\n**Hybrid Data+AI JD**\\\nData & AI Engineer with 6+ years bridging ELT pipelines and ML/GenAI systems. Skilled in PySpark, dbt, Databricks, and OpenAI APIs with experience in feature engineering, governance, and deploying ML + GenAI solutions to productio",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 94
    },
    "id": "ai_ai_ml_gen_ai_316"
  },
  {
    "text": "rs bridging ELT pipelines and ML/GenAI systems. Skilled in PySpark, dbt, Databricks, and OpenAI APIs with experience in feature engineering, governance, and deploying ML + GenAI solutions to production at scale.\n\n---\n\n",
    "metadata": {
      "persona": "ai",
      "file_path": "kb/ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md",
      "chunk_index": 95
    },
    "id": "ai_ai_ml_gen_ai_317"
  },
  {
    "text": "---\ntags: [analytics-engineer, python, pandas, pyspark, data-transformation]\npersona: ae\n---\n\n# Python for Analytics Engineers - Tejuu's Skills\n\n## Introduction\n**Tejuu's Python Journey:**\nAs an Analytics Engineer, I use Python daily - primarily for data transformation, testing, and automation. While I'm not a software engineer, my Python skills help me build reliable data pipelines and solve business problems efficiently.\n\n## Pandas for Data Transformation\n\n**Tejuu's Common Use Cases:**\nI use pandas for prototyping transformations, data quality checks, and ad-hoc analysis.\n\n**Data Cleaning Example:**\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Read data\ndf_customers = pd.read_csv('customers.csv')\n\n# Clean and standardize\ndf_clean = (\n    df_customers\n    # Remove duplicates\n    .drop_duplicates(subset='customer_id', keep='last')\n    \n    # Clean email addresses\n    .assign(email=lambda x: x['email'].str.lower().str.strip())\n    \n    # Standardize ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 0
    },
    "id": "ae_ae_python_skills_318"
  },
  {
    "text": "    # Remove duplicates\n    .drop_duplicates(subset='customer_id', keep='last')\n    \n    # Clean email addresses\n    .assign(email=lambda x: x['email'].str.lower().str.strip())\n    \n    # Standardize names\n    .assign(\n        first_name=lambda x: x['first_name'].str.title().str.strip(),\n        last_name=lambda x: x['last_name'].str.title().str.strip()\n    )\n    \n    # Handle missing values\n    .assign(\n        phone=lambda x: x['phone'].fillna('Unknown'),\n        city=lambda x: x['city'].fillna('Not Provided')\n    )\n    \n    # Parse dates\n    .assign(registration_date=lambda x: pd.to_datetime(x['registration_date']))\n    \n    # Calculate customer tenure\n    .assign(\n        customer_tenure_days=lambda x: (datetime.now() - x['registration_date']).dt.days\n    )\n)\n\nprint(df_clean.head())\n```\n\n**Business Logic Implementation:**\n```python\n# Calculate customer segment based on business rules\n\ndef calculate_customer_segment(ltv, tenure_days, order_count):\n    \"\"\"\n    Business rules from Mar",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 1
    },
    "id": "ae_ae_python_skills_319"
  },
  {
    "text": "`\n\n**Business Logic Implementation:**\n```python\n# Calculate customer segment based on business rules\n\ndef calculate_customer_segment(ltv, tenure_days, order_count):\n    \"\"\"\n    Business rules from Marketing team:\n    - Platinum: LTV > $10,000 OR (LTV > $5,000 AND tenure > 365 days)\n    - Gold: LTV > $5,000 OR (LTV > $2,000 AND order_count > 10)\n    - Silver: LTV > $1,000 OR order_count > 5\n    - Bronze: Everyone else\n    \"\"\"\n    if ltv > 10000 or (ltv > 5000 and tenure_days > 365):\n        return 'Platinum'\n    elif ltv > 5000 or (ltv > 2000 and order_count > 10):\n        return 'Gold'\n    elif ltv > 1000 or order_count > 5:\n        return 'Silver'\n    else:\n        return 'Bronze'\n\n# Apply business rules\ndf_clean['customer_segment'] = df_clean.apply(\n    lambda row: calculate_customer_segment(\n        row['lifetime_value'],\n        row['customer_tenure_days'],\n        row['order_count']\n    ),\n    axis=1\n)\n```\n\n**Data Quality Checks:**\n```python\n# Comprehensive data quality report\n\nde",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 2
    },
    "id": "ae_ae_python_skills_320"
  },
  {
    "text": "ment(\n        row['lifetime_value'],\n        row['customer_tenure_days'],\n        row['order_count']\n    ),\n    axis=1\n)\n```\n\n**Data Quality Checks:**\n```python\n# Comprehensive data quality report\n\ndef generate_data_quality_report(df, table_name):\n    \"\"\"Generate data quality report for stakeholders\"\"\"\n    \n    report = {\n        'table_name': table_name,\n        'timestamp': datetime.now(),\n        'row_count': len(df),\n        'column_count': len(df.columns),\n        'checks': []\n    }\n    \n    # Check 1: Null values\n    null_counts = df.isnull().sum()\n    null_pct = (null_counts / len(df) * 100).round(2)\n    \n    for col in df.columns:\n        if null_counts[col] > 0:\n            report['checks'].append({\n                'check': 'null_values',\n                'column': col,\n                'null_count': int(null_counts[col]),\n                'null_percentage': float(null_pct[col]),\n                'status': 'WARNING' if null_pct[col] > 5 else 'INFO'\n            })\n    \n    # Check ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 3
    },
    "id": "ae_ae_python_skills_321"
  },
  {
    "text": "     'null_count': int(null_counts[col]),\n                'null_percentage': float(null_pct[col]),\n                'status': 'WARNING' if null_pct[col] > 5 else 'INFO'\n            })\n    \n    # Check 2: Duplicates\n    dup_count = df.duplicated().sum()\n    if dup_count > 0:\n        report['checks'].append({\n            'check': 'duplicates',\n            'duplicate_count': int(dup_count),\n            'status': 'ERROR'\n        })\n    \n    # Check 3: Data types\n    for col in df.columns:\n        expected_type = get_expected_type(col)  # From metadata\n        actual_type = str(df[col].dtype)\n        if expected_type != actual_type:\n            report['checks'].append({\n                'check': 'data_type_mismatch',\n                'column': col,\n                'expected': expected_type,\n                'actual': actual_type,\n                'status': 'ERROR'\n            })\n    \n    # Check 4: Value ranges\n    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    for col",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 4
    },
    "id": "ae_ae_python_skills_322"
  },
  {
    "text": "          'actual': actual_type,\n                'status': 'ERROR'\n            })\n    \n    # Check 4: Value ranges\n    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    for col in numeric_cols:\n        if col == 'order_amount':\n            invalid_count = ((df[col] < 0) | (df[col] > 1000000)).sum()\n            if invalid_count > 0:\n                report['checks'].append({\n                    'check': 'value_range',\n                    'column': col,\n                    'invalid_count': int(invalid_count),\n                    'status': 'ERROR'\n                })\n    \n    # Generate summary\n    error_count = len([c for c in report['checks'] if c['status'] == 'ERROR'])\n    warning_count = len([c for c in report['checks'] if c['status'] == 'WARNING'])\n    \n    report['summary'] = {\n        'total_checks': len(report['checks']),\n        'errors': error_count,\n        'warnings': warning_count,\n        'overall_status': 'FAILED' if error_count > 0 else 'PASSED'\n    }",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 5
    },
    "id": "ae_ae_python_skills_323"
  },
  {
    "text": "summary'] = {\n        'total_checks': len(report['checks']),\n        'errors': error_count,\n        'warnings': warning_count,\n        'overall_status': 'FAILED' if error_count > 0 else 'PASSED'\n    }\n    \n    return report\n\n# Run quality checks\nreport = generate_data_quality_report(df_clean, 'dim_customer')\nprint(f\"Data Quality: {report['summary']['overall_status']}\")\nprint(f\"Errors: {report['summary']['errors']}, Warnings: {report['summary']['warnings']}\")\n```\n\n**Business Value:**\nFinance CFO asked: \"How do I know the customer segments are correct?\"\nI showed him the data quality report with business rule validation. He was impressed!\n\n## PySpark for Large-Scale Transformations\n\n**Tejuu's PySpark Usage:**\nFor processing millions of rows, I use PySpark in Databricks.\n\n**Example: Customer LTV Calculation:**\n```python\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql import functions as F\n\nspark = SparkSession.builder.appName(\"CustomerLTV\").getOrCreate()\n\n# Read data\ndf_order",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 6
    },
    "id": "ae_ae_python_skills_324"
  },
  {
    "text": "TV Calculation:**\n```python\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql import functions as F\n\nspark = SparkSession.builder.appName(\"CustomerLTV\").getOrCreate()\n\n# Read data\ndf_orders = spark.read.format(\"delta\").load(\"/mnt/raw/orders\")\ndf_customers = spark.read.format(\"delta\").load(\"/mnt/raw/customers\")\n\n# Calculate customer metrics\ndf_customer_metrics = (\n    df_orders\n    .filter(F.col(\"order_status\") == \"completed\")\n    .groupBy(\"customer_id\")\n    .agg(\n        F.min(\"order_date\").alias(\"first_order_date\"),\n        F.max(\"order_date\").alias(\"last_order_date\"),\n        F.count(\"order_id\").alias(\"total_orders\"),\n        F.sum(\"order_amount\").alias(\"lifetime_value\"),\n        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n        F.countDistinct(F.col(\"order_date\").cast(\"date\")).alias(\"active_days\")\n    )\n    .withColumn(\n        \"customer_tenure_days\",\n        F.datediff(F.current_date(), F.col(\"first_order_date\"))\n    )\n    .withColumn(\n        \"days_since_last_ord",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 7
    },
    "id": "ae_ae_python_skills_325"
  },
  {
    "text": "\"date\")).alias(\"active_days\")\n    )\n    .withColumn(\n        \"customer_tenure_days\",\n        F.datediff(F.current_date(), F.col(\"first_order_date\"))\n    )\n    .withColumn(\n        \"days_since_last_order\",\n        F.datediff(F.current_date(), F.col(\"last_order_date\"))\n    )\n    .withColumn(\n        \"is_active\",\n        F.when(F.col(\"days_since_last_order\") <= 90, True).otherwise(False)\n    )\n)\n\n# Join with customer dimensions\ndf_customer_enriched = (\n    df_customer_metrics\n    .join(df_customers, \"customer_id\", \"left\")\n    .select(\n        \"customer_id\",\n        \"customer_name\",\n        \"email\",\n        \"first_order_date\",\n        \"last_order_date\",\n        \"total_orders\",\n        \"lifetime_value\",\n        \"avg_order_value\",\n        \"customer_tenure_days\",\n        \"days_since_last_order\",\n        \"is_active\"\n    )\n)\n\n# Apply customer segmentation\ndf_final = (\n    df_customer_enriched\n    .withColumn(\n        \"customer_segment\",\n        F.when(\n            (F.col(\"lifetime_value\") > 100",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 8
    },
    "id": "ae_ae_python_skills_326"
  },
  {
    "text": "        \"is_active\"\n    )\n)\n\n# Apply customer segmentation\ndf_final = (\n    df_customer_enriched\n    .withColumn(\n        \"customer_segment\",\n        F.when(\n            (F.col(\"lifetime_value\") > 10000) | \n            ((F.col(\"lifetime_value\") > 5000) & (F.col(\"customer_tenure_days\") > 365)),\n            \"Platinum\"\n        )\n        .when(\n            (F.col(\"lifetime_value\") > 5000) | \n            ((F.col(\"lifetime_value\") > 2000) & (F.col(\"total_orders\") > 10)),\n            \"Gold\"\n        )\n        .when(\n            (F.col(\"lifetime_value\") > 1000) | (F.col(\"total_orders\") > 5),\n            \"Silver\"\n        )\n        .otherwise(\"Bronze\")\n    )\n)\n\n# Write to Delta\n(\n    df_final.write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"overwriteSchema\", \"true\")\n    .save(\"/mnt/analytics/dim_customer\")\n)\n\nprint(f\"✅ Processed {df_final.count():,} customers\")\n```\n\n**Window Functions for Rankings:**\n```python\n# Calculate product rankings within each category\n\nfrom pyspark.sql.windo",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 9
    },
    "id": "ae_ae_python_skills_327"
  },
  {
    "text": "ytics/dim_customer\")\n)\n\nprint(f\"✅ Processed {df_final.count():,} customers\")\n```\n\n**Window Functions for Rankings:**\n```python\n# Calculate product rankings within each category\n\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"product_category\").orderBy(F.desc(\"total_revenue\"))\n\ndf_product_rankings = (\n    df_sales\n    .groupBy(\"product_category\", \"product_id\", \"product_name\")\n    .agg(\n        F.sum(\"sales_amount\").alias(\"total_revenue\"),\n        F.sum(\"quantity\").alias(\"total_quantity\"),\n        F.count(\"order_id\").alias(\"order_count\")\n    )\n    .withColumn(\"category_rank\", F.row_number().over(window_spec))\n    .withColumn(\"revenue_pct_of_category\", \n        F.col(\"total_revenue\") / F.sum(\"total_revenue\").over(Window.partitionBy(\"product_category\"))\n    )\n    .filter(F.col(\"category_rank\") <= 10)  # Top 10 per category\n    .orderBy(\"product_category\", \"category_rank\")\n)\n```\n\n## Python for dbt Integration\n\n**Tejuu's dbt Helper Scripts:**\n\n**Script to Generate d",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 10
    },
    "id": "ae_ae_python_skills_328"
  },
  {
    "text": "ter(F.col(\"category_rank\") <= 10)  # Top 10 per category\n    .orderBy(\"product_category\", \"category_rank\")\n)\n```\n\n## Python for dbt Integration\n\n**Tejuu's dbt Helper Scripts:**\n\n**Script to Generate dbt Models from Database:**\n```python\n# generate_dbt_models.py\n# Automatically generate dbt staging models from source tables\n\nimport psycopg2\nimport yaml\nfrom pathlib import Path\n\ndef get_table_schema(conn, schema, table):\n    \"\"\"Get column information for a table\"\"\"\n    query = f\"\"\"\n        SELECT \n            column_name,\n            data_type,\n            is_nullable\n        FROM information_schema.columns\n        WHERE table_schema = '{schema}'\n          AND table_name = '{table}'\n        ORDER BY ordinal_position\n    \"\"\"\n    \n    cursor = conn.cursor()\n    cursor.execute(query)\n    columns = cursor.fetchall()\n    return columns\n\ndef generate_staging_model(schema, table, columns):\n    \"\"\"Generate dbt staging model SQL\"\"\"\n    \n    model_name = f\"stg_{schema}_{table}\"\n    \n    # SQL temp",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 11
    },
    "id": "ae_ae_python_skills_329"
  },
  {
    "text": "s = cursor.fetchall()\n    return columns\n\ndef generate_staging_model(schema, table, columns):\n    \"\"\"Generate dbt staging model SQL\"\"\"\n    \n    model_name = f\"stg_{schema}_{table}\"\n    \n    # SQL template\n    sql = f\"\"\"-- models/staging/{schema}/{model_name}.sql\n\n{{{{\n    config(\n        materialized='view',\n        schema='staging'\n    )\n}}}}\n\nWITH source AS (\n    SELECT * FROM {{{{ source('{schema}', '{table}') }}}}\n),\n\nrenamed AS (\n    SELECT\n\"\"\"\n    \n    # Add columns\n    col_lines = []\n    for col_name, data_type, is_nullable in columns:\n        # Clean column name\n        clean_name = col_name.lower().replace(' ', '_')\n        \n        # Add type casting if needed\n        if data_type in ['varchar', 'text']:\n            col_lines.append(f\"        TRIM({col_name}) AS {clean_name}\")\n        elif data_type == 'timestamp':\n            col_lines.append(f\"        {col_name}::TIMESTAMP AS {clean_name}\")\n        else:\n            col_lines.append(f\"        {col_name} AS {clean_name}\")\n  ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 12
    },
    "id": "ae_ae_python_skills_330"
  },
  {
    "text": "      elif data_type == 'timestamp':\n            col_lines.append(f\"        {col_name}::TIMESTAMP AS {clean_name}\")\n        else:\n            col_lines.append(f\"        {col_name} AS {clean_name}\")\n    \n    sql += \",\\n\".join(col_lines)\n    sql += \"\\n    FROM source\\n)\\n\\nSELECT * FROM renamed\"\n    \n    return model_name, sql\n\ndef generate_schema_yml(schema, table, columns):\n    \"\"\"Generate dbt schema.yml\"\"\"\n    \n    model_name = f\"stg_{schema}_{table}\"\n    \n    schema_dict = {\n        'version': 2,\n        'models': [{\n            'name': model_name,\n            'description': f'Staging model for {schema}.{table}',\n            'columns': [\n                {\n                    'name': col[0].lower().replace(' ', '_'),\n                    'description': '',\n                    'tests': ['not_null'] if col[2] == 'NO' else []\n                }\n                for col in columns\n            ]\n        }]\n    }\n    \n    return yaml.dump(schema_dict, default_flow_style=False)\n\n# Connect to da",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 13
    },
    "id": "ae_ae_python_skills_331"
  },
  {
    "text": "t_null'] if col[2] == 'NO' else []\n                }\n                for col in columns\n            ]\n        }]\n    }\n    \n    return yaml.dump(schema_dict, default_flow_style=False)\n\n# Connect to database\nconn = psycopg2.connect(\n    host=\"your-database.com\",\n    database=\"analytics\",\n    user=\"readonly\",\n    password=\"secret\"\n)\n\n# Generate models for all tables in raw schema\ncursor = conn.cursor()\ncursor.execute(\"\"\"\n    SELECT table_name\n    FROM information_schema.tables\n    WHERE table_schema = 'raw'\n    ORDER BY table_name\n\"\"\")\n\ntables = cursor.fetchall()\n\nfor (table_name,) in tables:\n    print(f\"Generating model for raw.{table_name}...\")\n    \n    # Get schema\n    columns = get_table_schema(conn, 'raw', table_name)\n    \n    # Generate SQL\n    model_name, sql = generate_staging_model('raw', table_name, columns)\n    \n    # Write SQL file\n    sql_path = Path(f\"models/staging/raw/{model_name}.sql\")\n    sql_path.parent.mkdir(parents=True, exist_ok=True)\n    sql_path.write_text(sql)\n  ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 14
    },
    "id": "ae_ae_python_skills_332"
  },
  {
    "text": "'raw', table_name, columns)\n    \n    # Write SQL file\n    sql_path = Path(f\"models/staging/raw/{model_name}.sql\")\n    sql_path.parent.mkdir(parents=True, exist_ok=True)\n    sql_path.write_text(sql)\n    \n    # Generate schema.yml\n    schema_yml = generate_schema_yml('raw', table_name, columns)\n    yml_path = Path(f\"models/staging/raw/schema.yml\")\n    yml_path.write_text(schema_yml)\n    \n    print(f\"  ✅ Created {sql_path}\")\n\nconn.close()\nprint(\"\\n🎉 All models generated!\")\n```\n\n**Business Impact:**\nSaved 20+ hours manually writing staging models for 50+ source tables!\n\n## Automation Scripts\n\n**Tejuu's Daily Helpers:**\n\n**Script to Check Data Freshness:**\n```python\n# check_data_freshness.py\n# Alert if source data is stale\n\nimport boto3\nfrom datetime import datetime, timedelta\nimport json\n\ndef check_s3_freshness(bucket, prefix, max_age_hours=24):\n    \"\"\"Check if files in S3 are recent\"\"\"\n    \n    s3 = boto3.client('s3')\n    \n    response = s3.list_objects_v2(\n        Bucket=bucket,\n        ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 15
    },
    "id": "ae_ae_python_skills_333"
  },
  {
    "text": "check_s3_freshness(bucket, prefix, max_age_hours=24):\n    \"\"\"Check if files in S3 are recent\"\"\"\n    \n    s3 = boto3.client('s3')\n    \n    response = s3.list_objects_v2(\n        Bucket=bucket,\n        Prefix=prefix,\n        MaxKeys=1\n    )\n    \n    if 'Contents' not in response:\n        return False, f\"No files found in {prefix}\"\n    \n    last_modified = response['Contents'][0]['LastModified']\n    age_hours = (datetime.now(last_modified.tzinfo) - last_modified).seconds / 3600\n    \n    is_fresh = age_hours < max_age_hours\n    message = f\"Data is {age_hours:.1f} hours old (max: {max_age_hours})\"\n    \n    return is_fresh, message\n\ndef send_alert(message):\n    \"\"\"Send Slack alert\"\"\"\n    sns = boto3.client('sns')\n    sns.publish(\n        TopicArn='arn:aws:sns:us-east-1:123456789:data-freshness-alerts',\n        Subject='Data Freshness Alert',\n        Message=message\n    )\n\n# Check critical data sources\nsources_to_check = [\n    ('my-bucket', 'raw/salesforce/', 12),\n    ('my-bucket', 'raw/datab",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 16
    },
    "id": "ae_ae_python_skills_334"
  },
  {
    "text": "lerts',\n        Subject='Data Freshness Alert',\n        Message=message\n    )\n\n# Check critical data sources\nsources_to_check = [\n    ('my-bucket', 'raw/salesforce/', 12),\n    ('my-bucket', 'raw/database_extracts/', 24),\n    ('my-bucket', 'raw/api_data/', 6)\n]\n\nalerts = []\n\nfor bucket, prefix, max_age in sources_to_check:\n    is_fresh, message = check_s3_freshness(bucket, prefix, max_age)\n    \n    if not is_fresh:\n        alerts.append(f\"⚠️  {prefix}: {message}\")\n    else:\n        print(f\"✅ {prefix}: {message}\")\n\nif alerts:\n    alert_message = \"Data Freshness Issues:\\n\\n\" + \"\\n\".join(alerts)\n    send_alert(alert_message)\n    print(\"\\n❌ Alerts sent!\")\nelse:\n    print(\"\\n✅ All data sources are fresh!\")\n```\n\n**Script to Compare dbt Runs:**\n```python\n# compare_dbt_runs.py\n# Compare results between dbt runs to catch issues\n\nimport json\nfrom pathlib import Path\n\ndef load_run_results(file_path):\n    \"\"\"Load dbt run_results.json\"\"\"\n    with open(file_path) as f:\n        return json.load(f)\n\nde",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 17
    },
    "id": "ae_ae_python_skills_335"
  },
  {
    "text": "ween dbt runs to catch issues\n\nimport json\nfrom pathlib import Path\n\ndef load_run_results(file_path):\n    \"\"\"Load dbt run_results.json\"\"\"\n    with open(file_path) as f:\n        return json.load(f)\n\ndef compare_runs(baseline_path, current_path):\n    \"\"\"Compare two dbt runs\"\"\"\n    \n    baseline = load_run_results(baseline_path)\n    current = load_run_results(current_path)\n    \n    # Get model results\n    baseline_models = {\n        r['unique_id']: r \n        for r in baseline['results']\n    }\n    \n    current_models = {\n        r['unique_id']: r \n        for r in current['results']\n    }\n    \n    issues = []\n    \n    # Check for new failures\n    for model_id, result in current_models.items():\n        if result['status'] == 'error':\n            baseline_result = baseline_models.get(model_id, {})\n            if baseline_result.get('status') != 'error':\n                issues.append(f\"❌ NEW FAILURE: {model_id}\")\n    \n    # Check for large row count changes\n    for model_id in baseline_model",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 18
    },
    "id": "ae_ae_python_skills_336"
  },
  {
    "text": "{})\n            if baseline_result.get('status') != 'error':\n                issues.append(f\"❌ NEW FAILURE: {model_id}\")\n    \n    # Check for large row count changes\n    for model_id in baseline_models.keys() & current_models.keys():\n        baseline_rows = baseline_models[model_id].get('rows_affected', 0)\n        current_rows = current_models[model_id].get('rows_affected', 0)\n        \n        if baseline_rows > 0:\n            pct_change = abs(current_rows - baseline_rows) / baseline_rows * 100\n            \n            if pct_change > 20:  # More than 20% change\n                issues.append(\n                    f\"⚠️  ROW COUNT CHANGE: {model_id} \"\n                    f\"({baseline_rows:,} → {current_rows:,}, {pct_change:.1f}% change)\"\n                )\n    \n    # Check for slow models\n    for model_id, result in current_models.items():\n        execution_time = result.get('execution_time', 0)\n        if execution_time > 300:  # More than 5 minutes\n            issues.append(f\"🐌 SLOW MODE",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 19
    },
    "id": "ae_ae_python_skills_337"
  },
  {
    "text": "for model_id, result in current_models.items():\n        execution_time = result.get('execution_time', 0)\n        if execution_time > 300:  # More than 5 minutes\n            issues.append(f\"🐌 SLOW MODEL: {model_id} ({execution_time:.0f}s)\")\n    \n    return issues\n\n# Run comparison\nissues = compare_runs(\n    'target/baseline_run_results.json',\n    'target/run_results.json'\n)\n\nif issues:\n    print(\"\\n🚨 Issues found:\\n\")\n    for issue in issues:\n        print(issue)\nelse:\n    print(\"\\n✅ No issues found!\")\n```\n\nThese Python skills help me build reliable, maintainable analytics pipelines that deliver business value!\n\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_python_skills.md",
      "file_name": "ae_python_skills.md",
      "chunk_index": 20
    },
    "id": "ae_ae_python_skills_338"
  },
  {
    "text": "---\ntags: [analytics-engineer, dbt, data-modeling, sql, data-warehouse, metrics]\npersona: ae\n---\n\n# Analytics Engineer & Tejuu's Experience\n\n## Modern Data Stack and Analytics Engineering\n\n### What is an Analytics Engineer?\n**Tejuu's Role:**\nSo as an Analytics Engineer, I'm kind of the bridge between data engineering and business intelligence. I take raw data from data warehouses and transform it into clean, well-modeled datasets that business users can easily understand and use for analysis.\n\nThe key difference from a traditional BI developer is that I write code (mostly SQL and Python) to build data models, use version control, write tests, and follow software engineering best practices. It's like being a data engineer but focused on the analytics layer rather than data pipelines.\n\n**My Responsibilities:**\n```\n1. Data Modeling\n   - Design dimensional models (star/snowflake schemas)\n   - Create reusable data marts\n   - Define metrics and KPIs\n   - Document data lineage\n\n2. Transformat",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 21
    },
    "id": "ae_ae_overview_339"
  },
  {
    "text": "Responsibilities:**\n```\n1. Data Modeling\n   - Design dimensional models (star/snowflake schemas)\n   - Create reusable data marts\n   - Define metrics and KPIs\n   - Document data lineage\n\n2. Transformation Development\n   - Write SQL transformations\n   - Build dbt models\n   - Create staging, intermediate, and mart layers\n   - Implement business logic\n\n3. Data Quality & Testing\n   - Write data quality tests\n   - Implement validation rules\n   - Monitor data freshness\n   - Alert on anomalies\n\n4. Documentation & Governance\n   - Document data models\n   - Define metrics definitions\n   - Create data dictionaries\n   - Maintain data catalog\n\n5. Collaboration\n   - Work with data engineers on data pipelines\n   - Partner with analysts on requirements\n   - Support BI developers with data models\n   - Train business users on data usage\n```\n\n## dbt (Data Build Tool)\n\n### Building Transformation Pipelines with dbt\n**Tejuu's dbt Experience:**\nI use dbt daily to transform raw data into analytics-ready datas",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 22
    },
    "id": "ae_ae_overview_340"
  },
  {
    "text": " business users on data usage\n```\n\n## dbt (Data Build Tool)\n\n### Building Transformation Pipelines with dbt\n**Tejuu's dbt Experience:**\nI use dbt daily to transform raw data into analytics-ready datasets. It's become the standard tool for analytics engineering, and I love how it brings software engineering practices to data transformation.\n\n**My dbt Project Structure:**\n```\nmy_dbt_project/\n├── models/\n│   ├── staging/          # Raw data cleaning\n│   │   ├── stg_customers.sql\n│   │   ├── stg_orders.sql\n│   │   └── stg_products.sql\n│   ├── intermediate/     # Business logic\n│   │   ├── int_customer_orders.sql\n│   │   └── int_order_items.sql\n│   └── marts/           # Final analytics tables\n│       ├── finance/\n│       │   ├── fct_revenue.sql\n│       │   └── dim_customers.sql\n│       └── marketing/\n│           ├── fct_campaigns.sql\n│           └── dim_customer_segments.sql\n├── tests/\n│   └── assert_positive_revenue.sql\n├── macros/\n│   └── generate_schema_name.sql\n└── dbt_project.yml\n```\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 23
    },
    "id": "ae_ae_overview_341"
  },
  {
    "text": "keting/\n│           ├── fct_campaigns.sql\n│           └── dim_customer_segments.sql\n├── tests/\n│   └── assert_positive_revenue.sql\n├── macros/\n│   └── generate_schema_name.sql\n└── dbt_project.yml\n```\n\n**Staging Layer Example:**\n```sql\n-- models/staging/stg_customers.sql\n{{\n    config(\n        materialized='view',\n        schema='staging'\n    )\n}}\n\nWITH source AS (\n    SELECT * FROM {{ source('raw', 'customers') }}\n),\n\nrenamed AS (\n    SELECT\n        customer_id,\n        TRIM(LOWER(email)) AS email,\n        TRIM(first_name) AS first_name,\n        TRIM(last_name) AS last_name,\n        CONCAT(first_name, ' ', last_name) AS full_name,\n        phone,\n        address,\n        city,\n        state,\n        zip_code,\n        country,\n        created_at,\n        updated_at\n    FROM source\n    WHERE customer_id IS NOT NULL\n)\n\nSELECT * FROM renamed\n```\n\n**Intermediate Layer Example:**\n```sql\n-- models/intermediate/int_customer_orders.sql\n{{\n    config(\n        materialized='ephemeral'\n    )\n}}\n\nWI",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 24
    },
    "id": "ae_ae_overview_342"
  },
  {
    "text": "customer_id IS NOT NULL\n)\n\nSELECT * FROM renamed\n```\n\n**Intermediate Layer Example:**\n```sql\n-- models/intermediate/int_customer_orders.sql\n{{\n    config(\n        materialized='ephemeral'\n    )\n}}\n\nWITH customers AS (\n    SELECT * FROM {{ ref('stg_customers') }}\n),\n\norders AS (\n    SELECT * FROM {{ ref('stg_orders') }}\n),\n\ncustomer_orders AS (\n    SELECT\n        c.customer_id,\n        c.full_name,\n        c.email,\n        c.city,\n        c.state,\n        o.order_id,\n        o.order_date,\n        o.order_amount,\n        o.order_status\n    FROM customers c\n    LEFT JOIN orders o ON c.customer_id = o.customer_id\n)\n\nSELECT * FROM customer_orders\n```\n\n**Marts Layer Example:**\n```sql\n-- models/marts/finance/fct_revenue.sql\n{{\n    config(\n        materialized='incremental',\n        unique_key='order_id',\n        schema='finance'\n    )\n}}\n\nWITH customer_orders AS (\n    SELECT * FROM {{ ref('int_customer_orders') }}\n),\n\nrevenue_metrics AS (\n    SELECT\n        order_id,\n        customer_id,\n    ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 25
    },
    "id": "ae_ae_overview_343"
  },
  {
    "text": "der_id',\n        schema='finance'\n    )\n}}\n\nWITH customer_orders AS (\n    SELECT * FROM {{ ref('int_customer_orders') }}\n),\n\nrevenue_metrics AS (\n    SELECT\n        order_id,\n        customer_id,\n        order_date,\n        order_amount,\n        \n        -- Time dimensions\n        DATE_TRUNC('month', order_date) AS order_month,\n        DATE_TRUNC('quarter', order_date) AS order_quarter,\n        DATE_TRUNC('year', order_date) AS order_year,\n        \n        -- Customer metrics\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date\n        ) AS customer_order_number,\n        \n        -- Revenue metrics\n        SUM(order_amount) OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) AS customer_lifetime_value,\n        \n        CURRENT_TIMESTAMP AS dbt_updated_at\n        \n    FROM customer_orders\n    WHERE order_status = 'completed'\n    \n    {% if is_incre",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 26
    },
    "id": "ae_ae_overview_344"
  },
  {
    "text": "ND CURRENT ROW\n        ) AS customer_lifetime_value,\n        \n        CURRENT_TIMESTAMP AS dbt_updated_at\n        \n    FROM customer_orders\n    WHERE order_status = 'completed'\n    \n    {% if is_incremental() %}\n        AND order_date > (SELECT MAX(order_date) FROM {{ this }})\n    {% endif %}\n)\n\nSELECT * FROM revenue_metrics\n```\n\n### dbt Tests and Data Quality\n**My Testing Strategy:**\n```yaml\n# models/staging/schema.yml\nversion: 2\n\nmodels:\n  - name: stg_customers\n    description: Cleaned and standardized customer data\n    columns:\n      - name: customer_id\n        description: Primary key for customers\n        tests:\n          - unique\n          - not_null\n      \n      - name: email\n        description: Customer email address\n        tests:\n          - not_null\n          - unique\n          - dbt_utils.email\n      \n      - name: created_at\n        description: Timestamp when customer was created\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 27
    },
    "id": "ae_ae_overview_345"
  },
  {
    "text": " - dbt_utils.email\n      \n      - name: created_at\n        description: Timestamp when customer was created\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \">= '2020-01-01'\"\n\n  - name: stg_orders\n    description: Cleaned and standardized order data\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n      \n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('stg_customers')\n              field: customer_id\n      \n      - name: order_amount\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"\n```\n\n**Custom Tests:**\n```sql\n-- tests/assert_revenue_positive.sql\nSELECT\n    order_id,\n    order_amount\nFROM {{ ref('fct_revenue') }}\nWHERE order_amount <= 0\n```\n\n### dbt Macros for Reusability\n**My Commonly Used Macros:**\n```sql\n-- macros/cents_to_dollars.sql\n{% macro cents_to_dollars(colu",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 28
    },
    "id": "ae_ae_overview_346"
  },
  {
    "text": "order_amount\nFROM {{ ref('fct_revenue') }}\nWHERE order_amount <= 0\n```\n\n### dbt Macros for Reusability\n**My Commonly Used Macros:**\n```sql\n-- macros/cents_to_dollars.sql\n{% macro cents_to_dollars(column_name, precision=2) %}\n    ROUND({{ column_name }} / 100.0, {{ precision }})\n{% endmacro %}\n\n-- Usage in model:\nSELECT\n    order_id,\n    {{ cents_to_dollars('amount_cents') }} AS amount_dollars\nFROM orders\n\n-- macros/generate_surrogate_key.sql\n{% macro generate_surrogate_key(field_list) %}\n    MD5(CONCAT(\n        {% for field in field_list %}\n            COALESCE(CAST({{ field }} AS VARCHAR), '')\n            {% if not loop.last %} || '|' || {% endif %}\n        {% endfor %}\n    ))\n{% endmacro %}\n\n-- Usage:\nSELECT\n    {{ generate_surrogate_key(['customer_id', 'order_date']) }} AS order_key,\n    *\nFROM orders\n\n-- macros/pivot_metric.sql\n{% macro pivot_metric(column, values, metric, agg_function='SUM') %}\n    {% for value in values %}\n        {{ agg_function }}(\n            CASE WHEN {{ colu",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 29
    },
    "id": "ae_ae_overview_347"
  },
  {
    "text": "  *\nFROM orders\n\n-- macros/pivot_metric.sql\n{% macro pivot_metric(column, values, metric, agg_function='SUM') %}\n    {% for value in values %}\n        {{ agg_function }}(\n            CASE WHEN {{ column }} = '{{ value }}' \n            THEN {{ metric }} \n            ELSE 0 END\n        ) AS {{ value | replace(' ', '_') | lower }}\n        {% if not loop.last %}, {% endif %}\n    {% endfor %}\n{% endmacro %}\n```\n\n## Metrics Layer and Semantic Modeling\n\n### Defining Business Metrics\n**Tejuu's Metrics Framework:**\n```yaml\n# metrics/revenue_metrics.yml\nversion: 2\n\nmetrics:\n  - name: total_revenue\n    label: Total Revenue\n    model: ref('fct_revenue')\n    description: Sum of all completed order amounts\n    \n    calculation_method: sum\n    expression: order_amount\n    \n    timestamp: order_date\n    time_grains: [day, week, month, quarter, year]\n    \n    dimensions:\n      - customer_segment\n      - product_category\n      - region\n    \n    filters:\n      - field: order_status\n        operator: '='\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 30
    },
    "id": "ae_ae_overview_348"
  },
  {
    "text": "ime_grains: [day, week, month, quarter, year]\n    \n    dimensions:\n      - customer_segment\n      - product_category\n      - region\n    \n    filters:\n      - field: order_status\n        operator: '='\n        value: \"'completed'\"\n\n  - name: average_order_value\n    label: Average Order Value\n    model: ref('fct_revenue')\n    description: Average amount per order\n    \n    calculation_method: average\n    expression: order_amount\n    \n    timestamp: order_date\n    time_grains: [day, week, month, quarter, year]\n\n  - name: customer_lifetime_value\n    label: Customer Lifetime Value\n    model: ref('dim_customers')\n    description: Total revenue per customer\n    \n    calculation_method: sum\n    expression: total_spent\n    \n    dimensions:\n      - customer_segment\n      - acquisition_channel\n```\n\n## SQL Optimization for Analytics\n\n### Query Performance Optimization\n**Tejuu's SQL Optimization Techniques:**\n```sql\n-- BEFORE: Slow query with subqueries\nSELECT \n    c.customer_id,\n    c.customer_name,",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 31
    },
    "id": "ae_ae_overview_349"
  },
  {
    "text": "SQL Optimization for Analytics\n\n### Query Performance Optimization\n**Tejuu's SQL Optimization Techniques:**\n```sql\n-- BEFORE: Slow query with subqueries\nSELECT \n    c.customer_id,\n    c.customer_name,\n    (SELECT COUNT(*) FROM orders o WHERE o.customer_id = c.customer_id) as order_count,\n    (SELECT SUM(amount) FROM orders o WHERE o.customer_id = c.customer_id) as total_spent\nFROM customers c;\n\n-- AFTER: Optimized with JOIN\nSELECT \n    c.customer_id,\n    c.customer_name,\n    COUNT(o.order_id) as order_count,\n    SUM(o.amount) as total_spent\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.customer_name;\n\n-- BEFORE: Multiple passes over data\nSELECT customer_id, SUM(amount) FROM orders GROUP BY customer_id;\nSELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id;\nSELECT customer_id, AVG(amount) FROM orders GROUP BY customer_id;\n\n-- AFTER: Single pass with multiple aggregations\nSELECT \n    customer_id,\n    SUM(amount) as total_amount,\n  ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 32
    },
    "id": "ae_ae_overview_350"
  },
  {
    "text": "s GROUP BY customer_id;\nSELECT customer_id, AVG(amount) FROM orders GROUP BY customer_id;\n\n-- AFTER: Single pass with multiple aggregations\nSELECT \n    customer_id,\n    SUM(amount) as total_amount,\n    COUNT(*) as order_count,\n    AVG(amount) as avg_amount\nFROM orders\nGROUP BY customer_id;\n\n-- Using CTEs for readability and performance\nWITH monthly_sales AS (\n    SELECT \n        DATE_TRUNC('month', order_date) as month,\n        SUM(amount) as total_sales\n    FROM orders\n    WHERE order_date >= '2023-01-01'\n    GROUP BY DATE_TRUNC('month', order_date)\n),\nsales_with_growth AS (\n    SELECT \n        month,\n        total_sales,\n        LAG(total_sales) OVER (ORDER BY month) as prev_month_sales,\n        (total_sales - LAG(total_sales) OVER (ORDER BY month)) / \n        LAG(total_sales) OVER (ORDER BY month) * 100 as growth_rate\n    FROM monthly_sales\n)\nSELECT * FROM sales_with_growth\nWHERE growth_rate IS NOT NULL\nORDER BY month;\n```\n\n## Data Warehouse Modeling\n\n### Dimensional Modeling\n**Teju",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 33
    },
    "id": "ae_ae_overview_351"
  },
  {
    "text": "R BY month) * 100 as growth_rate\n    FROM monthly_sales\n)\nSELECT * FROM sales_with_growth\nWHERE growth_rate IS NOT NULL\nORDER BY month;\n```\n\n## Data Warehouse Modeling\n\n### Dimensional Modeling\n**Tejuu's Dimensional Model Design:**\n```sql\n-- Dimension Table: Customers\nCREATE TABLE dim_customers (\n    customer_key INT PRIMARY KEY,  -- Surrogate key\n    customer_id VARCHAR(50),        -- Natural key\n    customer_name VARCHAR(200),\n    email VARCHAR(200),\n    segment VARCHAR(50),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    country VARCHAR(50),\n    first_order_date DATE,\n    customer_since_days INT,\n    is_active BOOLEAN,\n    effective_date DATE,            -- SCD Type 2\n    expiration_date DATE,           -- SCD Type 2\n    is_current BOOLEAN,             -- SCD Type 2\n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n);\n\n-- Fact Table: Sales\nCREATE TABLE fct_sales (\n    sale_key BIGINT PRIMARY KEY,\n    order_id VARCHAR(50),\n    customer_key INT REFERENCES dim_customers(customer_key",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 34
    },
    "id": "ae_ae_overview_352"
  },
  {
    "text": "ESTAMP,\n    updated_at TIMESTAMP\n);\n\n-- Fact Table: Sales\nCREATE TABLE fct_sales (\n    sale_key BIGINT PRIMARY KEY,\n    order_id VARCHAR(50),\n    customer_key INT REFERENCES dim_customers(customer_key),\n    product_key INT REFERENCES dim_products(product_key),\n    date_key INT REFERENCES dim_date(date_key),\n    employee_key INT REFERENCES dim_employees(employee_key),\n    \n    -- Measures\n    quantity INT,\n    unit_price DECIMAL(10,2),\n    discount_amount DECIMAL(10,2),\n    tax_amount DECIMAL(10,2),\n    total_amount DECIMAL(10,2),\n    cost_amount DECIMAL(10,2),\n    profit_amount DECIMAL(10,2),\n    \n    -- Degenerate dimensions\n    order_number VARCHAR(50),\n    invoice_number VARCHAR(50),\n    \n    created_at TIMESTAMP\n);\n\n-- Date Dimension (very important for analytics)\nCREATE TABLE dim_date (\n    date_key INT PRIMARY KEY,\n    date DATE,\n    day_of_week VARCHAR(10),\n    day_of_month INT,\n    day_of_year INT,\n    week_of_year INT,\n    month_number INT,\n    month_name VARCHAR(10),\n    quar",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 35
    },
    "id": "ae_ae_overview_353"
  },
  {
    "text": " (\n    date_key INT PRIMARY KEY,\n    date DATE,\n    day_of_week VARCHAR(10),\n    day_of_month INT,\n    day_of_year INT,\n    week_of_year INT,\n    month_number INT,\n    month_name VARCHAR(10),\n    quarter INT,\n    year INT,\n    is_weekend BOOLEAN,\n    is_holiday BOOLEAN,\n    holiday_name VARCHAR(100),\n    fiscal_year INT,\n    fiscal_quarter INT,\n    fiscal_month INT\n);\n```\n\n## Data Governance and Documentation\n\n### Data Catalog and Lineage\n**My Documentation Approach:**\n```markdown\n# Data Model: Customer Analytics\n\n## Purpose\nProvides a comprehensive view of customer behavior, segmentation, and lifetime value for marketing and sales teams.\n\n## Source Systems\n- Salesforce CRM (customer data)\n- Shopify (order data)\n- Google Analytics (web behavior)\n\n## Refresh Schedule\n- Staging: Every 1 hour\n- Intermediate: Every 2 hours\n- Marts: Daily at 6 AM EST\n\n## Data Lineage\n```\nraw.salesforce.accounts \n  → staging.stg_customers \n    → intermediate.int_customer_orders \n      → marts.dim_customers\n`",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 36
    },
    "id": "ae_ae_overview_354"
  },
  {
    "text": "\n- Intermediate: Every 2 hours\n- Marts: Daily at 6 AM EST\n\n## Data Lineage\n```\nraw.salesforce.accounts \n  → staging.stg_customers \n    → intermediate.int_customer_orders \n      → marts.dim_customers\n```\n\n## Key Metrics\n| Metric | Definition | Formula |\n|--------|-----------|---------|\n| Customer Lifetime Value | Total revenue from customer | SUM(order_amount) |\n| Average Order Value | Average amount per order | SUM(order_amount) / COUNT(orders) |\n| Purchase Frequency | Orders per customer per year | COUNT(orders) / customer_tenure_years |\n\n## Data Quality Rules\n- customer_id must be unique and not null\n- email must be valid format\n- total_spent must be >= 0\n- first_order_date must be <= last_order_date\n\n## Access Control\n- Finance team: Full access\n- Marketing team: Read access (excluding PII)\n- Sales team: Read access (own region only)\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- dbt development and testing\n- SQL optimization and performance tuning\n- Dimensional modeling ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 37
    },
    "id": "ae_ae_overview_355"
  },
  {
    "text": "PII)\n- Sales team: Read access (own region only)\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- dbt development and testing\n- SQL optimization and performance tuning\n- Dimensional modeling (star/snowflake schemas)\n- Data quality and validation\n- Metrics definition and semantic modeling\n- Version control (Git) for data transformations\n\n### Tools & Technologies:\n- **Transformation**: dbt, SQL, Python\n- **Data Warehouses**: Snowflake, BigQuery, Redshift\n- **Version Control**: Git, GitHub, GitLab\n- **Orchestration**: Airflow, Dagster, Prefect\n- **BI Tools**: Power BI, Tableau, Looker\n- **Data Quality**: Great Expectations, dbt tests\n\n### Achievements:\n- Built 100+ dbt models serving 50+ analysts\n- Reduced data transformation time from 6 hours to 1 hour\n- Implemented data quality tests catching 95% of issues\n- Created reusable metrics layer used across 20+ dashboards\n- Documented 200+ data models improving data discovery\n- Trained 15+ analysts on dbt and SQL best practices\n\n### P",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 38
    },
    "id": "ae_ae_overview_356"
  },
  {
    "text": "ts catching 95% of issues\n- Created reusable metrics layer used across 20+ dashboards\n- Documented 200+ data models improving data discovery\n- Trained 15+ analysts on dbt and SQL best practices\n\n### Project Examples:\n- Customer 360 data mart (unified customer view)\n- Financial reporting data warehouse (P&L, balance sheet)\n- Marketing analytics platform (campaign performance, attribution)\n- Product analytics (usage, engagement, retention)\n- Operational metrics (efficiency, productivity, quality)\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_overview.md",
      "file_name": "ae_overview.md",
      "chunk_index": 39
    },
    "id": "ae_ae_overview_357"
  },
  {
    "text": "---\ntags: [analytics-engineer, data-modeling, dimensional-modeling, star-schema, snowflake-schema, kimball]\npersona: ae\n---\n\n# Data Modeling for Analytics Engineers - Tejuu's Approach\n\n## Introduction\n**Tejuu's Philosophy:**\nAs an Analytics Engineer, I believe good data modeling is the foundation of reliable analytics. My approach combines business understanding with technical design - I always start by understanding what decisions stakeholders need to make, then design models that make those decisions easy.\n\n## Dimensional Modeling Fundamentals\n\n### Star Schema Design\n**Tejuu's Experience:**\nStar schemas are my go-to for most business analytics. They're simple for business users to understand and perform well in modern data warehouses. I've built dozens of star schemas across finance, sales, marketing, and operations.\n\n**Example: Sales Analytics Star Schema**\n```\nFact Table: fct_sales\n- sale_id (PK)\n- date_key (FK)\n- customer_key (FK)\n- product_key (FK)\n- store_key (FK)\n- sales_amount",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 40
    },
    "id": "ae_ae_data_modeling_358"
  },
  {
    "text": "es, marketing, and operations.\n\n**Example: Sales Analytics Star Schema**\n```\nFact Table: fct_sales\n- sale_id (PK)\n- date_key (FK)\n- customer_key (FK)\n- product_key (FK)\n- store_key (FK)\n- sales_amount\n- quantity\n- discount_amount\n- net_amount\n- cost_amount\n- profit_amount\n\nDimension Tables:\n- dim_date (date_key, date, month, quarter, year, day_of_week, is_holiday)\n- dim_customer (customer_key, customer_id, name, segment, region, acquisition_date)\n- dim_product (product_key, product_id, name, category, subcategory, brand, unit_cost)\n- dim_store (store_key, store_id, name, city, state, region, size, type)\n```\n\n**Business Benefits I Emphasize:**\n- Executives can easily understand \"What are my sales by region?\"\n- Analysts can slice and dice by any dimension without complex joins\n- BI tools like Power BI and Tableau work great with star schemas\n- Query performance is predictable and fast\n\n### Slowly Changing Dimensions (SCD)\n\n**Tejuu's Real-World Scenario:**\nLet me share a challenge I faced",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 41
    },
    "id": "ae_ae_data_modeling_359"
  },
  {
    "text": "e Power BI and Tableau work great with star schemas\n- Query performance is predictable and fast\n\n### Slowly Changing Dimensions (SCD)\n\n**Tejuu's Real-World Scenario:**\nLet me share a challenge I faced: tracking customer segments over time. Customers move from \"Silver\" to \"Gold\" tier, but we needed to report on both historical and current segments.\n\n**SCD Type 2 Implementation:**\n```sql\n-- dim_customer with SCD Type 2\nCREATE TABLE dim_customer (\n    customer_key BIGINT PRIMARY KEY,        -- Surrogate key\n    customer_id VARCHAR(50),                -- Natural key\n    customer_name VARCHAR(200),\n    customer_segment VARCHAR(50),           -- Changes over time\n    region VARCHAR(50),\n    \n    -- SCD Type 2 fields\n    effective_date DATE,\n    end_date DATE,\n    is_current BOOLEAN,\n    \n    -- Audit fields\n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n);\n\n-- When segment changes, we insert a new row\nINSERT INTO dim_customer VALUES\n(1001, 'CUST001', 'John Smith', 'Silver', 'West', '2023",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 42
    },
    "id": "ae_ae_data_modeling_360"
  },
  {
    "text": "Audit fields\n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n);\n\n-- When segment changes, we insert a new row\nINSERT INTO dim_customer VALUES\n(1001, 'CUST001', 'John Smith', 'Silver', 'West', '2023-01-01', '2023-06-30', FALSE, NOW(), NOW()),\n(1002, 'CUST001', 'John Smith', 'Gold', 'West', '2023-07-01', '9999-12-31', TRUE, NOW(), NOW());\n```\n\n**Business Value:**\n- Marketing can analyze \"What campaigns worked to upgrade customers from Silver to Gold?\"\n- Finance can report \"Revenue by customer segment as it was in Q1 2023\"\n- Stakeholders get accurate historical analysis\n\n### Fact Table Design Patterns\n\n**Tejuu's Fact Table Types:**\n\n**1. Transaction Fact Table**\n```sql\n-- fct_orders: One row per order\nCREATE TABLE fct_orders (\n    order_key BIGINT PRIMARY KEY,\n    order_id VARCHAR(50),\n    date_key INT,\n    customer_key BIGINT,\n    product_key BIGINT,\n    \n    -- Additive measures (can sum across all dimensions)\n    order_amount DECIMAL(18,2),\n    quantity INT,\n    discount_amount DECI",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 43
    },
    "id": "ae_ae_data_modeling_361"
  },
  {
    "text": " date_key INT,\n    customer_key BIGINT,\n    product_key BIGINT,\n    \n    -- Additive measures (can sum across all dimensions)\n    order_amount DECIMAL(18,2),\n    quantity INT,\n    discount_amount DECIMAL(18,2),\n    tax_amount DECIMAL(18,2),\n    \n    -- Semi-additive measures (can sum across some dimensions)\n    inventory_level INT,  -- Additive across products, not across time\n    \n    -- Non-additive measures (cannot sum)\n    unit_price DECIMAL(18,2),  -- Average or calculated, not summed\n    discount_percentage DECIMAL(5,2)\n);\n```\n\n**2. Periodic Snapshot Fact Table**\n```sql\n-- fct_daily_inventory: One row per product per day\nCREATE TABLE fct_daily_inventory (\n    snapshot_key BIGINT PRIMARY KEY,\n    date_key INT,\n    product_key BIGINT,\n    warehouse_key BIGINT,\n    \n    -- Point-in-time measures\n    beginning_inventory INT,\n    ending_inventory INT,\n    units_received INT,\n    units_shipped INT,\n    units_returned INT,\n    \n    -- Calculated measures\n    inventory_value DECIMAL(18,2",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 44
    },
    "id": "ae_ae_data_modeling_362"
  },
  {
    "text": " measures\n    beginning_inventory INT,\n    ending_inventory INT,\n    units_received INT,\n    units_shipped INT,\n    units_returned INT,\n    \n    -- Calculated measures\n    inventory_value DECIMAL(18,2),\n    days_of_supply DECIMAL(10,2)\n);\n```\n\n**3. Accumulating Snapshot Fact Table**\n```sql\n-- fct_order_fulfillment: One row per order, updated as it progresses\nCREATE TABLE fct_order_fulfillment (\n    order_key BIGINT PRIMARY KEY,\n    order_id VARCHAR(50),\n    customer_key BIGINT,\n    \n    -- Multiple date foreign keys for different stages\n    order_date_key INT,\n    payment_date_key INT,\n    shipped_date_key INT,\n    delivered_date_key INT,\n    \n    -- Elapsed time measures\n    order_to_payment_hours INT,\n    payment_to_ship_hours INT,\n    ship_to_delivery_hours INT,\n    total_fulfillment_hours INT,\n    \n    -- Amounts\n    order_amount DECIMAL(18,2),\n    shipping_cost DECIMAL(18,2)\n);\n```\n\n**Business Scenario:**\nOperations wanted to track order fulfillment performance. Using an accumulat",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 45
    },
    "id": "ae_ae_data_modeling_363"
  },
  {
    "text": "urs INT,\n    \n    -- Amounts\n    order_amount DECIMAL(18,2),\n    shipping_cost DECIMAL(18,2)\n);\n```\n\n**Business Scenario:**\nOperations wanted to track order fulfillment performance. Using an accumulating snapshot, we could answer:\n- \"What's our average time from order to delivery?\"\n- \"Where are the bottlenecks in fulfillment?\"\n- \"Which orders are stuck in payment processing?\"\n\n## Conformed Dimensions\n\n**Tejuu's Cross-Functional Design:**\nOne of my biggest wins was creating conformed dimensions that work across multiple business functions.\n\n**Example: Conformed Date Dimension**\n```sql\n-- dim_date: Used by Sales, Finance, Marketing, Operations\nCREATE TABLE dim_date (\n    date_key INT PRIMARY KEY,\n    date DATE,\n    \n    -- Calendar attributes\n    day_of_week VARCHAR(10),\n    day_name VARCHAR(10),\n    day_of_month INT,\n    day_of_year INT,\n    \n    -- Week attributes\n    week_of_year INT,\n    iso_week INT,\n    week_start_date DATE,\n    week_end_date DATE,\n    \n    -- Month attributes\n    ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 46
    },
    "id": "ae_ae_data_modeling_364"
  },
  {
    "text": "(10),\n    day_of_month INT,\n    day_of_year INT,\n    \n    -- Week attributes\n    week_of_year INT,\n    iso_week INT,\n    week_start_date DATE,\n    week_end_date DATE,\n    \n    -- Month attributes\n    month_number INT,\n    month_name VARCHAR(10),\n    month_abbr VARCHAR(3),\n    month_start_date DATE,\n    month_end_date DATE,\n    \n    -- Quarter attributes\n    quarter_number INT,\n    quarter_name VARCHAR(10),\n    quarter_start_date DATE,\n    quarter_end_date DATE,\n    \n    -- Year attributes\n    year INT,\n    fiscal_year INT,\n    fiscal_quarter INT,\n    fiscal_month INT,\n    \n    -- Business attributes\n    is_weekend BOOLEAN,\n    is_holiday BOOLEAN,\n    holiday_name VARCHAR(100),\n    is_business_day BOOLEAN,\n    \n    -- Comparison attributes\n    prior_year_date DATE,\n    prior_year_date_key INT\n);\n```\n\n**Business Benefits:**\n- Finance reports fiscal quarters consistently\n- Marketing aligns campaigns with calendar and fiscal periods\n- Operations excludes weekends/holidays from SLA calculat",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 47
    },
    "id": "ae_ae_data_modeling_365"
  },
  {
    "text": "NT\n);\n```\n\n**Business Benefits:**\n- Finance reports fiscal quarters consistently\n- Marketing aligns campaigns with calendar and fiscal periods\n- Operations excludes weekends/holidays from SLA calculations\n- Everyone speaks the same language about time\n\n## Data Mart Design\n\n**Tejuu's Approach to Marts:**\nI organize data marts by business function. Each mart has its own fact and dimension tables, optimized for that team's specific questions.\n\n**Finance Mart Example:**\n```sql\n-- Finance wants: Revenue, Profit, Budget vs Actual\n\n-- fct_revenue_daily\nCREATE TABLE finance.fct_revenue_daily (\n    date_key INT,\n    customer_key BIGINT,\n    product_key BIGINT,\n    region_key BIGINT,\n    \n    -- Actual metrics\n    gross_revenue DECIMAL(18,2),\n    discounts DECIMAL(18,2),\n    returns DECIMAL(18,2),\n    net_revenue DECIMAL(18,2),\n    cost_of_goods DECIMAL(18,2),\n    gross_profit DECIMAL(18,2),\n    \n    -- Budget metrics\n    budgeted_revenue DECIMAL(18,2),\n    revenue_variance DECIMAL(18,2),\n    re",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 48
    },
    "id": "ae_ae_data_modeling_366"
  },
  {
    "text": "   net_revenue DECIMAL(18,2),\n    cost_of_goods DECIMAL(18,2),\n    gross_profit DECIMAL(18,2),\n    \n    -- Budget metrics\n    budgeted_revenue DECIMAL(18,2),\n    revenue_variance DECIMAL(18,2),\n    revenue_variance_pct DECIMAL(5,2)\n);\n\n-- dim_account_hierarchy\nCREATE TABLE finance.dim_account_hierarchy (\n    account_key BIGINT PRIMARY KEY,\n    account_code VARCHAR(50),\n    account_name VARCHAR(200),\n    account_type VARCHAR(50),\n    \n    -- Hierarchy\n    level_1_name VARCHAR(100),\n    level_2_name VARCHAR(100),\n    level_3_name VARCHAR(100),\n    level_4_name VARCHAR(100),\n    \n    -- Business attributes\n    is_active BOOLEAN,\n    requires_approval BOOLEAN\n);\n```\n\n**Marketing Mart Example:**\n```sql\n-- Marketing wants: Campaign Performance, Customer Acquisition, LTV\n\n-- fct_campaign_performance\nCREATE TABLE marketing.fct_campaign_performance (\n    date_key INT,\n    campaign_key BIGINT,\n    channel_key BIGINT,\n    segment_key BIGINT,\n    \n    -- Metrics\n    impressions INT,\n    clicks INT",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 49
    },
    "id": "ae_ae_data_modeling_367"
  },
  {
    "text": "nce\nCREATE TABLE marketing.fct_campaign_performance (\n    date_key INT,\n    campaign_key BIGINT,\n    channel_key BIGINT,\n    segment_key BIGINT,\n    \n    -- Metrics\n    impressions INT,\n    clicks INT,\n    conversions INT,\n    spend_amount DECIMAL(18,2),\n    revenue_generated DECIMAL(18,2),\n    \n    -- Calculated metrics\n    click_through_rate DECIMAL(5,4),\n    conversion_rate DECIMAL(5,4),\n    cost_per_click DECIMAL(10,2),\n    cost_per_acquisition DECIMAL(10,2),\n    return_on_ad_spend DECIMAL(10,2)\n);\n\n-- dim_campaign\nCREATE TABLE marketing.dim_campaign (\n    campaign_key BIGINT PRIMARY KEY,\n    campaign_id VARCHAR(50),\n    campaign_name VARCHAR(200),\n    campaign_type VARCHAR(50),\n    \n    -- Campaign attributes\n    start_date DATE,\n    end_date DATE,\n    target_segment VARCHAR(100),\n    objective VARCHAR(100),\n    budget_amount DECIMAL(18,2)\n);\n```\n\n## Metric Layer\n\n**Tejuu's Metric Definitions:**\nOne of my key responsibilities is defining metrics consistently across the organizatio",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 50
    },
    "id": "ae_ae_data_modeling_368"
  },
  {
    "text": " objective VARCHAR(100),\n    budget_amount DECIMAL(18,2)\n);\n```\n\n## Metric Layer\n\n**Tejuu's Metric Definitions:**\nOne of my key responsibilities is defining metrics consistently across the organization.\n\n**Example: Customer Metrics**\n```yaml\n# metrics.yml - I use YAML to document metrics\n\nmetrics:\n  - name: customer_lifetime_value\n    description: Total revenue from a customer over their lifetime\n    calculation: SUM(fct_sales.net_amount) WHERE customer_key = X\n    owner: Marketing\n    business_definition: |\n      Expected total revenue from a customer from first purchase to churn.\n      Used for customer acquisition cost justification and segment analysis.\n    \n  - name: customer_acquisition_cost\n    description: Cost to acquire a new customer\n    calculation: marketing_spend / new_customers_acquired\n    owner: Marketing\n    business_definition: |\n      Total marketing and sales spend divided by number of new customers.\n      Target is to keep CAC below 30% of LTV.\n    \n  - name: mont",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 51
    },
    "id": "ae_ae_data_modeling_369"
  },
  {
    "text": "ers_acquired\n    owner: Marketing\n    business_definition: |\n      Total marketing and sales spend divided by number of new customers.\n      Target is to keep CAC below 30% of LTV.\n    \n  - name: monthly_recurring_revenue\n    description: Predictable revenue per month\n    calculation: SUM(subscription_amount) WHERE status = 'active'\n    owner: Finance\n    business_definition: |\n      Monthly revenue from all active subscriptions.\n      Key metric for SaaS business health and forecasting.\n```\n\n**Business Value:**\n- No more \"Why do Finance and Marketing have different revenue numbers?\"\n- New analysts can look up metric definitions\n- BI dashboards use consistent calculations\n- Stakeholders trust the numbers\n\n## Data Quality in Models\n\n**Tejuu's Testing Strategy:**\nI implement tests at every layer to catch data quality issues before stakeholders see them.\n\n**dbt Tests Example:**\n```yaml\n# models/marts/finance/fct_revenue_daily.yml\n\nversion: 2\n\nmodels:\n  - name: fct_revenue_daily\n    descri",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 52
    },
    "id": "ae_ae_data_modeling_370"
  },
  {
    "text": "ayer to catch data quality issues before stakeholders see them.\n\n**dbt Tests Example:**\n```yaml\n# models/marts/finance/fct_revenue_daily.yml\n\nversion: 2\n\nmodels:\n  - name: fct_revenue_daily\n    description: Daily revenue fact table\n    \n    tests:\n      - dbt_utils.recency:\n          datepart: day\n          field: date_key\n          interval: 1\n          \n    columns:\n      - name: date_key\n        tests:\n          - not_null\n          - relationships:\n              to: ref('dim_date')\n              field: date_key\n              \n      - name: customer_key\n        tests:\n          - not_null\n          - relationships:\n              to: ref('dim_customer')\n              field: customer_key\n              \n      - name: net_revenue\n        tests:\n          - not_null\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 0\n              max_value: 1000000\n              \n      - name: gross_profit\n        tests:\n          - not_null\n          \n      - nam",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 53
    },
    "id": "ae_ae_data_modeling_371"
  },
  {
    "text": "ons.expect_column_values_to_be_between:\n              min_value: 0\n              max_value: 1000000\n              \n      - name: gross_profit\n        tests:\n          - not_null\n          \n      - name: revenue_variance_pct\n        tests:\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: -100\n              max_value: 100\n```\n\n**Custom Tests:**\n```sql\n-- tests/assert_revenue_matches_source.sql\n-- Make sure our aggregated revenue matches the source system\n\nWITH source_revenue AS (\n    SELECT\n        DATE(order_date) AS order_date,\n        SUM(order_amount) AS total_revenue\n    FROM {{ source('raw', 'orders') }}\n    WHERE order_status = 'completed'\n    GROUP BY 1\n),\n\ndbt_revenue AS (\n    SELECT\n        d.date,\n        SUM(f.net_revenue) AS total_revenue\n    FROM {{ ref('fct_revenue_daily') }} f\n    JOIN {{ ref('dim_date') }} d ON f.date_key = d.date_key\n    GROUP BY 1\n)\n\nSELECT\n    s.order_date,\n    s.total_revenue AS source_total,\n    d.total_reven",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 54
    },
    "id": "ae_ae_data_modeling_372"
  },
  {
    "text": "\n    FROM {{ ref('fct_revenue_daily') }} f\n    JOIN {{ ref('dim_date') }} d ON f.date_key = d.date_key\n    GROUP BY 1\n)\n\nSELECT\n    s.order_date,\n    s.total_revenue AS source_total,\n    d.total_revenue AS dbt_total,\n    ABS(s.total_revenue - d.total_revenue) AS difference\nFROM source_revenue s\nLEFT JOIN dbt_revenue d ON s.order_date = d.date\nWHERE ABS(s.total_revenue - d.total_revenue) > 0.01\n```\n\n**Business Impact:**\nFinance CFO asked: \"Can I trust these numbers for the board meeting?\"\nMy answer: \"Yes - we have 50+ automated tests running daily, and I get alerted immediately if anything fails.\"\n\n## Performance Optimization\n\n**Tejuu's Optimization Strategies:**\n\n**1. Materialization Strategy**\n```sql\n-- Staging: Views (lightweight, always fresh)\n{{ config(materialized='view') }}\n\n-- Intermediate: Ephemeral or Views (no storage)\n{{ config(materialized='ephemeral') }}\n\n-- Marts: Tables or Incremental (fast queries)\n{{ config(materialized='table') }}\n\n-- Large Facts: Incremental (only pr",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 55
    },
    "id": "ae_ae_data_modeling_373"
  },
  {
    "text": "mediate: Ephemeral or Views (no storage)\n{{ config(materialized='ephemeral') }}\n\n-- Marts: Tables or Incremental (fast queries)\n{{ config(materialized='table') }}\n\n-- Large Facts: Incremental (only process new data)\n{{ config(\n    materialized='incremental',\n    unique_key='order_id',\n    on_schema_change='fail'\n) }}\n```\n\n**2. Partitioning and Clustering**\n```sql\n-- Snowflake example\n{{ config(\n    materialized='incremental',\n    unique_key='order_id',\n    cluster_by=['order_date', 'customer_id']\n) }}\n\n-- BigQuery example\n{{ config(\n    materialized='incremental',\n    unique_key='order_id',\n    partition_by={\n        'field': 'order_date',\n        'data_type': 'date',\n        'granularity': 'day'\n    },\n    cluster_by=['customer_id', 'product_id']\n) }}\n```\n\n**Business Scenario:**\nDashboard was taking 2 minutes to load. After adding clustering by date and customer, it dropped to 5 seconds. Product manager was thrilled!\n\n## Documentation\n\n**Tejuu's Documentation Approach:**\nI treat docum",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 56
    },
    "id": "ae_ae_data_modeling_374"
  },
  {
    "text": " was taking 2 minutes to load. After adding clustering by date and customer, it dropped to 5 seconds. Product manager was thrilled!\n\n## Documentation\n\n**Tejuu's Documentation Approach:**\nI treat documentation as code - it lives with the models and is version controlled.\n\n**Example:**\n```yaml\n# models/marts/finance/fct_revenue_daily.yml\n\nversion: 2\n\nmodels:\n  - name: fct_revenue_daily\n    description: |\n      ## Purpose\n      Daily revenue fact table for financial reporting and analysis.\n      \n      ## Business Logic\n      - Revenue is recognized on order completion date\n      - Returns are subtracted from gross revenue\n      - Budget data comes from finance.budget_monthly table\n      \n      ## Refresh Schedule\n      - Runs daily at 6 AM UTC\n      - Data is available for previous day by 7 AM\n      \n      ## Data Quality\n      - Revenue reconciles to source within $100\n      - All dates have corresponding budget records\n      - No negative net revenue allowed\n      \n      ## Key Stakeho",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 57
    },
    "id": "ae_ae_data_modeling_375"
  },
  {
    "text": "AM\n      \n      ## Data Quality\n      - Revenue reconciles to source within $100\n      - All dates have corresponding budget records\n      - No negative net revenue allowed\n      \n      ## Key Stakeholders\n      - Owner: Finance team\n      - Primary users: CFO, FP&A team, Regional managers\n      - SLA: 99.5% uptime\n      \n    columns:\n      - name: date_key\n        description: |\n          Foreign key to dim_date.\n          Use this to filter by any date attribute (month, quarter, fiscal period).\n          \n      - name: net_revenue\n        description: |\n          **Business Definition:** Gross revenue minus discounts and returns.\n          **Formula:** gross_revenue - discounts - returns\n          **Example:** $1000 order with $50 discount and $20 return = $930 net revenue\n          **Use for:** Revenue reporting, variance analysis, forecasting\n```\n\nThis detailed documentation helps new team members, auditors, and future me understand the models!\n\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 58
    },
    "id": "ae_ae_data_modeling_376"
  },
  {
    "text": "e for:** Revenue reporting, variance analysis, forecasting\n```\n\nThis detailed documentation helps new team members, auditors, and future me understand the models!\n\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_data_modeling.md",
      "file_name": "ae_data_modeling.md",
      "chunk_index": 59
    },
    "id": "ae_ae_data_modeling_377"
  },
  {
    "text": "---\ntags: [analytics-engineer, azure, azure-data-factory, synapse, databricks, azure-sql]\npersona: ae\n---\n\n# Azure Cloud for Analytics Engineers - Tejuu's Experience\n\n## Introduction\n**Tejuu's Azure Journey:**\nI've been working with Azure data services for 2+ years, primarily focusing on analytics workloads. My role involves orchestrating data pipelines, building transformations in Synapse and Databricks, and ensuring our analytics stack runs smoothly in the cloud.\n\n## Azure Data Factory (ADF)\n\n**Tejuu's ADF Use Cases:**\nI use ADF to orchestrate my dbt transformations, copy data from various sources, and trigger downstream processes.\n\n**Example: Daily dbt Orchestration Pipeline**\n```json\n{\n  \"name\": \"pl_daily_analytics_refresh\",\n  \"description\": \"Orchestrate daily analytics refresh with dbt\",\n  \"activities\": [\n    {\n      \"name\": \"Check Data Freshness\",\n      \"type\": \"Lookup\",\n      \"inputs\": [{\n        \"referenceName\": \"ds_synapse_control_table\",\n        \"type\": \"DatasetReference\"\n   ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 60
    },
    "id": "ae_ae_azure_cloud_378"
  },
  {
    "text": "t\",\n  \"activities\": [\n    {\n      \"name\": \"Check Data Freshness\",\n      \"type\": \"Lookup\",\n      \"inputs\": [{\n        \"referenceName\": \"ds_synapse_control_table\",\n        \"type\": \"DatasetReference\"\n      }],\n      \"outputs\": [],\n      \"typeProperties\": {\n        \"source\": {\n          \"type\": \"SqlDWSource\",\n          \"sqlReaderQuery\": \"SELECT MAX(updated_at) as last_update FROM raw.orders\"\n        }\n      }\n    },\n    {\n      \"name\": \"Run dbt Models\",\n      \"type\": \"DatabricksNotebook\",\n      \"dependsOn\": [{\"activity\": \"Check Data Freshness\"}],\n      \"typeProperties\": {\n        \"notebookPath\": \"/Shared/dbt/run_dbt_production\",\n        \"baseParameters\": {\n          \"dbt_command\": \"dbt run --select tag:daily\",\n          \"target\": \"prod\"\n        }\n      }\n    },\n    {\n      \"name\": \"Run dbt Tests\",\n      \"type\": \"DatabricksNotebook\",\n      \"dependsOn\": [{\"activity\": \"Run dbt Models\"}],\n      \"typeProperties\": {\n        \"notebookPath\": \"/Shared/dbt/run_dbt_tests\",\n        \"baseParameters\": {",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 61
    },
    "id": "ae_ae_azure_cloud_379"
  },
  {
    "text": "ts\",\n      \"type\": \"DatabricksNotebook\",\n      \"dependsOn\": [{\"activity\": \"Run dbt Models\"}],\n      \"typeProperties\": {\n        \"notebookPath\": \"/Shared/dbt/run_dbt_tests\",\n        \"baseParameters\": {\n          \"dbt_command\": \"dbt test --select tag:daily\"\n        }\n      }\n    },\n    {\n      \"name\": \"Send Success Notification\",\n      \"type\": \"WebActivity\",\n      \"dependsOn\": [{\"activity\": \"Run dbt Tests\"}],\n      \"typeProperties\": {\n        \"url\": \"https://hooks.slack.com/services/YOUR/WEBHOOK/URL\",\n        \"method\": \"POST\",\n        \"body\": {\n          \"text\": \"✅ Daily analytics refresh completed successfully\"\n        }\n      }\n    }\n  ],\n  \"parameters\": {\n    \"execution_date\": {\n      \"type\": \"String\",\n      \"defaultValue\": \"@formatDateTime(utcnow(), 'yyyy-MM-dd')\"\n    }\n  }\n}\n```\n\n**Business Impact:**\n- Marketing gets fresh campaign metrics by 7 AM daily\n- Finance dashboards auto-refresh overnight\n- 99.5% pipeline success rate with automated alerting\n\n**Copy Activity for Incremental ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 62
    },
    "id": "ae_ae_azure_cloud_380"
  },
  {
    "text": "ness Impact:**\n- Marketing gets fresh campaign metrics by 7 AM daily\n- Finance dashboards auto-refresh overnight\n- 99.5% pipeline success rate with automated alerting\n\n**Copy Activity for Incremental Loads:**\n```json\n{\n  \"name\": \"copy_salesforce_data\",\n  \"type\": \"Copy\",\n  \"inputs\": [{\n    \"referenceName\": \"ds_salesforce_opportunities\",\n    \"type\": \"DatasetReference\"\n  }],\n  \"outputs\": [{\n    \"referenceName\": \"ds_adls_raw_salesforce\",\n    \"type\": \"DatasetReference\"\n  }],\n  \"typeProperties\": {\n    \"source\": {\n      \"type\": \"SalesforceSource\",\n      \"query\": \"SELECT Id, AccountId, Amount, CloseDate, StageName, LastModifiedDate FROM Opportunity WHERE LastModifiedDate > @{formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-ddTHH:mm:ssZ')}\"\n    },\n    \"sink\": {\n      \"type\": \"ParquetSink\",\n      \"storeSettings\": {\n        \"type\": \"AzureBlobFSWriteSettings\",\n        \"copyBehavior\": \"PreserveHierarchy\"\n      },\n      \"formatSettings\": {\n        \"type\": \"ParquetWriteSettings\",\n        \"fileExtension",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 63
    },
    "id": "ae_ae_azure_cloud_381"
  },
  {
    "text": "reSettings\": {\n        \"type\": \"AzureBlobFSWriteSettings\",\n        \"copyBehavior\": \"PreserveHierarchy\"\n      },\n      \"formatSettings\": {\n        \"type\": \"ParquetWriteSettings\",\n        \"fileExtension\": \".parquet\"\n      }\n    }\n  }\n}\n```\n\n## Azure Synapse Analytics\n\n**Tejuu's Synapse Setup:**\nWe use Synapse as our enterprise data warehouse. I create and maintain the serving layer where business users query data.\n\n**Creating Analytics Tables:**\n```sql\n-- Create fact table with distribution and indexing\n\nCREATE TABLE analytics.fct_sales\n(\n    sale_id VARCHAR(50) NOT NULL,\n    date_key INT NOT NULL,\n    customer_key BIGINT NOT NULL,\n    product_key BIGINT NOT NULL,\n    store_key BIGINT NOT NULL,\n    \n    sales_amount DECIMAL(18,2),\n    quantity INT,\n    discount_amount DECIMAL(18,2),\n    net_amount DECIMAL(18,2),\n    cost_amount DECIMAL(18,2),\n    profit_amount DECIMAL(18,2),\n    \n    created_at DATETIME2,\n    updated_at DATETIME2\n)\nWITH\n(\n    DISTRIBUTION = HASH(customer_key),  -- Distri",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 64
    },
    "id": "ae_ae_azure_cloud_382"
  },
  {
    "text": "amount DECIMAL(18,2),\n    cost_amount DECIMAL(18,2),\n    profit_amount DECIMAL(18,2),\n    \n    created_at DATETIME2,\n    updated_at DATETIME2\n)\nWITH\n(\n    DISTRIBUTION = HASH(customer_key),  -- Distribute by commonly joined column\n    CLUSTERED COLUMNSTORE INDEX,        -- Best for analytics\n    PARTITION (date_key RANGE RIGHT FOR VALUES (20230101, 20230201, 20230301))\n);\n\n-- Create dimension table\n\nCREATE TABLE analytics.dim_customer\n(\n    customer_key BIGINT NOT NULL,\n    customer_id VARCHAR(50) NOT NULL,\n    customer_name VARCHAR(200),\n    customer_segment VARCHAR(50),\n    customer_tier VARCHAR(20),\n    region VARCHAR(100),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    \n    -- SCD Type 2 fields\n    effective_date DATE,\n    end_date DATE,\n    is_current BIT,\n    \n    created_at DATETIME2,\n    updated_at DATETIME2\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,  -- Small dimension tables - replicate to all nodes\n    CLUSTERED COLUMNSTORE INDEX\n);\n```\n\n**Performance Optimization:**\n```sql\n-",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 65
    },
    "id": "ae_ae_azure_cloud_383"
  },
  {
    "text": "IME2,\n    updated_at DATETIME2\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,  -- Small dimension tables - replicate to all nodes\n    CLUSTERED COLUMNSTORE INDEX\n);\n```\n\n**Performance Optimization:**\n```sql\n-- Update statistics for query optimization\nCREATE STATISTICS stat_fct_sales_customer_key ON analytics.fct_sales(customer_key);\nCREATE STATISTICS stat_fct_sales_date_key ON analytics.fct_sales(date_key);\nCREATE STATISTICS stat_fct_sales_amount ON analytics.fct_sales(sales_amount);\n\n-- Create materialized view for common aggregations\nCREATE MATERIALIZED VIEW analytics.mv_daily_sales_summary\nWITH (DISTRIBUTION = HASH(date_key))\nAS\nSELECT\n    date_key,\n    customer_segment,\n    product_category,\n    region,\n    COUNT(DISTINCT sale_id) AS order_count,\n    SUM(sales_amount) AS total_sales,\n    SUM(quantity) AS total_quantity,\n    SUM(profit_amount) AS total_profit\nFROM analytics.fct_sales f\nJOIN analytics.dim_customer c ON f.customer_key = c.customer_key AND c.is_current = 1\nJOIN analytics.dim_p",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 66
    },
    "id": "ae_ae_azure_cloud_384"
  },
  {
    "text": "uantity) AS total_quantity,\n    SUM(profit_amount) AS total_profit\nFROM analytics.fct_sales f\nJOIN analytics.dim_customer c ON f.customer_key = c.customer_key AND c.is_current = 1\nJOIN analytics.dim_product p ON f.product_key = p.product_key\nJOIN analytics.dim_store s ON f.store_key = s.store_key\nGROUP BY\n    date_key,\n    customer_segment,\n    product_category,\n    region;\n```\n\n**Business Value:**\nDashboard queries that took 30 seconds now return in 2 seconds. Product managers love the speed improvement!\n\n**Incremental Load Pattern:**\n```sql\n-- Merge pattern for incremental updates\n\nMERGE INTO analytics.fct_sales AS target\nUSING staging.stg_sales_incremental AS source\nON target.sale_id = source.sale_id\n\nWHEN MATCHED AND source.updated_at > target.updated_at THEN\n    UPDATE SET\n        sales_amount = source.sales_amount,\n        quantity = source.quantity,\n        discount_amount = source.discount_amount,\n        net_amount = source.net_amount,\n        updated_at = source.updated_at\n\nW",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 67
    },
    "id": "ae_ae_azure_cloud_385"
  },
  {
    "text": "es_amount = source.sales_amount,\n        quantity = source.quantity,\n        discount_amount = source.discount_amount,\n        net_amount = source.net_amount,\n        updated_at = source.updated_at\n\nWHEN NOT MATCHED THEN\n    INSERT (\n        sale_id, date_key, customer_key, product_key, store_key,\n        sales_amount, quantity, discount_amount, net_amount,\n        created_at, updated_at\n    )\n    VALUES (\n        source.sale_id, source.date_key, source.customer_key, source.product_key, source.store_key,\n        source.sales_amount, source.quantity, source.discount_amount, source.net_amount,\n        source.created_at, source.updated_at\n    );\n```\n\n## Azure Databricks\n\n**Tejuu's Databricks Workflows:**\nI run dbt transformations in Databricks for heavy-duty data processing.\n\n**dbt Integration with Databricks:**\n```python\n# Databricks Notebook: run_dbt_production.py\n\nimport os\nimport subprocess\nfrom datetime import datetime\n\n# Setup\ndbt_project_path = \"/Workspace/Shared/dbt_project\"\nos.ch",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 68
    },
    "id": "ae_ae_azure_cloud_386"
  },
  {
    "text": "n with Databricks:**\n```python\n# Databricks Notebook: run_dbt_production.py\n\nimport os\nimport subprocess\nfrom datetime import datetime\n\n# Setup\ndbt_project_path = \"/Workspace/Shared/dbt_project\"\nos.chdir(dbt_project_path)\n\n# Get parameters\ndbt_command = dbutils.widgets.get(\"dbt_command\")  # e.g., \"dbt run --select tag:daily\"\ntarget = dbutils.widgets.get(\"target\")  # e.g., \"prod\"\n\nprint(f\"Running dbt command: {dbt_command}\")\nprint(f\"Target environment: {target}\")\n\n# Run dbt\ntry:\n    result = subprocess.run(\n        dbt_command.split(),\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    \n    print(\"✅ dbt command succeeded\")\n    print(result.stdout)\n    \n    # Log success to control table\n    spark.sql(f\"\"\"\n        INSERT INTO control.dbt_run_log\n        VALUES (\n            '{datetime.now()}',\n            '{dbt_command}',\n            'SUCCESS',\n            '{result.stdout[:1000]}'  -- Truncate long output\n        )\n    \"\"\")\n    \nexcept subprocess.CalledProcessEr",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 69
    },
    "id": "ae_ae_azure_cloud_387"
  },
  {
    "text": "         '{datetime.now()}',\n            '{dbt_command}',\n            'SUCCESS',\n            '{result.stdout[:1000]}'  -- Truncate long output\n        )\n    \"\"\")\n    \nexcept subprocess.CalledProcessError as e:\n    print(\"❌ dbt command failed\")\n    print(e.stderr)\n    \n    # Log failure\n    spark.sql(f\"\"\"\n        INSERT INTO control.dbt_run_log\n        VALUES (\n            '{datetime.now()}',\n            '{dbt_command}',\n            'FAILED',\n            '{e.stderr[:1000]}'\n        )\n    \"\"\")\n    \n    # Send alert\n    dbutils.notebook.exit(\"FAILED\")\n    raise\n```\n\n**Delta Lake for Analytics:**\n```python\n# Create Delta table optimized for analytics\n\nfrom delta.tables import DeltaTable\nfrom pyspark.sql import functions as F\n\n# Read source data\ndf_sales = spark.read.format(\"delta\").load(\"/mnt/raw/sales\")\n\n# Transform and aggregate\ndf_daily_summary = (\n    df_sales\n    .filter(F.col(\"order_status\") == \"completed\")\n    .groupBy(\n        F.to_date(\"order_date\").alias(\"date\"),\n        \"custome",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 70
    },
    "id": "ae_ae_azure_cloud_388"
  },
  {
    "text": "raw/sales\")\n\n# Transform and aggregate\ndf_daily_summary = (\n    df_sales\n    .filter(F.col(\"order_status\") == \"completed\")\n    .groupBy(\n        F.to_date(\"order_date\").alias(\"date\"),\n        \"customer_segment\",\n        \"product_category\"\n    )\n    .agg(\n        F.count(\"order_id\").alias(\"order_count\"),\n        F.sum(\"order_amount\").alias(\"total_sales\"),\n        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n    )\n)\n\n# Write to Delta with partitioning\n(\n    df_daily_summary.write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .partitionBy(\"date\")\n    .option(\"overwriteSchema\", \"true\")\n    .save(\"/mnt/analytics/fct_daily_sales_summary\")\n)\n\n# Optimize Delta table\nspark.sql(\"\"\"\n    OPTIMIZE delta.`/mnt/analytics/fct_daily_sales_summary`\n    ZORDER BY (customer_segment, product_category)\n\"\"\")\n```\n\n**Business Scenario:**\nWe process 100M+ rows of clickstream data daily. Using Databricks with Delta Lake, processing completes i",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 71
    },
    "id": "ae_ae_azure_cloud_389"
  },
  {
    "text": "s_summary`\n    ZORDER BY (customer_segment, product_category)\n\"\"\")\n```\n\n**Business Scenario:**\nWe process 100M+ rows of clickstream data daily. Using Databricks with Delta Lake, processing completes in 15 minutes vs 2 hours with traditional methods.\n\n## Azure Data Lake Storage (ADLS)\n\n**Tejuu's ADLS Organization:**\n```\nadls://analyticsdata@company.dfs.core.windows.net/\n\n├── raw/                          # Landing zone\n│   ├── salesforce/\n│   │   └── 2024/01/15/opportunities.parquet\n│   ├── erp/\n│   │   └── 2024/01/15/orders.parquet\n│   └── web_analytics/\n│       └── 2024/01/15/clickstream.json\n│\n├── staging/                      # Cleaned data\n│   ├── stg_opportunities/\n│   ├── stg_orders/\n│   └── stg_clickstream/\n│\n├── analytics/                    # Business logic applied\n│   ├── fct_sales/\n│   ├── fct_campaigns/\n│   ├── dim_customer/\n│   └── dim_product/\n│\n└── reporting/                    # Aggregated for BI\n    ├── daily_revenue_summary/\n    ├── customer_ltv/\n    └── product_perfo",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 72
    },
    "id": "ae_ae_azure_cloud_390"
  },
  {
    "text": "es/\n│   ├── fct_campaigns/\n│   ├── dim_customer/\n│   └── dim_product/\n│\n└── reporting/                    # Aggregated for BI\n    ├── daily_revenue_summary/\n    ├── customer_ltv/\n    └── product_performance/\n```\n\n**Lifecycle Management:**\n```python\n# Python script to manage ADLS lifecycle\n\nfrom azure.storage.filedatalake import DataLakeServiceClient\n\n# Move old raw files to archive\ndef archive_old_files(days_old=90):\n    \"\"\"Move files older than 90 days to archive tier\"\"\"\n    \n    service_client = DataLakeServiceClient.from_connection_string(conn_str)\n    file_system_client = service_client.get_file_system_client(\"analyticsdata\")\n    \n    paths = file_system_client.get_paths(path=\"raw\")\n    \n    for path in paths:\n        if is_older_than(path.last_modified, days_old):\n            # Move to cool tier or archive\n            properties = {'tier': 'Archive'}\n            path.set_access_control_recursive(properties)\n            print(f\"Archived: {path.name}\")\n```\n\n## Azure Key Vault Integr",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 73
    },
    "id": "ae_ae_azure_cloud_391"
  },
  {
    "text": "o cool tier or archive\n            properties = {'tier': 'Archive'}\n            path.set_access_control_recursive(properties)\n            print(f\"Archived: {path.name}\")\n```\n\n## Azure Key Vault Integration\n\n**Tejuu's Secret Management:**\n```python\n# dbt profile with Azure Key Vault\n\n# profiles.yml\nmy_project:\n  target: prod\n  outputs:\n    prod:\n      type: databricks\n      host: \"{{ env_var('DBX_HOST') }}\"\n      http_path: \"{{ env_var('DBX_HTTP_PATH') }}\"\n      token: \"{{ env_var('DBX_TOKEN') }}\"  # Retrieved from Key Vault\n      schema: analytics\n      threads: 4\n\n# ADF pipeline retrieves secrets\n{\n  \"activities\": [\n    {\n      \"name\": \"Get DB Password\",\n      \"type\": \"WebActivity\",\n      \"typeProperties\": {\n        \"url\": \"https://my-keyvault.vault.azure.net/secrets/db-password?api-version=7.0\",\n        \"method\": \"GET\",\n        \"authentication\": {\n          \"type\": \"MSI\",\n          \"resource\": \"https://vault.azure.net\"\n        }\n      }\n    }\n  ]\n}\n```\n\n## Cost Optimization\n\n**Tejuu'",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 74
    },
    "id": "ae_ae_azure_cloud_392"
  },
  {
    "text": "on=7.0\",\n        \"method\": \"GET\",\n        \"authentication\": {\n          \"type\": \"MSI\",\n          \"resource\": \"https://vault.azure.net\"\n        }\n      }\n    }\n  ]\n}\n```\n\n## Cost Optimization\n\n**Tejuu's Cost Management:**\n```sql\n-- Query to find expensive tables in Synapse\n\nSELECT\n    t.name AS table_name,\n    SUM(p.rows) AS row_count,\n    SUM(a.used_pages) * 8 / 1024 / 1024 AS size_gb,\n    t.distribution_policy_desc,\n    COUNT(DISTINCT p.partition_number) AS partition_count\nFROM sys.tables t\nJOIN sys.indexes i ON t.object_id = i.object_id\nJOIN sys.partitions p ON i.object_id = p.object_id AND i.index_id = p.index_id\nJOIN sys.allocation_units a ON p.partition_id = a.container_id\nWHERE t.schema_id = SCHEMA_ID('analytics')\nGROUP BY t.name, t.distribution_policy_desc\nORDER BY size_gb DESC;\n```\n\n**Databricks Cluster Optimization:**\n```json\n{\n  \"cluster_name\": \"dbt-analytics-cluster\",\n  \"spark_version\": \"13.3.x-scala2.12\",\n  \"node_type_id\": \"Standard_DS3_v2\",\n  \"autoscale\": {\n    \"min_worker",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 75
    },
    "id": "ae_ae_azure_cloud_393"
  },
  {
    "text": "\n\n**Databricks Cluster Optimization:**\n```json\n{\n  \"cluster_name\": \"dbt-analytics-cluster\",\n  \"spark_version\": \"13.3.x-scala2.12\",\n  \"node_type_id\": \"Standard_DS3_v2\",\n  \"autoscale\": {\n    \"min_workers\": 2,\n    \"max_workers\": 8\n  },\n  \"autotermination_minutes\": 30,\n  \"spark_conf\": {\n    \"spark.databricks.delta.preview.enabled\": \"true\",\n    \"spark.sql.adaptive.enabled\": \"true\",\n    \"spark.databricks.cluster.profile\": \"singleNode\"\n  }\n}\n```\n\n**Business Impact:**\nReduced Azure costs by 30% by:\n- Auto-terminating idle Databricks clusters\n- Using materialized views in Synapse\n- Implementing data lifecycle policies\n- Scheduling pipelines during off-peak hours\n\n## Monitoring and Alerting\n\n**Tejuu's Monitoring Setup:**\n```python\n# Log Analytics query for pipeline monitoring\n\n# KQL query in Azure Monitor\nAzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.DATAFACTORY\"\n| where Category == \"PipelineRuns\"\n| where Status == \"Failed\"\n| where TimeGenerated > ago(24h)\n| extend PipelineName = tostr",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 76
    },
    "id": "ae_ae_azure_cloud_394"
  },
  {
    "text": "onitor\nAzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.DATAFACTORY\"\n| where Category == \"PipelineRuns\"\n| where Status == \"Failed\"\n| where TimeGenerated > ago(24h)\n| extend PipelineName = tostring(split(resource_name, '/')[2])\n| project\n    TimeGenerated,\n    PipelineName,\n    Status,\n    ErrorMessage = ActivityError\n| summarize FailureCount = count() by PipelineName\n| where FailureCount > 0\n```\n\n**Alerting Logic:**\n```yaml\n# Alert rule for failed dbt runs\n\nalert_rule:\n  name: \"dbt_daily_run_failed\"\n  description: \"Alert when daily dbt run fails\"\n  severity: \"High\"\n  condition:\n    query: |\n      AzureDiagnostics\n      | where ActivityName contains \"Run dbt Models\"\n      | where Status == \"Failed\"\n      | where TimeGenerated > ago(30m)\n  action:\n    action_group: \"analytics-team-alerts\"\n    email: [\"tejuu@company.com\", \"data-team@company.com\"]\n    slack_webhook: \"https://hooks.slack.com/services/YOUR/WEBHOOK\"\n```\n\nMy focus in Azure is making analytics reliable, performant, and c",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 77
    },
    "id": "ae_ae_azure_cloud_395"
  },
  {
    "text": "rts\"\n    email: [\"tejuu@company.com\", \"data-team@company.com\"]\n    slack_webhook: \"https://hooks.slack.com/services/YOUR/WEBHOOK\"\n```\n\nMy focus in Azure is making analytics reliable, performant, and cost-effective while empowering business users!\n\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_azure_cloud.md",
      "file_name": "ae_azure_cloud.md",
      "chunk_index": 78
    },
    "id": "ae_ae_azure_cloud_396"
  },
  {
    "text": "---\ntags: [analytics-engineer, aws, redshift, glue, s3, lambda, athena]\npersona: ae\n---\n\n# AWS Cloud for Analytics Engineers - Tejuu's Experience\n\n## Introduction\n**Tejuu's AWS Background:**\nI've worked with AWS data services on several projects, building scalable analytics solutions using Redshift, Glue, S3, and Athena. My focus is always on creating business-friendly data models that stakeholders can trust.\n\n## Amazon Redshift\n\n**Tejuu's Redshift Architecture:**\nRedshift is our data warehouse where business users run analytics queries. I design and maintain the serving layer.\n\n**Creating Analytics Tables:**\n```sql\n-- Fact table with distribution and sort keys\n\nCREATE TABLE analytics.fct_orders\n(\n    order_id VARCHAR(50) NOT NULL,\n    order_date DATE NOT NULL SORTKEY,  -- Frequent filter column\n    customer_id VARCHAR(50) NOT NULL DISTKEY,  -- Join column\n    product_id VARCHAR(50) NOT NULL,\n    \n    order_amount DECIMAL(18,2),\n    quantity INTEGER,\n    discount_amount DECIMAL(18,2),\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 79
    },
    "id": "ae_ae_aws_cloud_397"
  },
  {
    "text": "column\n    customer_id VARCHAR(50) NOT NULL DISTKEY,  -- Join column\n    product_id VARCHAR(50) NOT NULL,\n    \n    order_amount DECIMAL(18,2),\n    quantity INTEGER,\n    discount_amount DECIMAL(18,2),\n    tax_amount DECIMAL(18,2),\n    net_amount DECIMAL(18,2),\n    \n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n)\nDISTSTYLE KEY;\n\n-- Dimension table with ALL distribution\n\nCREATE TABLE analytics.dim_customer\n(\n    customer_id VARCHAR(50) NOT NULL SORTKEY,\n    customer_name VARCHAR(200),\n    customer_email VARCHAR(200),\n    customer_segment VARCHAR(50),\n    customer_tier VARCHAR(20),\n    \n    -- Address\n    street VARCHAR(200),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    zip_code VARCHAR(20),\n    country VARCHAR(50),\n    \n    -- SCD Type 2\n    effective_date DATE,\n    expiration_date DATE,\n    is_current BOOLEAN,\n    \n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n)\nDISTSTYLE ALL;  -- Small table replicated to all nodes\n```\n\n**Business Scenario:**\nMarketing wanted customer lif",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 80
    },
    "id": "ae_ae_aws_cloud_398"
  },
  {
    "text": "DATE,\n    is_current BOOLEAN,\n    \n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n)\nDISTSTYLE ALL;  -- Small table replicated to all nodes\n```\n\n**Business Scenario:**\nMarketing wanted customer lifetime value by segment. This star schema made it simple:\n\n```sql\n-- Customer LTV by Segment - runs in 3 seconds\n\nSELECT\n    c.customer_segment,\n    COUNT(DISTINCT c.customer_id) AS customer_count,\n    SUM(f.net_amount) AS total_revenue,\n    AVG(f.net_amount) AS avg_order_value,\n    SUM(f.net_amount) / NULLIF(COUNT(DISTINCT c.customer_id), 0) AS ltv_per_customer\nFROM analytics.fct_orders f\nJOIN analytics.dim_customer c \n    ON f.customer_id = c.customer_id \n    AND c.is_current = TRUE\nWHERE f.order_date >= DATEADD(year, -2, CURRENT_DATE)\nGROUP BY c.customer_segment\nORDER BY ltv_per_customer DESC;\n```\n\n**Incremental Load Pattern:**\n```sql\n-- Upsert pattern using staging table\n\nBEGIN TRANSACTION;\n\n-- Step 1: Delete updated records from target\nDELETE FROM analytics.fct_orders\nUSING staging.stg",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 81
    },
    "id": "ae_ae_aws_cloud_399"
  },
  {
    "text": "C;\n```\n\n**Incremental Load Pattern:**\n```sql\n-- Upsert pattern using staging table\n\nBEGIN TRANSACTION;\n\n-- Step 1: Delete updated records from target\nDELETE FROM analytics.fct_orders\nUSING staging.stg_orders_incremental\nWHERE analytics.fct_orders.order_id = staging.stg_orders_incremental.order_id;\n\n-- Step 2: Insert all records from staging\nINSERT INTO analytics.fct_orders\nSELECT * FROM staging.stg_orders_incremental;\n\n-- Step 3: Commit\nEND TRANSACTION;\n\n-- Vacuum to reclaim space\nVACUUM analytics.fct_orders;\n\n-- Analyze for query optimizer\nANALYZE analytics.fct_orders;\n```\n\n**Redshift Materialized Views:**\n```sql\n-- Create materialized view for common aggregation\n\nCREATE MATERIALIZED VIEW analytics.mv_daily_revenue_summary\nSORTKEY(order_date)\nAS\nSELECT\n    d.order_date,\n    d.year,\n    d.quarter,\n    d.month,\n    c.customer_segment,\n    p.product_category,\n    \n    COUNT(DISTINCT f.order_id) AS order_count,\n    COUNT(DISTINCT f.customer_id) AS unique_customers,\n    SUM(f.order_amount)",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 82
    },
    "id": "ae_ae_aws_cloud_400"
  },
  {
    "text": "quarter,\n    d.month,\n    c.customer_segment,\n    p.product_category,\n    \n    COUNT(DISTINCT f.order_id) AS order_count,\n    COUNT(DISTINCT f.customer_id) AS unique_customers,\n    SUM(f.order_amount) AS gross_revenue,\n    SUM(f.discount_amount) AS total_discounts,\n    SUM(f.net_amount) AS net_revenue,\n    AVG(f.net_amount) AS avg_order_value\nFROM analytics.fct_orders f\nJOIN analytics.dim_date d ON f.order_date = d.order_date\nJOIN analytics.dim_customer c ON f.customer_id = c.customer_id AND c.is_current = TRUE\nJOIN analytics.dim_product p ON f.product_id = p.product_id\nGROUP BY 1, 2, 3, 4, 5, 6;\n\n-- Refresh materialized view (scheduled daily)\nREFRESH MATERIALIZED VIEW analytics.mv_daily_revenue_summary;\n```\n\n**Business Value:**\nCFO's daily revenue dashboard loads in 2 seconds instead of 45 seconds!\n\n## AWS Glue\n\n**Tejuu's Glue ETL Jobs:**\nI use Glue for data ingestion and transformation before loading into Redshift.\n\n**PySpark Job for Data Transformation:**\n```python\n# Glue Job: trans",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 83
    },
    "id": "ae_ae_aws_cloud_401"
  },
  {
    "text": "5 seconds!\n\n## AWS Glue\n\n**Tejuu's Glue ETL Jobs:**\nI use Glue for data ingestion and transformation before loading into Redshift.\n\n**PySpark Job for Data Transformation:**\n```python\n# Glue Job: transform_customer_data.py\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 'execution_date'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Read source data from S3\ndf_customers = spark.read.parquet(\"s3://my-bucket/raw/customers/\")\n\n# Transform data\ndf_transformed = (\n    df_customers\n    \n    # Clean and standardize\n    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\"))))\n    .withColumn(\"customer_name\", F.trim(F.col(\"custom",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 84
    },
    "id": "ae_ae_aws_cloud_402"
  },
  {
    "text": ")\n\n# Transform data\ndf_transformed = (\n    df_customers\n    \n    # Clean and standardize\n    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\"))))\n    .withColumn(\"customer_name\", F.trim(F.col(\"customer_name\")))\n    .withColumn(\"phone\", F.regexp_replace(\"phone\", \"[^0-9]\", \"\"))\n    \n    # Derive columns\n    .withColumn(\"full_address\", \n        F.concat_ws(\", \", \"street\", \"city\", \"state\", \"zip_code\"))\n    \n    # Calculate customer tenure\n    .withColumn(\"customer_tenure_days\",\n        F.datediff(F.current_date(), \"registration_date\"))\n    \n    # Assign customer segment based on business rules\n    .withColumn(\"customer_segment\",\n        F.when(F.col(\"lifetime_value\") > 10000, \"Platinum\")\n         .when(F.col(\"lifetime_value\") > 5000, \"Gold\")\n         .when(F.col(\"lifetime_value\") > 1000, \"Silver\")\n         .otherwise(\"Bronze\"))\n    \n    # Handle SCD Type 2\n    .withColumn(\"effective_date\", F.col(\"updated_date\"))\n    .withColumn(\"expiration_date\", F.lit(\"2099-12-31\").cast(\"date\"))\n    .wit",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 85
    },
    "id": "ae_ae_aws_cloud_403"
  },
  {
    "text": "ver\")\n         .otherwise(\"Bronze\"))\n    \n    # Handle SCD Type 2\n    .withColumn(\"effective_date\", F.col(\"updated_date\"))\n    .withColumn(\"expiration_date\", F.lit(\"2099-12-31\").cast(\"date\"))\n    .withColumn(\"is_current\", F.lit(True))\n)\n\n# Remove duplicates - keep latest record\nwindow = Window.partitionBy(\"customer_id\").orderBy(F.desc(\"updated_date\"))\ndf_deduped = (\n    df_transformed\n    .withColumn(\"row_num\", F.row_number().over(window))\n    .filter(F.col(\"row_num\") == 1)\n    .drop(\"row_num\")\n)\n\n# Write to S3 in staging area\n(\n    df_deduped.write\n    .mode(\"overwrite\")\n    .partitionBy(\"effective_date\")\n    .parquet(\"s3://my-bucket/staging/customers/\")\n)\n\n# Load into Redshift\nglueContext.write_dynamic_frame.from_options(\n    frame=DynamicFrame.fromDF(df_deduped, glueContext, \"df_deduped\"),\n    connection_type=\"redshift\",\n    connection_options={\n        \"url\": \"jdbc:redshift://my-cluster.redshift.amazonaws.com:5439/analytics\",\n        \"dbtable\": \"staging.stg_customers\",\n        \"use",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 86
    },
    "id": "ae_ae_aws_cloud_404"
  },
  {
    "text": "\"),\n    connection_type=\"redshift\",\n    connection_options={\n        \"url\": \"jdbc:redshift://my-cluster.redshift.amazonaws.com:5439/analytics\",\n        \"dbtable\": \"staging.stg_customers\",\n        \"user\": \"admin\",\n        \"password\": \"{{resolve:secretsmanager:redshift-password}}\",\n        \"redshiftTmpDir\": \"s3://my-bucket/temp/\"\n    }\n)\n\njob.commit()\n```\n\n**Glue Crawler for Schema Discovery:**\n```python\n# Create Glue Crawler using boto3\n\nimport boto3\n\nglue = boto3.client('glue')\n\nresponse = glue.create_crawler(\n    Name='crawler-raw-data',\n    Role='AWSGlueServiceRole',\n    DatabaseName='raw_data_catalog',\n    Description='Discover schema for raw data files',\n    Targets={\n        'S3Targets': [\n            {\n                'Path': 's3://my-bucket/raw/salesforce/',\n                'Exclusions': ['*.tmp', '*.log']\n            },\n            {\n                'Path': 's3://my-bucket/raw/google_analytics/'\n            }\n        ]\n    },\n    Schedule='cron(0 8 * * ? *)',  # Daily at 8 AM\n ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 87
    },
    "id": "ae_ae_aws_cloud_405"
  },
  {
    "text": "ons': ['*.tmp', '*.log']\n            },\n            {\n                'Path': 's3://my-bucket/raw/google_analytics/'\n            }\n        ]\n    },\n    Schedule='cron(0 8 * * ? *)',  # Daily at 8 AM\n    SchemaChangePolicy={\n        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n        'DeleteBehavior': 'DEPRECATE_IN_DATABASE'\n    },\n    RecrawlPolicy={\n        'RecrawlBehavior': 'CRAWL_NEW_FOLDERS_ONLY'\n    }\n)\n```\n\n**Business Impact:**\nAutomated schema discovery saves 5 hours/week of manual catalog updates!\n\n## Amazon S3\n\n**Tejuu's S3 Data Lake Structure:**\n```\ns3://my-analytics-bucket/\n\n├── raw/                          # Landing zone\n│   ├── salesforce/\n│   │   └── 2024-01-15/\n│   │       └── opportunities.json\n│   ├── database_extracts/\n│   │   └── 2024-01-15/\n│   │       └── orders.csv\n│   └── api_data/\n│       └── 2024-01-15/\n│           └── marketing_campaigns.json\n│\n├── staging/                      # Cleaned & transformed\n│   ├── customers/\n│   │   └── effective_date=2024-01-15/\n│  ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 88
    },
    "id": "ae_ae_aws_cloud_406"
  },
  {
    "text": "└── api_data/\n│       └── 2024-01-15/\n│           └── marketing_campaigns.json\n│\n├── staging/                      # Cleaned & transformed\n│   ├── customers/\n│   │   └── effective_date=2024-01-15/\n│   ├── orders/\n│   │   └── order_date=2024-01-15/\n│   └── products/\n│\n├── analytics/                    # Business logic\n│   ├── fct_sales/\n│   ├── fct_campaigns/\n│   ├── dim_customer/\n│   └── dim_product/\n│\n└── logs/                         # Pipeline logs\n    └── glue_jobs/\n        └── 2024-01-15/\n```\n\n**S3 Lifecycle Policies:**\n```json\n{\n  \"Rules\": [\n    {\n      \"Id\": \"archive-old-raw-data\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\n        \"Prefix\": \"raw/\"\n      },\n      \"Transitions\": [\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"INTELLIGENT_TIERING\"\n        },\n        {\n          \"Days\": 365,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    },\n    {\n      \"Id\": \"delete-staging-data\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\n        \"Prefix\": \"staging/\"\n  ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 89
    },
    "id": "ae_ae_aws_cloud_407"
  },
  {
    "text": "         \"Days\": 365,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    },\n    {\n      \"Id\": \"delete-staging-data\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\n        \"Prefix\": \"staging/\"\n      },\n      \"Expiration\": {\n        \"Days\": 30\n      }\n    }\n  ]\n}\n```\n\n**Business Value:**\nReduced S3 storage costs by 40% with lifecycle policies!\n\n## AWS Lambda\n\n**Tejuu's Lambda Use Cases:**\nI use Lambda for lightweight data processing and pipeline orchestration.\n\n**Lambda Function for Data Validation:**\n```python\n# lambda_validate_data.py\n\nimport json\nimport boto3\nimport pandas as pd\nfrom datetime import datetime\n\ns3 = boto3.client('s3')\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Validate incoming data files before processing\n    \"\"\"\n    \n    # Get file info from S3 event\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n    \n    print(f\"Validating file: s3://{bucket}/{key}\")\n    \n    # Read f",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 90
    },
    "id": "ae_ae_aws_cloud_408"
  },
  {
    "text": "fo from S3 event\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n    \n    print(f\"Validating file: s3://{bucket}/{key}\")\n    \n    # Read file\n    obj = s3.get_object(Bucket=bucket, Key=key)\n    df = pd.read_csv(obj['Body'])\n    \n    # Validation rules\n    errors = []\n    \n    # 1. Check required columns\n    required_cols = ['order_id', 'customer_id', 'order_date', 'order_amount']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        errors.append(f\"Missing required columns: {missing_cols}\")\n    \n    # 2. Check for nulls in key columns\n    null_counts = df[required_cols].isnull().sum()\n    if null_counts.any():\n        errors.append(f\"Null values found: {null_counts[null_counts > 0].to_dict()}\")\n    \n    # 3. Check date format\n    try:\n        pd.to_datetime(df['order_date'])\n    except:\n        errors.append(\"Invalid date format in order_date column\")\n    \n    # 4. Check nume",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 91
    },
    "id": "ae_ae_aws_cloud_409"
  },
  {
    "text": "0].to_dict()}\")\n    \n    # 3. Check date format\n    try:\n        pd.to_datetime(df['order_date'])\n    except:\n        errors.append(\"Invalid date format in order_date column\")\n    \n    # 4. Check numeric columns\n    if df['order_amount'].dtype not in ['int64', 'float64']:\n        errors.append(\"order_amount must be numeric\")\n    \n    # 5. Check for duplicates\n    dup_count = df['order_id'].duplicated().sum()\n    if dup_count > 0:\n        errors.append(f\"Found {dup_count} duplicate order_ids\")\n    \n    # Send results\n    if errors:\n        # Validation failed - send alert\n        message = f\"\"\"\n        ❌ Data validation FAILED\n        \n        File: s3://{bucket}/{key}\n        Timestamp: {datetime.now()}\n        \n        Errors:\n        {chr(10).join(errors)}\n        \"\"\"\n        \n        sns.publish(\n            TopicArn='arn:aws:sns:us-east-1:123456789:data-validation-alerts',\n            Subject='Data Validation Failed',\n            Message=message\n        )\n        \n        # Move fi",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 92
    },
    "id": "ae_ae_aws_cloud_410"
  },
  {
    "text": "s.publish(\n            TopicArn='arn:aws:sns:us-east-1:123456789:data-validation-alerts',\n            Subject='Data Validation Failed',\n            Message=message\n        )\n        \n        # Move file to error folder\n        error_key = key.replace('raw/', 'errors/')\n        s3.copy_object(\n            Bucket=bucket,\n            CopySource={'Bucket': bucket, 'Key': key},\n            Key=error_key\n        )\n        s3.delete_object(Bucket=bucket, Key=key)\n        \n        return {'statusCode': 400, 'body': json.dumps(errors)}\n    \n    else:\n        # Validation passed\n        print(f\"✅ Validation passed for {key}\")\n        \n        # Move file to validated folder\n        validated_key = key.replace('raw/', 'validated/')\n        s3.copy_object(\n            Bucket=bucket,\n            CopySource={'Bucket': bucket, 'Key': key},\n            Key=validated_key\n        )\n        s3.delete_object(Bucket=bucket, Key=key)\n        \n        return {'statusCode': 200, 'body': 'Validation successful",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 93
    },
    "id": "ae_ae_aws_cloud_411"
  },
  {
    "text": "urce={'Bucket': bucket, 'Key': key},\n            Key=validated_key\n        )\n        s3.delete_object(Bucket=bucket, Key=key)\n        \n        return {'statusCode': 200, 'body': 'Validation successful'}\n```\n\n**Business Impact:**\nCatches data quality issues before they hit production. Saves 10+ hours/month of firefighting!\n\n## Amazon Athena\n\n**Tejuu's Athena Queries:**\nI use Athena for ad-hoc analysis on S3 data lake.\n\n**Create External Table:**\n```sql\n-- Create external table on S3 data\n\nCREATE EXTERNAL TABLE IF NOT EXISTS raw.orders (\n    order_id STRING,\n    customer_id STRING,\n    order_date DATE,\n    order_amount DECIMAL(18,2),\n    product_id STRING,\n    quantity INT,\n    status STRING\n)\nPARTITIONED BY (order_year INT, order_month INT)\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/raw/orders/'\nTBLPROPERTIES ('parquet.compression'='SNAPPY');\n\n-- Add partitions\nMSCK REPAIR TABLE raw.orders;\n```\n\n**Complex Ana",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 94
    },
    "id": "ae_ae_aws_cloud_412"
  },
  {
    "text": "quet.serde.ParquetHiveSerDe'\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/raw/orders/'\nTBLPROPERTIES ('parquet.compression'='SNAPPY');\n\n-- Add partitions\nMSCK REPAIR TABLE raw.orders;\n```\n\n**Complex Analytics Query:**\n```sql\n-- Customer cohort analysis\n\nWITH first_purchase AS (\n    SELECT\n        customer_id,\n        MIN(order_date) AS cohort_date,\n        DATE_FORMAT(MIN(order_date), '%Y-%m') AS cohort_month\n    FROM raw.orders\n    WHERE status = 'completed'\n    GROUP BY customer_id\n),\n\nmonthly_orders AS (\n    SELECT\n        o.customer_id,\n        f.cohort_month,\n        DATE_FORMAT(o.order_date, '%Y-%m') AS order_month,\n        SUM(o.order_amount) AS revenue\n    FROM raw.orders o\n    JOIN first_purchase f ON o.customer_id = f.customer_id\n    WHERE o.status = 'completed'\n    GROUP BY 1, 2, 3\n),\n\ncohort_size AS (\n    SELECT\n        cohort_month,\n        COUNT(DISTINCT customer_id) AS cohort_size\n    FROM first_purchase\n    GROUP BY cohort_month\n)\n\nSELECT\n    m.cohort_month,\n    m.order_m",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 95
    },
    "id": "ae_ae_aws_cloud_413"
  },
  {
    "text": "3\n),\n\ncohort_size AS (\n    SELECT\n        cohort_month,\n        COUNT(DISTINCT customer_id) AS cohort_size\n    FROM first_purchase\n    GROUP BY cohort_month\n)\n\nSELECT\n    m.cohort_month,\n    m.order_month,\n    DATE_DIFF('month', \n        DATE_PARSE(m.cohort_month, '%Y-%m'),\n        DATE_PARSE(m.order_month, '%Y-%m')\n    ) AS months_since_first_purchase,\n    COUNT(DISTINCT m.customer_id) AS active_customers,\n    c.cohort_size,\n    CAST(COUNT(DISTINCT m.customer_id) AS DOUBLE) / c.cohort_size AS retention_rate,\n    SUM(m.revenue) AS total_revenue,\n    SUM(m.revenue) / COUNT(DISTINCT m.customer_id) AS revenue_per_customer\nFROM monthly_orders m\nJOIN cohort_size c ON m.cohort_month = c.cohort_month\nGROUP BY 1, 2, c.cohort_size\nORDER BY 1, 2;\n```\n\n**Business Use:**\nMarketing uses this cohort analysis to measure retention and optimize customer lifecycle campaigns.\n\n## AWS Step Functions\n\n**Tejuu's ETL Orchestration:**\n```json\n{\n  \"Comment\": \"Daily Analytics Refresh Pipeline\",\n  \"StartAt\": \"Va",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 96
    },
    "id": "ae_ae_aws_cloud_414"
  },
  {
    "text": "lysis to measure retention and optimize customer lifecycle campaigns.\n\n## AWS Step Functions\n\n**Tejuu's ETL Orchestration:**\n```json\n{\n  \"Comment\": \"Daily Analytics Refresh Pipeline\",\n  \"StartAt\": \"Validate Source Data\",\n  \"States\": {\n    \"Validate Source Data\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:validate-data\",\n      \"Next\": \"Run Glue ETL Job\"\n    },\n    \"Run Glue ETL Job\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\",\n      \"Parameters\": {\n        \"JobName\": \"transform-customer-data\",\n        \"Arguments\": {\n          \"--execution_date.$\": \"$.execution_date\"\n        }\n      },\n      \"Next\": \"Load to Redshift\"\n    },\n    \"Load to Redshift\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:load-redshift\",\n      \"Next\": \"Run dbt Transformations\"\n    },\n    \"Run dbt Transformations\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:fu",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 97
    },
    "id": "ae_ae_aws_cloud_415"
  },
  {
    "text": "east-1:123456789:function:load-redshift\",\n      \"Next\": \"Run dbt Transformations\"\n    },\n    \"Run dbt Transformations\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:run-dbt\",\n      \"Parameters\": {\n        \"dbt_command\": \"dbt run --select tag:daily\"\n      },\n      \"Next\": \"Refresh Materialized Views\"\n    },\n    \"Refresh Materialized Views\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:refresh-mv\",\n      \"Next\": \"Send Success Notification\",\n      \"Catch\": [{\n        \"ErrorEquals\": [\"States.ALL\"],\n        \"Next\": \"Send Failure Notification\"\n      }]\n    },\n    \"Send Success Notification\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::sns:publish\",\n      \"Parameters\": {\n        \"TopicArn\": \"arn:aws:sns:us-east-1:123456789:analytics-success\",\n        \"Message\": \"✅ Daily analytics refresh completed successfully\"\n      },\n      \"End\": true\n    },\n    \"Send Failure Notification\": {\n      \"Type\": \"",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 98
    },
    "id": "ae_ae_aws_cloud_416"
  },
  {
    "text": "s:sns:us-east-1:123456789:analytics-success\",\n        \"Message\": \"✅ Daily analytics refresh completed successfully\"\n      },\n      \"End\": true\n    },\n    \"Send Failure Notification\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::sns:publish\",\n      \"Parameters\": {\n        \"TopicArn\": \"arn:aws:sns:us-east-1:123456789:analytics-failure\",\n        \"Message.$\": \"$.Error\"\n      },\n      \"End\": true\n    }\n  }\n}\n```\n\n## Cost Optimization\n\n**Tejuu's AWS Cost Strategies:**\n\n**1. Redshift Reserved Capacity:**\n- Committed to 1-year reserved instances\n- Saved 40% on compute costs\n\n**2. S3 Intelligent Tiering:**\n- Automatically moves infrequently accessed data to cheaper tiers\n- Reduced S3 costs by 30%\n\n**3. Athena Query Optimization:**\n```sql\n-- Partition pruning saves money\n\n-- BAD: Scans entire table ($$$)\nSELECT *\nFROM raw.orders\nWHERE order_date = '2024-01-15';\n\n-- GOOD: Uses partitions ($$)\nSELECT *\nFROM raw.orders\nWHERE order_year = 2024\n  AND order_month = 1\n  AND order_date = ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 99
    },
    "id": "ae_ae_aws_cloud_417"
  },
  {
    "text": "s entire table ($$$)\nSELECT *\nFROM raw.orders\nWHERE order_date = '2024-01-15';\n\n-- GOOD: Uses partitions ($$)\nSELECT *\nFROM raw.orders\nWHERE order_year = 2024\n  AND order_month = 1\n  AND order_date = '2024-01-15';\n```\n\n**4. Glue Job Optimization:**\n- Use Glue Flex for non-urgent jobs (50% cheaper)\n- Enable job bookmarks to avoid reprocessing\n- Right-size DPUs based on data volume\n\nMy AWS expertise helps deliver fast, reliable, cost-effective analytics that drive business decisions!\n\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_aws_cloud.md",
      "file_name": "ae_aws_cloud.md",
      "chunk_index": 100
    },
    "id": "ae_ae_aws_cloud_418"
  },
  {
    "text": "---\ntags: [analytics-engineer, dbt, transformation, testing, macros, jinja, incremental-models]\npersona: ae\n---\n\n# Advanced dbt Techniques - Tejuu's Expertise\n\n## Introduction\n**Tejuu's dbt Journey:**\nI've been working with dbt for 3+ years, and it's completely transformed how I build analytics. What started as simple SQL transformations evolved into a sophisticated data transformation pipeline with testing, documentation, and CI/CD. Let me share the advanced techniques I use daily.\n\n## Incremental Models\n\n**Tejuu's Use Case:**\nOur sales fact table has 50+ million rows and grows by 500K daily. Rebuilding the entire table takes 2 hours. Using incremental models, we process only new data in 5 minutes.\n\n**Basic Incremental Model:**\n```sql\n-- models/marts/sales/fct_orders.sql\n\n{{\n    config(\n        materialized='incremental',\n        unique_key='order_id',\n        on_schema_change='fail'\n    )\n}}\n\nSELECT\n    order_id,\n    customer_id,\n    order_date,\n    order_amount,\n    created_at,\n    ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 101
    },
    "id": "ae_ae_dbt_advanced_419"
  },
  {
    "text": "       materialized='incremental',\n        unique_key='order_id',\n        on_schema_change='fail'\n    )\n}}\n\nSELECT\n    order_id,\n    customer_id,\n    order_date,\n    order_amount,\n    created_at,\n    updated_at\nFROM {{ source('raw', 'orders') }}\n\n{% if is_incremental() %}\n    -- Only process new or updated records\n    WHERE updated_at > (SELECT MAX(updated_at) FROM {{ this }})\n{% endif %}\n```\n\n**Advanced Incremental with Late-Arriving Data:**\n```sql\n-- models/marts/sales/fct_orders_late_arriving.sql\n\n{{\n    config(\n        materialized='incremental',\n        unique_key='order_id',\n        on_schema_change='sync_all_columns'\n    )\n}}\n\nWITH source_data AS (\n    SELECT\n        order_id,\n        customer_id,\n        order_date,\n        order_amount,\n        order_status,\n        updated_at\n    FROM {{ source('raw', 'orders') }}\n    \n    {% if is_incremental() %}\n        -- Look back 7 days to catch late-arriving updates\n        WHERE updated_at > (SELECT DATEADD(day, -7, MAX(updated_at)) F",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 102
    },
    "id": "ae_ae_dbt_advanced_420"
  },
  {
    "text": " FROM {{ source('raw', 'orders') }}\n    \n    {% if is_incremental() %}\n        -- Look back 7 days to catch late-arriving updates\n        WHERE updated_at > (SELECT DATEADD(day, -7, MAX(updated_at)) FROM {{ this }})\n    {% endif %}\n),\n\ndeduplicated AS (\n    -- Keep only the latest version of each order\n    SELECT *\n    FROM (\n        SELECT *,\n               ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY updated_at DESC) AS rn\n        FROM source_data\n    )\n    WHERE rn = 1\n)\n\nSELECT\n    order_id,\n    customer_id,\n    order_date,\n    order_amount,\n    order_status,\n    updated_at\nFROM deduplicated\n```\n\n**Business Impact:**\nFinance can see yesterday's revenue by 6 AM instead of 9 AM. That 3-hour difference means CFO gets numbers before his 7 AM standup with the CEO.\n\n## Snapshots (Slowly Changing Dimensions)\n\n**Tejuu's SCD Implementation:**\nTracking customer segments over time was crucial for marketing analytics.\n\n**Timestamp Strategy:**\n```sql\n-- snapshots/snap_customers.sql\n\n{% sna",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 103
    },
    "id": "ae_ae_dbt_advanced_421"
  },
  {
    "text": " Changing Dimensions)\n\n**Tejuu's SCD Implementation:**\nTracking customer segments over time was crucial for marketing analytics.\n\n**Timestamp Strategy:**\n```sql\n-- snapshots/snap_customers.sql\n\n{% snapshot snap_customers %}\n\n{{\n    config(\n      target_schema='snapshots',\n      unique_key='customer_id',\n      strategy='timestamp',\n      updated_at='updated_at',\n      invalidate_hard_deletes=True\n    )\n}}\n\nSELECT\n    customer_id,\n    customer_name,\n    customer_segment,    -- Changes from Bronze → Silver → Gold\n    customer_tier,\n    customer_status,\n    region,\n    updated_at\nFROM {{ source('raw', 'customers') }}\n\n{% endsnapshot %}\n```\n\n**Check Strategy (for sources without updated_at):**\n```sql\n-- snapshots/snap_product_prices.sql\n\n{% snapshot snap_product_prices %}\n\n{{\n    config(\n      target_schema='snapshots',\n      unique_key='product_id',\n      strategy='check',\n      check_cols=['unit_price', 'list_price']  -- Track price changes\n    )\n}}\n\nSELECT\n    product_id,\n    product_nam",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 104
    },
    "id": "ae_ae_dbt_advanced_422"
  },
  {
    "text": "target_schema='snapshots',\n      unique_key='product_id',\n      strategy='check',\n      check_cols=['unit_price', 'list_price']  -- Track price changes\n    )\n}}\n\nSELECT\n    product_id,\n    product_name,\n    unit_price,\n    list_price,\n    category,\n    brand\nFROM {{ source('raw', 'products') }}\n\n{% endsnapshot %}\n```\n\n**Using Snapshot in Analysis:**\n```sql\n-- How much revenue came from customers who were \"Gold\" tier in Q1 2023?\n\nWITH gold_customers_q1 AS (\n    SELECT DISTINCT customer_id\n    FROM {{ ref('snap_customers') }}\n    WHERE customer_segment = 'Gold'\n      AND dbt_valid_from <= '2023-03-31'\n      AND (dbt_valid_to > '2023-01-01' OR dbt_valid_to IS NULL)\n)\n\nSELECT\n    SUM(o.order_amount) AS total_revenue\nFROM {{ ref('fct_orders') }} o\nJOIN gold_customers_q1 g ON o.customer_id = g.customer_id\nWHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n```\n\n## Macros\n\n**Tejuu's Custom Macros:**\n\n**1. Generate Date Spine:**\n```sql\n-- macros/generate_date_spine.sql\n\n{% macro generate_",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 105
    },
    "id": "ae_ae_dbt_advanced_423"
  },
  {
    "text": "ustomer_id\nWHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n```\n\n## Macros\n\n**Tejuu's Custom Macros:**\n\n**1. Generate Date Spine:**\n```sql\n-- macros/generate_date_spine.sql\n\n{% macro generate_date_spine(start_date, end_date) %}\n\nWITH date_spine AS (\n    {{ dbt_utils.date_spine(\n        datepart=\"day\",\n        start_date=\"TO_DATE('\" ~ start_date ~ \"', 'YYYY-MM-DD')\",\n        end_date=\"TO_DATE('\" ~ end_date ~ \"', 'YYYY-MM-DD')\"\n    )}}\n)\n\nSELECT\n    date_day,\n    DAYOFWEEK(date_day) AS day_of_week,\n    DAYNAME(date_day) AS day_name,\n    WEEKOFYEAR(date_day) AS week_of_year,\n    MONTH(date_day) AS month_number,\n    MONTHNAME(date_day) AS month_name,\n    QUARTER(date_day) AS quarter,\n    YEAR(date_day) AS year,\n    CASE WHEN DAYOFWEEK(date_day) IN (6, 7) THEN TRUE ELSE FALSE END AS is_weekend\nFROM date_spine\n\n{% endmacro %}\n```\n\n**Usage:**\n```sql\n-- models/staging/stg_date_spine.sql\n\n{{ generate_date_spine('2020-01-01', '2025-12-31') }}\n```\n\n**2. Pivot Table Macro:**\n```sql\n-- macr",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 106
    },
    "id": "ae_ae_dbt_advanced_424"
  },
  {
    "text": "weekend\nFROM date_spine\n\n{% endmacro %}\n```\n\n**Usage:**\n```sql\n-- models/staging/stg_date_spine.sql\n\n{{ generate_date_spine('2020-01-01', '2025-12-31') }}\n```\n\n**2. Pivot Table Macro:**\n```sql\n-- macros/pivot_table.sql\n\n{% macro pivot_table(source_table, row_key, column_key, value_column, agg_function='SUM') %}\n\nWITH source_data AS (\n    SELECT * FROM {{ source_table }}\n),\n\npivoted AS (\n    SELECT\n        {{ row_key }},\n        {% for col_value in get_column_values(source_table, column_key) %}\n        {{ agg_function }}(\n            CASE WHEN {{ column_key }} = '{{ col_value }}' \n            THEN {{ value_column }} \n            ELSE 0 END\n        ) AS {{ col_value | replace(' ', '_') | lower }}\n        {% if not loop.last %},{% endif %}\n        {% endfor %}\n    FROM source_data\n    GROUP BY {{ row_key }}\n)\n\nSELECT * FROM pivoted\n\n{% endmacro %}\n```\n\n**Usage:**\n```sql\n-- models/marts/sales/revenue_by_product_category.sql\n\n{{ pivot_table(\n    source_table=ref('fct_sales'),\n    row_key='d",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 107
    },
    "id": "ae_ae_dbt_advanced_425"
  },
  {
    "text": "Y {{ row_key }}\n)\n\nSELECT * FROM pivoted\n\n{% endmacro %}\n```\n\n**Usage:**\n```sql\n-- models/marts/sales/revenue_by_product_category.sql\n\n{{ pivot_table(\n    source_table=ref('fct_sales'),\n    row_key='date_key',\n    column_key='product_category',\n    value_column='sales_amount',\n    agg_function='SUM'\n) }}\n```\n\n**3. Surrogate Key Generation:**\n```sql\n-- macros/generate_surrogate_key.sql\n\n{% macro generate_surrogate_key(columns) %}\n    MD5(CAST(CONCAT(\n        {% for col in columns %}\n        COALESCE(CAST({{ col }} AS VARCHAR), '')\n        {% if not loop.last %}, '|', {% endif %}\n        {% endfor %}\n    ) AS VARCHAR))\n{% endmacro %}\n```\n\n**Usage:**\n```sql\nSELECT\n    {{ generate_surrogate_key(['customer_id', 'order_date']) }} AS order_key,\n    customer_id,\n    order_date,\n    order_amount\nFROM {{ source('raw', 'orders') }}\n```\n\n## Tests\n\n**Tejuu's Testing Strategy:**\n\n**1. Generic Tests:**\n```yaml\n# models/marts/sales/fct_orders.yml\n\nversion: 2\n\nmodels:\n  - name: fct_orders\n    tests:\n  ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 108
    },
    "id": "ae_ae_dbt_advanced_426"
  },
  {
    "text": "OM {{ source('raw', 'orders') }}\n```\n\n## Tests\n\n**Tejuu's Testing Strategy:**\n\n**1. Generic Tests:**\n```yaml\n# models/marts/sales/fct_orders.yml\n\nversion: 2\n\nmodels:\n  - name: fct_orders\n    tests:\n      - dbt_utils.equal_rowcount:\n          compare_model: source('raw', 'orders')\n          \n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n          \n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('dim_customer')\n              field: customer_id\n              \n      - name: order_amount\n        tests:\n          - not_null\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 0\n              max_value: 1000000\n```\n\n**2. Singular Tests:**\n```sql\n-- tests/assert_daily_revenue_positive.sql\n\nSELECT\n    order_date,\n    SUM(order_amount) AS daily_revenue\nFROM {{ ref('fct_orders') }}\nGROUP BY order_date\nHAVING SUM(order_amount) < 0\n```\n\n**3. Custom Generic",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 109
    },
    "id": "ae_ae_dbt_advanced_427"
  },
  {
    "text": "/assert_daily_revenue_positive.sql\n\nSELECT\n    order_date,\n    SUM(order_amount) AS daily_revenue\nFROM {{ ref('fct_orders') }}\nGROUP BY order_date\nHAVING SUM(order_amount) < 0\n```\n\n**3. Custom Generic Test:**\n```sql\n-- macros/tests/test_revenue_reconciliation.sql\n\n{% test revenue_reconciliation(model, revenue_column, source_table, source_column, tolerance=100) %}\n\nWITH model_total AS (\n    SELECT SUM({{ revenue_column }}) AS model_revenue\n    FROM {{ model }}\n),\n\nsource_total AS (\n    SELECT SUM({{ source_column }}) AS source_revenue\n    FROM {{ source_table }}\n)\n\nSELECT\n    model_revenue,\n    source_revenue,\n    ABS(model_revenue - source_revenue) AS difference\nFROM model_total\nCROSS JOIN source_total\nWHERE ABS(model_revenue - source_revenue) > {{ tolerance }}\n\n{% endtest %}\n```\n\n**Usage:**\n```yaml\nmodels:\n  - name: fct_revenue_daily\n    tests:\n      - revenue_reconciliation:\n          revenue_column: net_revenue\n          source_table: source('raw', 'orders')\n          source_column:",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 110
    },
    "id": "ae_ae_dbt_advanced_428"
  },
  {
    "text": "**\n```yaml\nmodels:\n  - name: fct_revenue_daily\n    tests:\n      - revenue_reconciliation:\n          revenue_column: net_revenue\n          source_table: source('raw', 'orders')\n          source_column: order_amount\n          tolerance: 50\n```\n\n**Business Value:**\nOur CFO once asked: \"How do I know these numbers are right?\"\nI showed him our 100+ automated tests running daily. He was impressed and now trusts the data.\n\n## Jinja Templating\n\n**Tejuu's Advanced Jinja:**\n\n**1. Dynamic Column Generation:**\n```sql\n-- models/marts/sales/sales_metrics_monthly.sql\n\nWITH monthly_sales AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', order_date) AS order_month,\n        SUM(order_amount) AS total_sales\n    FROM {{ ref('fct_orders') }}\n    GROUP BY 1, 2\n)\n\nSELECT\n    customer_id,\n    {% for month_offset in range(0, 12) %}\n    SUM(CASE \n        WHEN order_month = DATEADD('month', -{{ month_offset }}, DATE_TRUNC('month', CURRENT_DATE))\n        THEN total_sales \n        ELSE 0 \n    END) A",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 111
    },
    "id": "ae_ae_dbt_advanced_429"
  },
  {
    "text": "r month_offset in range(0, 12) %}\n    SUM(CASE \n        WHEN order_month = DATEADD('month', -{{ month_offset }}, DATE_TRUNC('month', CURRENT_DATE))\n        THEN total_sales \n        ELSE 0 \n    END) AS sales_m{{ month_offset }}{% if not loop.last %},{% endif %}\n    {% endfor %}\nFROM monthly_sales\nGROUP BY customer_id\n```\n\n**2. Environment-Specific Logic:**\n```sql\n-- models/staging/stg_orders.sql\n\nSELECT\n    order_id,\n    customer_id,\n    order_date,\n    order_amount\nFROM {{ source('raw', 'orders') }}\n\n{% if target.name == 'dev' %}\n    -- In dev, only process last 30 days for speed\n    WHERE order_date >= DATEADD('day', -30, CURRENT_DATE)\n{% elif target.name == 'prod' %}\n    -- In prod, process all data\n    WHERE order_date >= '2020-01-01'\n{% endif %}\n```\n\n**3. Dynamic Schema Generation:**\n```sql\n-- macros/generate_schema_name.sql\n\n{% macro generate_schema_name(custom_schema_name, node) -%}\n    {%- set default_schema = target.schema -%}\n    \n    {%- if target.name == 'prod' -%}\n        ",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 112
    },
    "id": "ae_ae_dbt_advanced_430"
  },
  {
    "text": "```sql\n-- macros/generate_schema_name.sql\n\n{% macro generate_schema_name(custom_schema_name, node) -%}\n    {%- set default_schema = target.schema -%}\n    \n    {%- if target.name == 'prod' -%}\n        {%- if custom_schema_name is not none -%}\n            {{ custom_schema_name | trim }}\n        {%- else -%}\n            {{ default_schema }}\n        {%- endif -%}\n    {%- else -%}\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n    {%- endif -%}\n{%- endmacro %}\n```\n\n## Packages\n\n**Tejuu's Essential dbt Packages:**\n\n```yaml\n# packages.yml\n\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 1.1.1\n    \n  - package: calogica/dbt_expectations\n    version: 0.10.0\n    \n  - package: dbt-labs/metrics\n    version: 1.6.0\n    \n  - package: dbt-labs/codegen\n    version: 0.11.0\n    \n  - package: dbt-labs/audit_helper\n    version: 0.9.0\n```\n\n**Using dbt_utils:**\n```sql\n-- models/staging/stg_customers.sql\n\nWITH source AS (\n    SELECT * FROM {{ source('raw', 'customers') }}\n),\n\ndeduplicated",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 113
    },
    "id": "ae_ae_dbt_advanced_431"
  },
  {
    "text": "age: dbt-labs/audit_helper\n    version: 0.9.0\n```\n\n**Using dbt_utils:**\n```sql\n-- models/staging/stg_customers.sql\n\nWITH source AS (\n    SELECT * FROM {{ source('raw', 'customers') }}\n),\n\ndeduplicated AS (\n    SELECT *\n    FROM source\n    {{ dbt_utils.deduplicate(\n        partition_by='customer_id',\n        order_by='updated_at DESC'\n    ) }}\n)\n\nSELECT\n    {{ dbt_utils.generate_surrogate_key(['customer_id', 'email']) }} AS customer_key,\n    customer_id,\n    email,\n    first_name,\n    last_name,\n    {{ dbt_utils.get_url_parameter('utm_source', 'registration_url') }} AS acquisition_source,\n    created_at,\n    updated_at\nFROM deduplicated\n```\n\n**Using dbt_expectations:**\n```yaml\nmodels:\n  - name: fct_orders\n    tests:\n      - dbt_expectations.expect_table_row_count_to_be_between:\n          min_value: 1000\n          max_value: 10000000\n          \n      - dbt_expectations.expect_table_columns_to_match_ordered_list:\n          column_list: [\"order_id\", \"customer_id\", \"order_date\", \"order_amou",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 114
    },
    "id": "ae_ae_dbt_advanced_432"
  },
  {
    "text": "n_value: 1000\n          max_value: 10000000\n          \n      - dbt_expectations.expect_table_columns_to_match_ordered_list:\n          column_list: [\"order_id\", \"customer_id\", \"order_date\", \"order_amount\"]\n          \n    columns:\n      - name: order_date\n        tests:\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: \"'2020-01-01'\"\n              max_value: \"current_date\"\n```\n\n## Exposures\n\n**Tejuu's Exposure Tracking:**\nI track all downstream dashboards and reports that use my dbt models.\n\n```yaml\n# models/exposures.yml\n\nversion: 2\n\nexposures:\n  - name: executive_revenue_dashboard\n    type: dashboard\n    maturity: high\n    url: https://powerbi.com/reports/executive-revenue\n    description: |\n      Daily revenue dashboard used by CFO and executive team for morning standup.\n      Shows revenue by region, product, and customer segment with YoY comparisons.\n    \n    depends_on:\n      - ref('fct_revenue_daily')\n      - ref('dim_date')\n      - ref('dim",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 115
    },
    "id": "ae_ae_dbt_advanced_433"
  },
  {
    "text": "m for morning standup.\n      Shows revenue by region, product, and customer segment with YoY comparisons.\n    \n    depends_on:\n      - ref('fct_revenue_daily')\n      - ref('dim_date')\n      - ref('dim_customer')\n      - ref('dim_product')\n    \n    owner:\n      name: Tejuu\n      email: tejuu@company.com\n    \n  - name: marketing_campaign_report\n    type: dashboard\n    maturity: medium\n    url: https://tableau.com/workbooks/marketing-campaigns\n    description: |\n      Weekly marketing campaign performance report.\n      Used by CMO and marketing managers to optimize spend.\n    \n    depends_on:\n      - ref('fct_campaign_performance')\n      - ref('dim_campaign')\n      - ref('dim_channel')\n```\n\n**Business Value:**\nWhen I need to make breaking changes, I know exactly which dashboards will be affected and can notify the right stakeholders.\n\n## CI/CD Integration\n\n**Tejuu's dbt CI/CD Pipeline:**\n\n```yaml\n# .github/workflows/dbt_ci.yml\n\nname: dbt CI\n\non:\n  pull_request:\n    branches: [main]\n\njobs:",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 116
    },
    "id": "ae_ae_dbt_advanced_434"
  },
  {
    "text": "ted and can notify the right stakeholders.\n\n## CI/CD Integration\n\n**Tejuu's dbt CI/CD Pipeline:**\n\n```yaml\n# .github/workflows/dbt_ci.yml\n\nname: dbt CI\n\non:\n  pull_request:\n    branches: [main]\n\njobs:\n  dbt-test:\n    runs-on: ubuntu-latest\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n        \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          \n      - name: Install dbt\n        run: |\n          pip install dbt-snowflake==1.6.0\n          dbt deps\n          \n      - name: Run dbt models on sample data\n        run: |\n          dbt run --select state:modified+ --defer --state ./prod_manifest\n          \n      - name: Run dbt tests\n        run: |\n          dbt test --select state:modified+\n          \n      - name: Generate and upload docs\n        run: |\n          dbt docs generate\n          # Upload to S3 or cloud storage\n```\n\n**Business Impact:**\n- Pull requests automatically run tests be",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 117
    },
    "id": "ae_ae_dbt_advanced_435"
  },
  {
    "text": "       \n      - name: Generate and upload docs\n        run: |\n          dbt docs generate\n          # Upload to S3 or cloud storage\n```\n\n**Business Impact:**\n- Pull requests automatically run tests before merge\n- No more \"oops, I broke production\"\n- Code reviews include test results\n- Junior analysts can contribute confidently\n\n## Performance Optimization\n\n**Tejuu's Optimization Techniques:**\n\n**1. Query Performance:**\n```sql\n-- BEFORE: Slow query with multiple CTEs\n\nWITH raw_orders AS (\n    SELECT * FROM {{ source('raw', 'orders') }}\n),\n\nraw_customers AS (\n    SELECT * FROM {{ source('raw', 'customers') }}\n),\n\nraw_products AS (\n    SELECT * FROM {{ source('raw', 'products') }}\n)\n\nSELECT\n    o.order_id,\n    c.customer_name,\n    p.product_name,\n    o.order_amount\nFROM raw_orders o\nLEFT JOIN raw_customers c ON o.customer_id = c.customer_id\nLEFT JOIN raw_products p ON o.product_id = p.product_id\n\n\n-- AFTER: Optimized with pre-filtered CTEs\n\nWITH recent_orders AS (\n    SELECT\n        order",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 118
    },
    "id": "ae_ae_dbt_advanced_436"
  },
  {
    "text": " raw_customers c ON o.customer_id = c.customer_id\nLEFT JOIN raw_products p ON o.product_id = p.product_id\n\n\n-- AFTER: Optimized with pre-filtered CTEs\n\nWITH recent_orders AS (\n    SELECT\n        order_id,\n        customer_id,\n        product_id,\n        order_amount\n    FROM {{ source('raw', 'orders') }}\n    WHERE order_date >= DATEADD('month', -3, CURRENT_DATE)  -- Filter early\n),\n\nrelevant_customers AS (\n    SELECT\n        customer_id,\n        customer_name\n    FROM {{ source('raw', 'customers') }}\n    WHERE customer_id IN (SELECT DISTINCT customer_id FROM recent_orders)  -- Only needed customers\n),\n\nrelevant_products AS (\n    SELECT\n        product_id,\n        product_name\n    FROM {{ source('raw', 'products') }}\n    WHERE product_id IN (SELECT DISTINCT product_id FROM recent_orders)  -- Only needed products\n)\n\nSELECT\n    o.order_id,\n    c.customer_name,\n    p.product_name,\n    o.order_amount\nFROM recent_orders o\nLEFT JOIN relevant_customers c ON o.customer_id = c.customer_id\nLEFT J",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 119
    },
    "id": "ae_ae_dbt_advanced_437"
  },
  {
    "text": "- Only needed products\n)\n\nSELECT\n    o.order_id,\n    c.customer_name,\n    p.product_name,\n    o.order_amount\nFROM recent_orders o\nLEFT JOIN relevant_customers c ON o.customer_id = c.customer_id\nLEFT JOIN relevant_products p ON o.product_id = p.product_id\n```\n\n**2. Incremental Processing:**\n```sql\n-- models/marts/sales/fct_orders_aggregated.sql\n\n{{\n    config(\n        materialized='incremental',\n        unique_key=['customer_id', 'order_month'],\n        on_schema_change='append_new_columns'\n    )\n}}\n\nWITH monthly_orders AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', order_date) AS order_month,\n        COUNT(DISTINCT order_id) AS order_count,\n        SUM(order_amount) AS total_spent,\n        AVG(order_amount) AS avg_order_value\n    FROM {{ ref('fct_orders') }}\n    \n    {% if is_incremental() %}\n    WHERE order_date >= (\n        SELECT DATEADD('month', -1, MAX(order_month))\n        FROM {{ this }}\n    )\n    {% endif %}\n    \n    GROUP BY 1, 2\n)\n\nSELECT * FROM monthly_orde",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 120
    },
    "id": "ae_ae_dbt_advanced_438"
  },
  {
    "text": "f is_incremental() %}\n    WHERE order_date >= (\n        SELECT DATEADD('month', -1, MAX(order_month))\n        FROM {{ this }}\n    )\n    {% endif %}\n    \n    GROUP BY 1, 2\n)\n\nSELECT * FROM monthly_orders\n```\n\nThese advanced dbt techniques have helped me build reliable, performant analytics pipelines that stakeholders trust!\n\n",
    "metadata": {
      "persona": "ae",
      "file_path": "kb_tejuu/analytics_engineer/ae_dbt_advanced.md",
      "file_name": "ae_dbt_advanced.md",
      "chunk_index": 121
    },
    "id": "ae_ae_dbt_advanced_439"
  },
  {
    "text": "---\ntags: [business-analyst, requirements-gathering, stakeholder-management, documentation, process-improvement]\npersona: ba\n---\n\n# Business Analyst Core Skills & Tejuu's Experience\n\n## Requirements Gathering and Analysis\n\n### Stakeholder Interviews and Workshops\n**Tejuu's Approach:**\nSo in my role as a Business Analyst, I've conducted hundreds of stakeholder interviews and workshops. What I found works best is starting with open-ended questions to understand the business problem before diving into solutions.\n\nFor example, at my previous company, we had a project where the sales team wanted a new reporting dashboard. Instead of just asking what reports they needed, I spent time understanding their daily workflows, pain points, and what decisions they were trying to make. This helped me uncover that they actually needed real-time alerts, not just another dashboard.\n\n**My Interview Technique:**\n```\nPreparation Phase:\n- Research the stakeholder's role and department\n- Review existing docu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 0
    },
    "id": "tejuu_ba_core_skills_440"
  },
  {
    "text": "over that they actually needed real-time alerts, not just another dashboard.\n\n**My Interview Technique:**\n```\nPreparation Phase:\n- Research the stakeholder's role and department\n- Review existing documentation and processes\n- Prepare open-ended questions\n- Set clear meeting objectives\n\nDuring the Interview:\n- Start with business context: \"Walk me through your typical day\"\n- Ask \"why\" questions: \"Why is this important to you?\"\n- Use active listening and take detailed notes\n- Clarify assumptions: \"So what I'm hearing is...\"\n- Ask about pain points: \"What frustrates you most about the current process?\"\n\nFollow-up:\n- Send meeting notes within 24 hours\n- Confirm understanding of key points\n- Identify any gaps or follow-up questions\n```\n\n### Requirements Documentation\n**Tejuu's Documentation Standards:**\nI've learned that good documentation is the foundation of successful projects. Here's how I structure my requirements documents:\n\n**Business Requirements Document (BRD) Template:**\n```\n1. Ex",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 1
    },
    "id": "tejuu_ba_core_skills_441"
  },
  {
    "text": "ndards:**\nI've learned that good documentation is the foundation of successful projects. Here's how I structure my requirements documents:\n\n**Business Requirements Document (BRD) Template:**\n```\n1. Executive Summary\n   - Project overview (2-3 paragraphs)\n   - Business objectives\n   - Expected benefits and ROI\n\n2. Current State Analysis\n   - As-Is process flows\n   - Pain points and challenges\n   - Current system limitations\n\n3. Future State Vision\n   - To-Be process flows\n   - Expected improvements\n   - Success metrics\n\n4. Functional Requirements\n   - User stories with acceptance criteria\n   - Business rules\n   - Data requirements\n\n5. Non-Functional Requirements\n   - Performance requirements\n   - Security and compliance\n   - Usability standards\n\n6. Assumptions and Constraints\n7. Dependencies and Risks\n8. Approval and Sign-off\n```\n\n**User Story Format I Use:**\n```\nAs a [role], I want to [action] so that [benefit].\n\nAcceptance Criteria:\n- Given [context]\n- When [action]\n- Then [expected r",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 2
    },
    "id": "tejuu_ba_core_skills_442"
  },
  {
    "text": "s and Risks\n8. Approval and Sign-off\n```\n\n**User Story Format I Use:**\n```\nAs a [role], I want to [action] so that [benefit].\n\nAcceptance Criteria:\n- Given [context]\n- When [action]\n- Then [expected result]\n\nExample:\nAs a Sales Manager, I want to view real-time sales performance by region \nso that I can identify underperforming areas and take immediate action.\n\nAcceptance Criteria:\n- Given I am logged into the dashboard\n- When I select a specific date range\n- Then I see sales data grouped by region with YoY comparison\n- And I can drill down to individual sales rep performance\n```\n\n## Process Mapping and Improvement\n\n### Creating Process Flow Diagrams\n**Tejuu's Process Mapping Experience:**\nOne of my key strengths is visualizing complex business processes. I use tools like Visio, Lucidchart, and Miro to create process maps that everyone can understand.\n\n**My Process Mapping Approach:**\n```\nStep 1: Identify Process Boundaries\n- Where does the process start?\n- Where does it end?\n- What tr",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 3
    },
    "id": "tejuu_ba_core_skills_443"
  },
  {
    "text": ", and Miro to create process maps that everyone can understand.\n\n**My Process Mapping Approach:**\n```\nStep 1: Identify Process Boundaries\n- Where does the process start?\n- Where does it end?\n- What triggers the process?\n\nStep 2: Map Current State (As-Is)\n- Interview process owners\n- Shadow users performing the process\n- Document every step, decision point, and handoff\n- Identify pain points and bottlenecks\n\nStep 3: Analyze and Identify Improvements\n- Look for redundant steps\n- Identify manual tasks that could be automated\n- Find bottlenecks causing delays\n- Spot quality issues or error-prone steps\n\nStep 4: Design Future State (To-Be)\n- Eliminate unnecessary steps\n- Automate manual processes\n- Streamline handoffs\n- Add quality checks where needed\n\nStep 5: Calculate Impact\n- Time savings\n- Cost reduction\n- Error reduction\n- Customer satisfaction improvement\n```\n\n**Real Example - Order Processing Improvement:**\n```\nAs-Is Process (12 steps, 45 minutes average):\n1. Customer calls to place o",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 4
    },
    "id": "tejuu_ba_core_skills_444"
  },
  {
    "text": "ost reduction\n- Error reduction\n- Customer satisfaction improvement\n```\n\n**Real Example - Order Processing Improvement:**\n```\nAs-Is Process (12 steps, 45 minutes average):\n1. Customer calls to place order\n2. Sales rep manually enters order in Excel\n3. Sales rep emails order to inventory team\n4. Inventory team checks stock manually\n5. Inventory team emails back availability\n6. Sales rep calls customer to confirm\n7. Sales rep enters order in ERP system\n8. Finance team manually creates invoice\n9. Finance emails invoice to customer\n10. Warehouse receives printed order form\n11. Warehouse picks and packs order\n12. Shipping updates tracking manually\n\nTo-Be Process (6 steps, 15 minutes average):\n1. Customer places order through web portal\n2. System automatically checks inventory\n3. System confirms order and sends confirmation email\n4. ERP system auto-generates invoice\n5. Warehouse receives digital order notification\n6. System auto-updates tracking information\n\nImpact:\n- 67% time reduction (45 ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 5
    },
    "id": "tejuu_ba_core_skills_445"
  },
  {
    "text": " order and sends confirmation email\n4. ERP system auto-generates invoice\n5. Warehouse receives digital order notification\n6. System auto-updates tracking information\n\nImpact:\n- 67% time reduction (45 min → 15 min)\n- 90% fewer errors (manual entry eliminated)\n- $50K annual cost savings\n- Improved customer satisfaction (instant confirmation)\n```\n\n## Data Analysis and Reporting\n\n### SQL for Business Analysis\n**Tejuu's SQL Skills:**\nAs a Business Analyst, I use SQL daily to extract insights from databases. Here are some common queries I run:\n\n**Sales Performance Analysis:**\n```sql\n-- Monthly sales trend with year-over-year comparison\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    SUM(order_amount) as current_year_sales,\n    LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date)) as previous_year_sales,\n    ROUND(((SUM(order_amount) - LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) \n           / LAG(SUM(order_amount), 12) OVER (ORDER",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 6
    },
    "id": "tejuu_ba_core_skills_446"
  },
  {
    "text": "', order_date)) as previous_year_sales,\n    ROUND(((SUM(order_amount) - LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) \n           / LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) * 100, 2) as yoy_growth_pct\nFROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '2 years'\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month;\n```\n\n**Customer Segmentation:**\n```sql\n-- RFM analysis for customer segmentation\nWITH customer_metrics AS (\n    SELECT \n        customer_id,\n        MAX(order_date) as last_order_date,\n        COUNT(DISTINCT order_id) as order_count,\n        SUM(order_amount) as total_spent\n    FROM orders\n    WHERE order_date >= CURRENT_DATE - INTERVAL '1 year'\n    GROUP BY customer_id\n),\nrfm_scores AS (\n    SELECT \n        customer_id,\n        DATEDIFF(CURRENT_DATE, last_order_date) as recency_days,\n        order_count as frequency,\n        total_spent as monetary,\n        NTILE(5) OVER (ORDER BY DATEDIFF(CURRENT_DATE,",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 7
    },
    "id": "tejuu_ba_core_skills_447"
  },
  {
    "text": " customer_id,\n        DATEDIFF(CURRENT_DATE, last_order_date) as recency_days,\n        order_count as frequency,\n        total_spent as monetary,\n        NTILE(5) OVER (ORDER BY DATEDIFF(CURRENT_DATE, last_order_date) DESC) as recency_score,\n        NTILE(5) OVER (ORDER BY order_count) as frequency_score,\n        NTILE(5) OVER (ORDER BY total_spent) as monetary_score\n    FROM customer_metrics\n)\nSELECT \n    customer_id,\n    recency_days,\n    frequency,\n    monetary,\n    CASE \n        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'\n        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n        WHEN recency_score >= 4 AND frequency_score <= 2 THEN 'Promising'\n        WHEN recency_score <= 2 AND frequency_score >= 3 THEN 'At Risk'\n        WHEN recency_score <= 2 AND frequency_score <= 2 THEN 'Lost'\n        ELSE 'Others'\n    END as customer_segment\nFROM rfm_scores;\n```\n\n**Product Performance:**\n```sql\n-- Top performing products with contribution an",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 8
    },
    "id": "tejuu_ba_core_skills_448"
  },
  {
    "text": "_score <= 2 AND frequency_score <= 2 THEN 'Lost'\n        ELSE 'Others'\n    END as customer_segment\nFROM rfm_scores;\n```\n\n**Product Performance:**\n```sql\n-- Top performing products with contribution analysis\nSELECT \n    product_name,\n    SUM(quantity_sold) as total_quantity,\n    SUM(revenue) as total_revenue,\n    ROUND(SUM(revenue) * 100.0 / SUM(SUM(revenue)) OVER (), 2) as revenue_contribution_pct,\n    ROUND(SUM(SUM(revenue)) OVER (ORDER BY SUM(revenue) DESC) * 100.0 / SUM(SUM(revenue)) OVER (), 2) as cumulative_pct\nFROM sales\nWHERE sale_date >= CURRENT_DATE - INTERVAL '90 days'\nGROUP BY product_name\nORDER BY total_revenue DESC\nLIMIT 20;\n```\n\n## Stakeholder Management\n\n### Communication Strategies\n**Tejuu's Stakeholder Management:**\nManaging different stakeholders with different priorities is one of the biggest challenges. Here's what I've learned:\n\n**Stakeholder Analysis Matrix:**\n```\nHigh Power, High Interest (Manage Closely):\n- C-level executives\n- Project sponsors\n- Key decision ma",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 9
    },
    "id": "tejuu_ba_core_skills_449"
  },
  {
    "text": " is one of the biggest challenges. Here's what I've learned:\n\n**Stakeholder Analysis Matrix:**\n```\nHigh Power, High Interest (Manage Closely):\n- C-level executives\n- Project sponsors\n- Key decision makers\nStrategy: Regular updates, involve in key decisions, address concerns immediately\n\nHigh Power, Low Interest (Keep Satisfied):\n- Department heads not directly involved\n- Compliance/legal teams\nStrategy: Periodic updates, keep informed of major changes\n\nLow Power, High Interest (Keep Informed):\n- End users\n- Team members\nStrategy: Regular communication, gather feedback, involve in testing\n\nLow Power, Low Interest (Monitor):\n- Peripheral stakeholders\nStrategy: Minimal communication, inform of major milestones\n```\n\n**My Communication Approach:**\n```\nFor Executives:\n- Focus on business impact and ROI\n- Use high-level dashboards and metrics\n- Keep updates brief (1-page summaries)\n- Highlight risks and mitigation plans\n\nFor Technical Teams:\n- Provide detailed requirements and specifications\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 10
    },
    "id": "tejuu_ba_core_skills_450"
  },
  {
    "text": "and ROI\n- Use high-level dashboards and metrics\n- Keep updates brief (1-page summaries)\n- Highlight risks and mitigation plans\n\nFor Technical Teams:\n- Provide detailed requirements and specifications\n- Use technical language appropriately\n- Be available for clarification\n- Respect their expertise and input\n\nFor End Users:\n- Use simple, non-technical language\n- Show how changes benefit them\n- Involve them in testing and feedback\n- Address their concerns empathetically\n\nFor Project Managers:\n- Provide clear timelines and dependencies\n- Flag blockers early\n- Keep requirements documentation updated\n- Attend all status meetings prepared\n```\n\n## Gap Analysis and Feasibility Studies\n\n### Conducting Gap Analysis\n**Tejuu's Gap Analysis Framework:**\n```\n1. Define Current State\n   - Document existing capabilities\n   - Identify current performance metrics\n   - List available resources\n\n2. Define Desired Future State\n   - Document required capabilities\n   - Set target performance metrics\n   - Ident",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 11
    },
    "id": "tejuu_ba_core_skills_451"
  },
  {
    "text": " capabilities\n   - Identify current performance metrics\n   - List available resources\n\n2. Define Desired Future State\n   - Document required capabilities\n   - Set target performance metrics\n   - Identify needed resources\n\n3. Identify Gaps\n   - Capability gaps\n   - Performance gaps\n   - Resource gaps\n   - Knowledge/skill gaps\n\n4. Prioritize Gaps\n   - Impact on business objectives\n   - Urgency and dependencies\n   - Cost and effort to close\n\n5. Develop Action Plan\n   - Quick wins (high impact, low effort)\n   - Strategic initiatives (high impact, high effort)\n   - Fill-ins (low impact, low effort)\n   - Reconsider (low impact, high effort)\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- Requirements gathering and documentation\n- Process mapping and improvement\n- SQL and data analysis\n- Stakeholder management\n- Gap analysis and feasibility studies\n\n### Tools & Technologies:\n- **Documentation**: Confluence, SharePoint, MS Word\n- **Process Mapping**: Visio, Lucidchart, Miro, Draw.io\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 12
    },
    "id": "tejuu_ba_core_skills_452"
  },
  {
    "text": "- Stakeholder management\n- Gap analysis and feasibility studies\n\n### Tools & Technologies:\n- **Documentation**: Confluence, SharePoint, MS Word\n- **Process Mapping**: Visio, Lucidchart, Miro, Draw.io\n- **Project Management**: Jira, Azure DevOps, Asana\n- **Data Analysis**: SQL, Excel, Power BI, Tableau\n- **Collaboration**: MS Teams, Slack, Zoom\n\n### Achievements:\n- Reduced process time by 67% through process optimization\n- Saved $50K annually by identifying automation opportunities\n- Improved customer satisfaction by 25% through better requirements\n- Successfully delivered 15+ projects on time and within budget\n- Managed stakeholder groups of 20+ people across multiple departments\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 13
    },
    "id": "tejuu_ba_core_skills_453"
  },
  {
    "text": "---\ntags: [sql, advanced-sql, window-functions, cte, optimization, healthcare, medicaid]\npersona: tejuu\n---\n\n# Advanced SQL for Business Analysis - Tejuu's Expertise\n\n## Healthcare & Medicaid Analytics SQL\n\n### Claims Analysis at Stryker\n**Tejuu's Real-World SQL for Medicaid:**\n\n```sql\n-- Medicaid Claims Analysis with Payment Patterns\nWITH claim_summary AS (\n    SELECT \n        claim_id,\n        patient_id,\n        provider_id,\n        claim_date,\n        service_date,\n        claim_amount,\n        paid_amount,\n        denied_amount,\n        claim_status,\n        denial_reason,\n        payer_name,\n        DATEDIFF(day, service_date, claim_date) as days_to_submit,\n        DATEDIFF(day, claim_date, payment_date) as days_to_payment\n    FROM fact_claims\n    WHERE payer_type = 'Medicaid'\n        AND claim_date >= '2023-01-01'\n),\npayment_metrics AS (\n    SELECT \n        provider_id,\n        COUNT(*) as total_claims,\n        SUM(claim_amount) as total_billed,\n        SUM(paid_amount) as total",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 14
    },
    "id": "tejuu_sql_advanced_454"
  },
  {
    "text": "  AND claim_date >= '2023-01-01'\n),\npayment_metrics AS (\n    SELECT \n        provider_id,\n        COUNT(*) as total_claims,\n        SUM(claim_amount) as total_billed,\n        SUM(paid_amount) as total_paid,\n        SUM(denied_amount) as total_denied,\n        AVG(paid_amount) as avg_payment,\n        AVG(days_to_payment) as avg_days_to_payment,\n        SUM(CASE WHEN claim_status = 'Denied' THEN 1 ELSE 0 END) as denied_count,\n        SUM(CASE WHEN claim_status = 'Paid' THEN 1 ELSE 0 END) as paid_count\n    FROM claim_summary\n    GROUP BY provider_id\n)\nSELECT \n    p.provider_id,\n    p.provider_name,\n    p.provider_specialty,\n    pm.total_claims,\n    pm.total_billed,\n    pm.total_paid,\n    ROUND((pm.total_paid / NULLIF(pm.total_billed, 0)) * 100, 2) as payment_rate_pct,\n    pm.denied_count,\n    ROUND((pm.denied_count * 100.0 / pm.total_claims), 2) as denial_rate_pct,\n    pm.avg_payment,\n    pm.avg_days_to_payment,\n    CASE \n        WHEN pm.avg_days_to_payment <= 30 THEN 'Fast'\n        WHEN p",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 15
    },
    "id": "tejuu_sql_advanced_455"
  },
  {
    "text": "ROUND((pm.denied_count * 100.0 / pm.total_claims), 2) as denial_rate_pct,\n    pm.avg_payment,\n    pm.avg_days_to_payment,\n    CASE \n        WHEN pm.avg_days_to_payment <= 30 THEN 'Fast'\n        WHEN pm.avg_days_to_payment <= 60 THEN 'Average'\n        ELSE 'Slow'\n    END as payment_speed_category\nFROM payment_metrics pm\nJOIN dim_provider p ON pm.provider_id = p.provider_id\nWHERE pm.total_claims >= 10  -- Minimum volume threshold\nORDER BY pm.total_billed DESC;\n```\n\n### Patient Readmission Analysis\n**30-Day Readmission Tracking:**\n\n```sql\n-- Identify 30-Day Hospital Readmissions\nWITH patient_visits AS (\n    SELECT \n        patient_id,\n        visit_id,\n        admission_date,\n        discharge_date,\n        diagnosis_code,\n        drg_code,\n        facility_id,\n        LEAD(admission_date) OVER (\n            PARTITION BY patient_id \n            ORDER BY admission_date\n        ) as next_admission_date,\n        LEAD(visit_id) OVER (\n            PARTITION BY patient_id \n            ORDER BY ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 16
    },
    "id": "tejuu_sql_advanced_456"
  },
  {
    "text": "ER (\n            PARTITION BY patient_id \n            ORDER BY admission_date\n        ) as next_admission_date,\n        LEAD(visit_id) OVER (\n            PARTITION BY patient_id \n            ORDER BY admission_date\n        ) as next_visit_id\n    FROM fact_patient_visits\n    WHERE discharge_date IS NOT NULL\n        AND admission_date >= '2023-01-01'\n),\nreadmissions AS (\n    SELECT \n        patient_id,\n        visit_id as initial_visit_id,\n        next_visit_id as readmission_visit_id,\n        admission_date as initial_admission,\n        discharge_date as initial_discharge,\n        next_admission_date as readmission_date,\n        DATEDIFF(day, discharge_date, next_admission_date) as days_between_visits,\n        diagnosis_code as initial_diagnosis,\n        drg_code as initial_drg\n    FROM patient_visits\n    WHERE next_admission_date IS NOT NULL\n        AND DATEDIFF(day, discharge_date, next_admission_date) <= 30\n)\nSELECT \n    r.patient_id,\n    p.patient_name,\n    p.age,\n    p.gender,\n    ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 17
    },
    "id": "tejuu_sql_advanced_457"
  },
  {
    "text": "ient_visits\n    WHERE next_admission_date IS NOT NULL\n        AND DATEDIFF(day, discharge_date, next_admission_date) <= 30\n)\nSELECT \n    r.patient_id,\n    p.patient_name,\n    p.age,\n    p.gender,\n    r.initial_visit_id,\n    r.readmission_visit_id,\n    r.initial_admission,\n    r.initial_discharge,\n    r.readmission_date,\n    r.days_between_visits,\n    r.initial_diagnosis,\n    d.diagnosis_description,\n    r.initial_drg,\n    drg.drg_description,\n    f.facility_name,\n    CASE \n        WHEN r.days_between_visits <= 7 THEN 'Very High Risk'\n        WHEN r.days_between_visits <= 14 THEN 'High Risk'\n        WHEN r.days_between_visits <= 21 THEN 'Medium Risk'\n        ELSE 'Low Risk'\n    END as readmission_risk_category\nFROM readmissions r\nJOIN dim_patient p ON r.patient_id = p.patient_id\nJOIN dim_diagnosis d ON r.initial_diagnosis = d.diagnosis_code\nJOIN dim_drg drg ON r.initial_drg = drg.drg_code\nJOIN fact_patient_visits v ON r.initial_visit_id = v.visit_id\nJOIN dim_facility f ON v.facility_id ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 18
    },
    "id": "tejuu_sql_advanced_458"
  },
  {
    "text": "agnosis d ON r.initial_diagnosis = d.diagnosis_code\nJOIN dim_drg drg ON r.initial_drg = drg.drg_code\nJOIN fact_patient_visits v ON r.initial_visit_id = v.visit_id\nJOIN dim_facility f ON v.facility_id = f.facility_id\nORDER BY r.days_between_visits, r.readmission_date DESC;\n\n-- Readmission Rate by Diagnosis\nSELECT \n    d.diagnosis_code,\n    d.diagnosis_description,\n    COUNT(DISTINCT v.visit_id) as total_discharges,\n    COUNT(DISTINCT r.readmission_visit_id) as readmissions,\n    ROUND((COUNT(DISTINCT r.readmission_visit_id) * 100.0 / \n           COUNT(DISTINCT v.visit_id)), 2) as readmission_rate_pct,\n    AVG(r.days_between_visits) as avg_days_to_readmission\nFROM fact_patient_visits v\nJOIN dim_diagnosis d ON v.primary_diagnosis = d.diagnosis_code\nLEFT JOIN readmissions r ON v.visit_id = r.initial_visit_id\nWHERE v.discharge_date >= '2023-01-01'\nGROUP BY d.diagnosis_code, d.diagnosis_description\nHAVING COUNT(DISTINCT v.visit_id) >= 20  -- Minimum volume\nORDER BY readmission_rate_pct DESC;\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 19
    },
    "id": "tejuu_sql_advanced_459"
  },
  {
    "text": "itial_visit_id\nWHERE v.discharge_date >= '2023-01-01'\nGROUP BY d.diagnosis_code, d.diagnosis_description\nHAVING COUNT(DISTINCT v.visit_id) >= 20  -- Minimum volume\nORDER BY readmission_rate_pct DESC;\n```\n\n## Advanced Window Functions for Business Analytics\n\n### Sales Performance Analysis\n**Tejuu's Sales Analytics at CVS Health:**\n\n```sql\n-- Comprehensive Sales Performance with Rankings and Trends\nWITH daily_sales AS (\n    SELECT \n        sale_date,\n        product_id,\n        store_id,\n        region,\n        SUM(quantity) as units_sold,\n        SUM(sales_amount) as daily_sales,\n        SUM(cost_amount) as daily_cost\n    FROM fact_sales\n    WHERE sale_date >= '2023-01-01'\n    GROUP BY sale_date, product_id, store_id, region\n),\nsales_with_metrics AS (\n    SELECT \n        sale_date,\n        product_id,\n        store_id,\n        region,\n        daily_sales,\n        daily_cost,\n        daily_sales - daily_cost as daily_profit,\n        \n        -- Running totals\n        SUM(daily_sales) OVE",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 20
    },
    "id": "tejuu_sql_advanced_460"
  },
  {
    "text": "product_id,\n        store_id,\n        region,\n        daily_sales,\n        daily_cost,\n        daily_sales - daily_cost as daily_profit,\n        \n        -- Running totals\n        SUM(daily_sales) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) as running_total_sales,\n        \n        -- Moving averages\n        AVG(daily_sales) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n        ) as moving_avg_7day,\n        \n        AVG(daily_sales) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n        ) as moving_avg_30day,\n        \n        -- Year-over-year comparison\n        LAG(daily_sales, 365) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n        ) as sales_last_year,",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 21
    },
    "id": "tejuu_sql_advanced_461"
  },
  {
    "text": "vg_30day,\n        \n        -- Year-over-year comparison\n        LAG(daily_sales, 365) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n        ) as sales_last_year,\n        \n        -- Month-over-month comparison\n        LAG(daily_sales, 30) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n        ) as sales_last_month,\n        \n        -- Rankings\n        RANK() OVER (\n            PARTITION BY sale_date, region \n            ORDER BY daily_sales DESC\n        ) as daily_rank_in_region,\n        \n        DENSE_RANK() OVER (\n            PARTITION BY DATE_TRUNC('month', sale_date), region \n            ORDER BY SUM(daily_sales) OVER (\n                PARTITION BY product_id, store_id, DATE_TRUNC('month', sale_date)\n            ) DESC\n        ) as monthly_rank_in_region,\n        \n        -- Percentiles\n        PERCENT_RANK() OVER (\n            PARTITION BY region \n            ORDER BY daily_sales\n        ) as sales_percen",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 22
    },
    "id": "tejuu_sql_advanced_462"
  },
  {
    "text": "   ) DESC\n        ) as monthly_rank_in_region,\n        \n        -- Percentiles\n        PERCENT_RANK() OVER (\n            PARTITION BY region \n            ORDER BY daily_sales\n        ) as sales_percentile\n        \n    FROM daily_sales\n)\nSELECT \n    sale_date,\n    p.product_name,\n    s.store_name,\n    sm.region,\n    sm.daily_sales,\n    sm.daily_profit,\n    sm.running_total_sales,\n    sm.moving_avg_7day,\n    sm.moving_avg_30day,\n    sm.sales_last_year,\n    ROUND(((sm.daily_sales - sm.sales_last_year) / \n           NULLIF(sm.sales_last_year, 0)) * 100, 2) as yoy_growth_pct,\n    sm.daily_rank_in_region,\n    sm.monthly_rank_in_region,\n    ROUND(sm.sales_percentile * 100, 2) as sales_percentile_rank,\n    CASE \n        WHEN sm.daily_sales > sm.moving_avg_30day * 1.2 THEN 'High Performance'\n        WHEN sm.daily_sales < sm.moving_avg_30day * 0.8 THEN 'Low Performance'\n        ELSE 'Normal'\n    END as performance_category\nFROM sales_with_metrics sm\nJOIN dim_product p ON sm.product_id = p.produc",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 23
    },
    "id": "tejuu_sql_advanced_463"
  },
  {
    "text": "  WHEN sm.daily_sales < sm.moving_avg_30day * 0.8 THEN 'Low Performance'\n        ELSE 'Normal'\n    END as performance_category\nFROM sales_with_metrics sm\nJOIN dim_product p ON sm.product_id = p.product_id\nJOIN dim_store s ON sm.store_id = s.store_id\nWHERE sale_date >= '2024-01-01'\nORDER BY sale_date DESC, sm.daily_sales DESC;\n```\n\n### Customer Cohort Analysis\n**Retention and Lifetime Value:**\n\n```sql\n-- Customer Cohort Analysis with Retention Rates\nWITH first_purchase AS (\n    SELECT \n        customer_id,\n        MIN(order_date) as cohort_month,\n        MIN(order_id) as first_order_id\n    FROM fact_orders\n    GROUP BY customer_id\n),\ncustomer_orders AS (\n    SELECT \n        o.customer_id,\n        o.order_date,\n        o.order_amount,\n        fp.cohort_month,\n        DATE_TRUNC('month', o.order_date) as order_month,\n        DATEDIFF(month, fp.cohort_month, DATE_TRUNC('month', o.order_date)) as months_since_first\n    FROM fact_orders o\n    JOIN first_purchase fp ON o.customer_id = fp.cust",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 24
    },
    "id": "tejuu_sql_advanced_464"
  },
  {
    "text": "der_date) as order_month,\n        DATEDIFF(month, fp.cohort_month, DATE_TRUNC('month', o.order_date)) as months_since_first\n    FROM fact_orders o\n    JOIN first_purchase fp ON o.customer_id = fp.customer_id\n    WHERE o.order_date >= '2023-01-01'\n),\ncohort_data AS (\n    SELECT \n        cohort_month,\n        months_since_first,\n        COUNT(DISTINCT customer_id) as customers,\n        SUM(order_amount) as total_revenue,\n        AVG(order_amount) as avg_order_value\n    FROM customer_orders\n    GROUP BY cohort_month, months_since_first\n),\ncohort_size AS (\n    SELECT \n        cohort_month,\n        COUNT(DISTINCT customer_id) as cohort_size\n    FROM first_purchase\n    GROUP BY cohort_month\n)\nSELECT \n    cd.cohort_month,\n    cs.cohort_size,\n    cd.months_since_first,\n    cd.customers,\n    ROUND((cd.customers * 100.0 / cs.cohort_size), 2) as retention_rate_pct,\n    cd.total_revenue,\n    cd.avg_order_value,\n    SUM(cd.total_revenue) OVER (\n        PARTITION BY cd.cohort_month \n        ORDER BY",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 25
    },
    "id": "tejuu_sql_advanced_465"
  },
  {
    "text": "(cd.customers * 100.0 / cs.cohort_size), 2) as retention_rate_pct,\n    cd.total_revenue,\n    cd.avg_order_value,\n    SUM(cd.total_revenue) OVER (\n        PARTITION BY cd.cohort_month \n        ORDER BY cd.months_since_first\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) as cumulative_revenue,\n    ROUND(SUM(cd.total_revenue) OVER (\n        PARTITION BY cd.cohort_month \n        ORDER BY cd.months_since_first\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) / cs.cohort_size, 2) as lifetime_value\nFROM cohort_data cd\nJOIN cohort_size cs ON cd.cohort_month = cs.cohort_month\nORDER BY cd.cohort_month, cd.months_since_first;\n```\n\n## Complex CTEs and Recursive Queries\n\n### Organizational Hierarchy Analysis\n**Tejuu's HR Analytics:**\n\n```sql\n-- Recursive CTE for Employee Hierarchy\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: Top-level managers (no manager)\n    SELECT \n        employee_id,\n        employee_name,\n        manager_id,\n        job_title,\n       ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 26
    },
    "id": "tejuu_sql_advanced_466"
  },
  {
    "text": "ierarchy\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: Top-level managers (no manager)\n    SELECT \n        employee_id,\n        employee_name,\n        manager_id,\n        job_title,\n        department,\n        salary,\n        1 as level,\n        CAST(employee_name AS VARCHAR(1000)) as hierarchy_path,\n        CAST(employee_id AS VARCHAR(1000)) as id_path\n    FROM dim_employee\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: Employees with managers\n    SELECT \n        e.employee_id,\n        e.employee_name,\n        e.manager_id,\n        e.job_title,\n        e.department,\n        e.salary,\n        eh.level + 1,\n        CAST(eh.hierarchy_path || ' > ' || e.employee_name AS VARCHAR(1000)),\n        CAST(eh.id_path || '>' || e.employee_id AS VARCHAR(1000))\n    FROM dim_employee e\n    INNER JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n    WHERE eh.level < 10  -- Prevent infinite recursion\n),\nteam_metrics AS (\n    SELECT \n        eh.employee_",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 27
    },
    "id": "tejuu_sql_advanced_467"
  },
  {
    "text": "  FROM dim_employee e\n    INNER JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n    WHERE eh.level < 10  -- Prevent infinite recursion\n),\nteam_metrics AS (\n    SELECT \n        eh.employee_id,\n        eh.employee_name,\n        eh.level,\n        COUNT(DISTINCT e2.employee_id) as direct_reports,\n        COUNT(DISTINCT e3.employee_id) as total_team_size,\n        AVG(e3.salary) as avg_team_salary,\n        SUM(e3.salary) as total_team_cost\n    FROM employee_hierarchy eh\n    LEFT JOIN dim_employee e2 ON eh.employee_id = e2.manager_id\n    LEFT JOIN employee_hierarchy e3 ON e3.id_path LIKE eh.id_path || '%'\n    GROUP BY eh.employee_id, eh.employee_name, eh.level\n)\nSELECT \n    eh.employee_id,\n    eh.employee_name,\n    eh.job_title,\n    eh.department,\n    eh.salary,\n    eh.level,\n    eh.hierarchy_path,\n    tm.direct_reports,\n    tm.total_team_size,\n    tm.avg_team_salary,\n    tm.total_team_cost,\n    ROUND((eh.salary / NULLIF(tm.avg_team_salary, 0)), 2) as salary_vs_team_avg\nFROM emplo",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 28
    },
    "id": "tejuu_sql_advanced_468"
  },
  {
    "text": "ierarchy_path,\n    tm.direct_reports,\n    tm.total_team_size,\n    tm.avg_team_salary,\n    tm.total_team_cost,\n    ROUND((eh.salary / NULLIF(tm.avg_team_salary, 0)), 2) as salary_vs_team_avg\nFROM employee_hierarchy eh\nJOIN team_metrics tm ON eh.employee_id = tm.employee_id\nORDER BY eh.level, eh.employee_name;\n```\n\n### Product Category Hierarchy\n**Inventory Analysis with Rollups:**\n\n```sql\n-- Product Category Hierarchy with Sales Rollup\nWITH RECURSIVE category_hierarchy AS (\n    SELECT \n        category_id,\n        category_name,\n        parent_category_id,\n        1 as level,\n        CAST(category_name AS VARCHAR(500)) as category_path\n    FROM dim_category\n    WHERE parent_category_id IS NULL\n    \n    UNION ALL\n    \n    SELECT \n        c.category_id,\n        c.category_name,\n        c.parent_category_id,\n        ch.level + 1,\n        CAST(ch.category_path || ' > ' || c.category_name AS VARCHAR(500))\n    FROM dim_category c\n    INNER JOIN category_hierarchy ch ON c.parent_category_id = ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 29
    },
    "id": "tejuu_sql_advanced_469"
  },
  {
    "text": "nt_category_id,\n        ch.level + 1,\n        CAST(ch.category_path || ' > ' || c.category_name AS VARCHAR(500))\n    FROM dim_category c\n    INNER JOIN category_hierarchy ch ON c.parent_category_id = ch.category_id\n),\nsales_by_category AS (\n    SELECT \n        p.category_id,\n        SUM(s.sales_amount) as total_sales,\n        SUM(s.quantity) as total_quantity,\n        COUNT(DISTINCT s.order_id) as order_count,\n        COUNT(DISTINCT s.customer_id) as customer_count\n    FROM fact_sales s\n    JOIN dim_product p ON s.product_id = p.product_id\n    WHERE s.sale_date >= '2024-01-01'\n    GROUP BY p.category_id\n)\nSELECT \n    ch.category_id,\n    ch.category_name,\n    ch.parent_category_id,\n    ch.level,\n    ch.category_path,\n    COALESCE(sc.total_sales, 0) as direct_sales,\n    COALESCE(sc.total_quantity, 0) as direct_quantity,\n    SUM(COALESCE(sc2.total_sales, 0)) as total_sales_with_children,\n    SUM(COALESCE(sc2.total_quantity, 0)) as total_quantity_with_children\nFROM category_hierarchy ch\nLE",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 30
    },
    "id": "tejuu_sql_advanced_470"
  },
  {
    "text": "ntity, 0) as direct_quantity,\n    SUM(COALESCE(sc2.total_sales, 0)) as total_sales_with_children,\n    SUM(COALESCE(sc2.total_quantity, 0)) as total_quantity_with_children\nFROM category_hierarchy ch\nLEFT JOIN sales_by_category sc ON ch.category_id = sc.category_id\nLEFT JOIN category_hierarchy ch2 ON ch2.category_path LIKE ch.category_path || '%'\nLEFT JOIN sales_by_category sc2 ON ch2.category_id = sc2.category_id\nGROUP BY \n    ch.category_id,\n    ch.category_name,\n    ch.parent_category_id,\n    ch.level,\n    ch.category_path,\n    sc.total_sales,\n    sc.total_quantity\nORDER BY ch.category_path;\n```\n\n## Data Quality and Validation SQL\n\n### Comprehensive Data Quality Checks\n**Tejuu's Data Validation Framework:**\n\n```sql\n-- Data Quality Report\nWITH null_checks AS (\n    SELECT \n        'fact_sales' as table_name,\n        'customer_id' as column_name,\n        COUNT(*) as total_records,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_count,\n        ROUND(SUM(CASE WHEN cust",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 31
    },
    "id": "tejuu_sql_advanced_471"
  },
  {
    "text": "es' as table_name,\n        'customer_id' as column_name,\n        COUNT(*) as total_records,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_count,\n        ROUND(SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as null_percentage\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n    \n    UNION ALL\n    \n    SELECT \n        'fact_sales',\n        'sale_date',\n        COUNT(*),\n        SUM(CASE WHEN sale_date IS NULL THEN 1 ELSE 0 END),\n        ROUND(SUM(CASE WHEN sale_date IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n    \n    UNION ALL\n    \n    SELECT \n        'fact_sales',\n        'sales_amount',\n        COUNT(*),\n        SUM(CASE WHEN sales_amount IS NULL THEN 1 ELSE 0 END),\n        ROUND(SUM(CASE WHEN sales_amount IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 32
    },
    "id": "tejuu_sql_advanced_472"
  },
  {
    "text": "les_amount IS NULL THEN 1 ELSE 0 END),\n        ROUND(SUM(CASE WHEN sales_amount IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n),\nrange_checks AS (\n    SELECT \n        'fact_sales' as table_name,\n        'sales_amount' as column_name,\n        MIN(sales_amount) as min_value,\n        MAX(sales_amount) as max_value,\n        AVG(sales_amount) as avg_value,\n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sales_amount) as median_value,\n        SUM(CASE WHEN sales_amount < 0 THEN 1 ELSE 0 END) as negative_count,\n        SUM(CASE WHEN sales_amount = 0 THEN 1 ELSE 0 END) as zero_count\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n),\nduplicate_checks AS (\n    SELECT \n        'fact_sales' as table_name,\n        'order_id' as key_column,\n        COUNT(*) as total_records,\n        COUNT(DISTINCT order_id) as unique_records,\n        COUNT(*) - COUNT(DISTINCT order_id) as duplicate_count",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 33
    },
    "id": "tejuu_sql_advanced_473"
  },
  {
    "text": "' as table_name,\n        'order_id' as key_column,\n        COUNT(*) as total_records,\n        COUNT(DISTINCT order_id) as unique_records,\n        COUNT(*) - COUNT(DISTINCT order_id) as duplicate_count\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n),\nreferential_integrity AS (\n    SELECT \n        'fact_sales -> dim_customer' as relationship,\n        COUNT(DISTINCT s.customer_id) as fact_keys,\n        COUNT(DISTINCT c.customer_id) as dim_keys,\n        COUNT(DISTINCT s.customer_id) - COUNT(DISTINCT c.customer_id) as orphaned_records\n    FROM fact_sales s\n    LEFT JOIN dim_customer c ON s.customer_id = c.customer_id\n    WHERE s.sale_date >= CURRENT_DATE - INTERVAL '30 days'\n)\nSELECT \n    'Null Checks' as check_type,\n    table_name,\n    column_name,\n    null_count as issue_count,\n    null_percentage as issue_percentage,\n    CASE WHEN null_percentage > 5 THEN 'FAIL' ELSE 'PASS' END as status\nFROM null_checks\nWHERE null_count > 0\n\nUNION ALL\n\nSELECT \n    'Range C",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 34
    },
    "id": "tejuu_sql_advanced_474"
  },
  {
    "text": " as issue_count,\n    null_percentage as issue_percentage,\n    CASE WHEN null_percentage > 5 THEN 'FAIL' ELSE 'PASS' END as status\nFROM null_checks\nWHERE null_count > 0\n\nUNION ALL\n\nSELECT \n    'Range Checks',\n    table_name,\n    column_name,\n    negative_count + zero_count,\n    ROUND((negative_count + zero_count) * 100.0 / \n          (SELECT COUNT(*) FROM fact_sales WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'), 2),\n    CASE WHEN negative_count > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM range_checks\n\nUNION ALL\n\nSELECT \n    'Duplicate Checks',\n    table_name,\n    key_column,\n    duplicate_count,\n    ROUND(duplicate_count * 100.0 / total_records, 2),\n    CASE WHEN duplicate_count > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM duplicate_checks\n\nUNION ALL\n\nSELECT \n    'Referential Integrity',\n    relationship,\n    '',\n    orphaned_records,\n    ROUND(orphaned_records * 100.0 / fact_keys, 2),\n    CASE WHEN orphaned_records > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM referential_integrity;\n```\n\n## Interview ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 35
    },
    "id": "tejuu_sql_advanced_475"
  },
  {
    "text": "ionship,\n    '',\n    orphaned_records,\n    ROUND(orphaned_records * 100.0 / fact_keys, 2),\n    CASE WHEN orphaned_records > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM referential_integrity;\n```\n\n## Interview Talking Points\n\n### Technical Achievements\n- Wrote 500+ SQL queries for business analytics\n- Optimized slow queries from 5 minutes to 10 seconds\n- Built automated data quality checks catching 95% of issues\n- Created reusable SQL templates for common analyses\n- Trained 15+ analysts on advanced SQL techniques\n\n### Problem-Solving Examples\n**Performance Optimization:**\n\"At Stryker, we had this Medicaid claims query that was taking 5 minutes to run. I analyzed the execution plan and found we were doing a full table scan on a 50 million row table. I added appropriate indexes on claim_date and provider_id, rewrote the query to use CTEs instead of subqueries, and got it down to 10 seconds.\"\n\n**Complex Business Logic:**\n\"At Central Bank, finance needed a complex calculation for regulatory reportin",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 36
    },
    "id": "tejuu_sql_advanced_476"
  },
  {
    "text": "der_id, rewrote the query to use CTEs instead of subqueries, and got it down to 10 seconds.\"\n\n**Complex Business Logic:**\n\"At Central Bank, finance needed a complex calculation for regulatory reporting that involved multiple date ranges, conditional aggregations, and hierarchical rollups. I broke it down into CTEs, tested each piece separately, and documented the logic. The final query was 200 lines but very maintainable and accurate.\"\n\n### Tools & Technologies\n- **Databases**: SQL Server, Oracle, PostgreSQL, MySQL\n- **SQL Skills**: Window functions, CTEs, recursive queries, performance tuning\n- **Tools**: SQL Server Management Studio, DBeaver, DataGrip\n- **Integration**: Power BI, Tableau, Python, Excel\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 37
    },
    "id": "tejuu_sql_advanced_477"
  },
  {
    "text": "---\ntags: [tejuu, experience, resume, business-analyst, bi-developer, analytics-engineer]\npersona: tejuu\n---\n\n# Tejuu's Professional Experience & Background\n\n## Professional Summary\n\nBusiness Intelligence & Analytics professional with hands-on expertise in SQL, Python, and modern BI stacks. Experienced in leading data migration initiatives, building ETL pipelines on Azure, and delivering enterprise-grade Power BI solutions. Adept at turning messy legacy systems into trusted metrics and scalable models that power decision-making for finance, operations, and leadership.\n\n## Professional Experience\n\n### Senior Analytics Engineer — Central Bank of Missouri\n**Duration:** Dec 2024 – Present\n\n**Key Responsibilities and Achievements:**\n\nSo at Central Bank of Missouri, I'm currently leading this major migration project where we're moving all these fragmented Excel files and legacy reports into Azure SQL with Power BI semantic models. What I did was write SQL and Python validation scripts that h",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 38
    },
    "id": "tejuu_tejuu_experience_478"
  },
  {
    "text": "jor migration project where we're moving all these fragmented Excel files and legacy reports into Azure SQL with Power BI semantic models. What I did was write SQL and Python validation scripts that helped us reconcile over 12 million rows of data. The result was pretty significant - we improved reporting efficiency by 25% and really reduced the manual workload that the team was dealing with.\n\nOne of the things I'm really proud of is building these certified dashboards with role-based security. I implemented RLS (row-level security) and standardized KPIs, which enabled secure self-service for our finance, audit, and risk teams. This actually reduced ad-hoc reporting requests by 30% across multiple departments, which was a huge win for everyone.\n\nI also implemented automated SQL quality checks and monitoring pipelines. What this did was improve the accuracy of 15+ recurring regulatory reports by 20%. This was super important because it strengthened our compliance confidence and increase",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 39
    },
    "id": "tejuu_tejuu_experience_479"
  },
  {
    "text": "cks and monitoring pipelines. What this did was improve the accuracy of 15+ recurring regulatory reports by 20%. This was super important because it strengthened our compliance confidence and increased trust among senior stakeholders.\n\nAnother major project was partnering with finance and audit groups to map over 200 legacy metrics into standardized semantic models. We ended up deprecating duplicate workbooks and consolidating 30% of manual spreadsheets into governed BI assets that are now used organization-wide.\n\nI also mentor analysts on DAX optimization, KPI documentation, and structured intake processes. This has raised delivery timeliness, improved collaboration, and freed over 30 analyst hours monthly from repetitive reporting and rework.\n\n**Technologies Used:**\n- Azure SQL, Power BI, DAX, Power Query\n- SQL, Python\n- Git for version control\n- Azure Data Factory\n\n### Analytics Engineer — Stryker\n**Duration:** Jan 2022 – Dec 2024\n\n**Key Responsibilities and Achievements:**\n\nAt Stry",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 40
    },
    "id": "tejuu_tejuu_experience_480"
  },
  {
    "text": ", DAX, Power Query\n- SQL, Python\n- Git for version control\n- Azure Data Factory\n\n### Analytics Engineer — Stryker\n**Duration:** Jan 2022 – Dec 2024\n\n**Key Responsibilities and Achievements:**\n\nAt Stryker, I directed this massive migration of global sales and service data from Oracle and Excel into Azure Synapse star schemas. I built the ETL using Azure Data Factory and Databricks, and validated over 15 million transactions using Python scripts. This enabled unified reporting across all regions, which was a game-changer for the company.\n\nOne of my biggest achievements was engineering these partitioned ELT pipelines with optimized orchestration. I cut refresh runtimes by 45% and lowered compute spend significantly. At the same time, we scaled secure data access to over 1,000 field reps, executives, and business stakeholders worldwide.\n\nI developed comprehensive data lineage documentation and KPI guardrails. What this did was lower discrepancies in quarterly executive scorecards, accelera",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 41
    },
    "id": "tejuu_tejuu_experience_481"
  },
  {
    "text": "utives, and business stakeholders worldwide.\n\nI developed comprehensive data lineage documentation and KPI guardrails. What this did was lower discrepancies in quarterly executive scorecards, accelerate month-end close by days, and improve trust in CFO reporting packages globally.\n\nI also collaborated with clinical and product teams to translate operational questions into measurable KPIs and drill-through dashboards. This work uncovered process inefficiencies and unlocked over $2 million in actionable revenue opportunities.\n\n**Technologies Used:**\n- Azure Synapse, Azure Data Factory, Databricks\n- SQL, Python, PySpark\n- Power BI, DAX\n- Oracle database\n- Star schema design\n\n**Challenges Overcome:**\n- Data quality issues during migration from legacy systems\n- Performance optimization for large-scale data processing\n- Stakeholder alignment across multiple global regions\n- Balancing technical debt with new feature delivery\n\n### Product Data Analyst — CVS Health\n**Duration:** May 2020 – Jan ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 42
    },
    "id": "tejuu_tejuu_experience_482"
  },
  {
    "text": "e-scale data processing\n- Stakeholder alignment across multiple global regions\n- Balancing technical debt with new feature delivery\n\n### Product Data Analyst — CVS Health\n**Duration:** May 2020 – Jan 2022\n\n**Key Responsibilities and Achievements:**\n\nAt CVS Health, I migrated pharmacy and inventory reporting from Access and Excel into Azure SQL with Power BI pipelines. I designed Python and SQL reconciliation scripts that validated over 10 million records and reduced manual reconciliation tasks by 40% consistently.\n\nI tuned complex SQL transformations and Power Query refreshes to stabilize recurring dashboards. This saved 25-30 analyst hours monthly and improved the accuracy and reliability of operational reporting across nationwide teams.\n\nI partnered with engineers to define semantic layer fields, KPI acceptance contracts, and validation scripts. This ensured smooth release cycles and reduced risks of downstream disruptions during production deployments.\n\nI also piloted A/B reporting ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 43
    },
    "id": "tejuu_tejuu_experience_483"
  },
  {
    "text": "r fields, KPI acceptance contracts, and validation scripts. This ensured smooth release cycles and reduced risks of downstream disruptions during production deployments.\n\nI also piloted A/B reporting tied to operational initiatives and staffing levers. I translated findings into measurable throughput improvements that directly influenced roadmap prioritization and leadership decision making.\n\n**Technologies Used:**\n- Azure SQL, Power BI\n- Python, SQL\n- Power Query, DAX\n- Access database migration\n- Excel automation\n\n**Key Learnings:**\n- Importance of data quality checks in migration projects\n- Stakeholder communication is critical for adoption\n- Documentation saves countless hours of rework\n- Automated testing catches issues before production\n\n### Business Analyst — Colruyt IT Group\n**Duration:** May 2018 – Dec 2019\n\n**Key Responsibilities and Achievements:**\n\nAt Colruyt IT Group, I consolidated fragmented Excel and Access reporting processes into standardized Power BI dashboards with ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 44
    },
    "id": "tejuu_tejuu_experience_484"
  },
  {
    "text": "tion:** May 2018 – Dec 2019\n\n**Key Responsibilities and Achievements:**\n\nAt Colruyt IT Group, I consolidated fragmented Excel and Access reporting processes into standardized Power BI dashboards with reusable DAX templates. This reduced duplication and increased trust in weekly financial and merchandising performance metrics.\n\nI designed SQL dimensional models to support finance and merchandising analyses. This accelerated month-end close cycles, provided interactive drilldowns, and replaced time-consuming manual spreadsheet-based efforts.\n\nI captured requirements across departments, authored detailed user stories, and developed UAT scripts with standardized KPI definitions. This ensured successful deliveries aligned with governance rules and stakeholder expectations.\n\n**Technologies Used:**\n- Power BI, DAX, Power Query\n- SQL Server\n- Excel, Access\n- Requirements gathering tools\n- UAT documentation\n\n**Skills Developed:**\n- Stakeholder management across multiple departments\n- Requiremen",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 45
    },
    "id": "tejuu_tejuu_experience_485"
  },
  {
    "text": "**\n- Power BI, DAX, Power Query\n- SQL Server\n- Excel, Access\n- Requirements gathering tools\n- UAT documentation\n\n**Skills Developed:**\n- Stakeholder management across multiple departments\n- Requirements gathering and documentation\n- UAT script development\n- KPI definition and governance\n\n## Technical Skills\n\n### Programming & Data\n- **SQL**: Expert level - Complex queries, window functions, CTEs, performance tuning\n- **Python**: Proficient - Pandas, NumPy, data validation scripts, automation\n- **R**: Intermediate - Statistical analysis, data visualization\n- **Git**: Version control for code and documentation\n\n### Analytics & BI Tools\n- **Power BI**: Expert - DAX, Power Query, data modeling, RLS, performance optimization\n- **Tableau**: Proficient - Dashboard design, calculated fields, parameters\n- **Excel**: Advanced - Power Pivot, Power Query, VBA, complex formulas\n\n### Cloud & Data Platforms\n- **Azure**: Synapse, Data Factory, Databricks, Azure SQL\n- **Snowflake**: Data warehousing, q",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 46
    },
    "id": "tejuu_tejuu_experience_486"
  },
  {
    "text": "eters\n- **Excel**: Advanced - Power Pivot, Power Query, VBA, complex formulas\n\n### Cloud & Data Platforms\n- **Azure**: Synapse, Data Factory, Databricks, Azure SQL\n- **Snowflake**: Data warehousing, query optimization\n- **AWS**: Basic familiarity with S3, Redshift\n- **dbt**: Data transformation, testing, documentation\n\n### Data & Business Skills\n- Data migration and ETL/ELT\n- KPI definition and governance\n- Dimensional modeling (star and snowflake schemas)\n- Forecasting and predictive analytics\n- Data quality and validation\n- Data lineage and documentation\n- Requirements gathering\n- Stakeholder management\n- UAT and testing\n\n## Education\n\n**MS Data Science**\nUniversity of North Texas, Denton, TX\n\n**B.Tech Information Technology**\nJNTU, Hyderabad, India\n\n## Certifications\n\n- **Scrum Alliance** — Certified Scrum Product Owner\n- **Google** — Business Intelligence Professional\n- **Microsoft** — Azure Data Engineer\n\n## Key Achievements & Metrics\n\n### Efficiency & Savings\n- Saved 25-30 hours ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 47
    },
    "id": "tejuu_tejuu_experience_487"
  },
  {
    "text": "** — Certified Scrum Product Owner\n- **Google** — Business Intelligence Professional\n- **Microsoft** — Azure Data Engineer\n\n## Key Achievements & Metrics\n\n### Efficiency & Savings\n- Saved 25-30 hours monthly through automation\n- Cut manual reconciliation by 40%\n- Reduced refresh runtime by 45%\n- Reduced compute spend by 22%\n\n### Scale & Volume\n- Reconciled 12M+ legacy rows\n- Built pipelines for 15M+ transactions\n- Supported 1K+ field users\n- Processed 10+ TB monthly\n\n### Impact & Adoption\n- Improved reporting efficiency by 25%\n- Reduced ad-hoc requests by 30%\n- Boosted dashboard adoption across regions\n- Consolidated 30% of manual spreadsheets\n\n### Financial Impact\n- Uncovered $2M+ revenue opportunity\n- Cut ad-hoc requests by 32%\n- Improved quarterly close by multiple days\n- Reduced operational costs significantly\n\n### Accuracy & Quality\n- Improved metric accuracy by 20%\n- Reduced discrepancies by 30%\n- Improved trust in KPIs across organization\n\n## Interview Talking Points\n\n### Proble",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 48
    },
    "id": "tejuu_tejuu_experience_488"
  },
  {
    "text": "ional costs significantly\n\n### Accuracy & Quality\n- Improved metric accuracy by 20%\n- Reduced discrepancies by 30%\n- Improved trust in KPIs across organization\n\n## Interview Talking Points\n\n### Problem-Solving Examples\n\n**Challenge: Legacy System Migration**\nSo at CVS Health, we had this huge challenge where all the pharmacy reporting was in Access databases and Excel spreadsheets. The data was inconsistent, reports were breaking, and analysts were spending hours manually reconciling numbers. What I did was design a migration strategy to move everything to Azure SQL and Power BI. I wrote Python scripts to validate the data during migration, and we reconciled over 10 million records. The result was that we reduced manual work by 40% and improved data accuracy significantly.\n\n**Challenge: Performance Optimization**\nAt Stryker, we had these ELT pipelines that were taking forever to run - like 2-3 hours for a single refresh. This was causing delays in reporting and frustrating users. I ana",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 49
    },
    "id": "tejuu_tejuu_experience_489"
  },
  {
    "text": "rformance Optimization**\nAt Stryker, we had these ELT pipelines that were taking forever to run - like 2-3 hours for a single refresh. This was causing delays in reporting and frustrating users. I analyzed the execution plans, identified bottlenecks, and implemented partitioning strategies in Databricks. I also optimized the orchestration in Azure Data Factory. The result was that we cut the runtime by 45% - from 2-3 hours down to about an hour. This also reduced our compute costs significantly.\n\n**Challenge: Stakeholder Alignment**\nOne challenge I faced at Central Bank was getting buy-in from different departments for the migration project. Finance wanted things one way, audit had different requirements, and risk had their own needs. What I did was organize workshops with each group, documented their requirements, and found common ground. I created a standardized KPI framework that met everyone's needs. The result was successful adoption across all departments and a 30% reduction in a",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 50
    },
    "id": "tejuu_tejuu_experience_490"
  },
  {
    "text": "mented their requirements, and found common ground. I created a standardized KPI framework that met everyone's needs. The result was successful adoption across all departments and a 30% reduction in ad-hoc reporting requests.\n\n### Technical Strengths\n\n**Power BI & DAX**\nI'm really strong in Power BI and DAX. I've built over 50 dashboards across different business functions. I know how to optimize DAX formulas for performance, implement RLS for security, and design semantic models that are easy for business users to understand.\n\n**SQL & Data Modeling**\nSQL is my bread and butter. I write complex queries daily, including window functions, CTEs, and performance-tuned joins. I'm experienced in dimensional modeling - both star and snowflake schemas. I know how to design data warehouses that are optimized for analytics.\n\n**Data Migration**\nI've led multiple large-scale data migration projects. I know how to plan migrations, validate data, handle edge cases, and ensure business continuity. I ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 51
    },
    "id": "tejuu_tejuu_experience_491"
  },
  {
    "text": " optimized for analytics.\n\n**Data Migration**\nI've led multiple large-scale data migration projects. I know how to plan migrations, validate data, handle edge cases, and ensure business continuity. I always write reconciliation scripts to make sure no data is lost and everything matches.\n\n**Stakeholder Management**\nI'm good at working with non-technical stakeholders. I can translate business requirements into technical specifications and explain technical concepts in business terms. I've worked with executives, finance teams, operations, and IT across multiple companies.\n\n## Work Style & Approach\n\n### My Philosophy\nI believe in building things that last. When I design a dashboard or data model, I think about maintainability, scalability, and documentation. I don't just deliver a solution - I make sure the team can maintain it after I'm done.\n\n### Collaboration\nI work well in cross-functional teams. I've collaborated with data engineers, software developers, business analysts, and execu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 52
    },
    "id": "tejuu_tejuu_experience_492"
  },
  {
    "text": " I make sure the team can maintain it after I'm done.\n\n### Collaboration\nI work well in cross-functional teams. I've collaborated with data engineers, software developers, business analysts, and executives. I believe in clear communication, documentation, and knowledge sharing.\n\n### Continuous Learning\nI'm always learning new tools and techniques. I recently learned dbt and have been exploring more advanced Azure services. I believe in staying current with industry trends and best practices.\n\n### Problem-Solving Approach\nWhen I face a problem, I start by understanding the root cause. I gather requirements, analyze the data, and design a solution. I believe in iterative development - start with an MVP, get feedback, and improve. I also believe in testing thoroughly before production deployment.\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 53
    },
    "id": "tejuu_tejuu_experience_493"
  },
  {
    "text": "---\ntags: [business-intelligence, power-bi, tableau, data-visualization, dashboards, analytics]\npersona: bi\n---\n\n# Business Intelligence Development & Tejuu's Experience\n\n## Power BI Development\n\n### Dashboard Design and Development\n**Tejuu's Power BI Expertise:**\nI've built over 50 Power BI dashboards across different business functions - sales, finance, operations, and HR. What I've learned is that a good dashboard tells a story and helps users make decisions quickly.\n\n**My Dashboard Design Principles:**\n```\n1. Know Your Audience\n   - Executives need high-level KPIs\n   - Managers need drill-down capabilities\n   - Analysts need detailed data access\n\n2. Follow the 5-Second Rule\n   - Key insights should be visible in 5 seconds\n   - Use clear titles and labels\n   - Highlight important metrics\n\n3. Use Visual Hierarchy\n   - Most important metrics at the top\n   - Use size and color to guide attention\n   - Group related metrics together\n\n4. Keep It Simple\n   - Maximum 5-7 visuals per page\n  ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 0
    },
    "id": "tejuu_bi_development_494"
  },
  {
    "text": "s\n\n3. Use Visual Hierarchy\n   - Most important metrics at the top\n   - Use size and color to guide attention\n   - Group related metrics together\n\n4. Keep It Simple\n   - Maximum 5-7 visuals per page\n   - Avoid chart junk and unnecessary decorations\n   - Use consistent colors and formatting\n\n5. Enable Interactivity\n   - Add filters and slicers\n   - Enable drill-through pages\n   - Use bookmarks for different views\n```\n\n**Sales Dashboard Example:**\n```\nPage 1: Executive Overview\n- Total Revenue (Card visual with YoY comparison)\n- Revenue Trend (Line chart with forecast)\n- Revenue by Region (Map visual)\n- Top 10 Products (Bar chart)\n- Key Metrics Table (Revenue, Orders, Avg Order Value)\n\nPage 2: Sales Performance\n- Sales by Sales Rep (Matrix with conditional formatting)\n- Sales vs Target (Gauge charts)\n- Win Rate by Product Category (Funnel chart)\n- Pipeline Analysis (Waterfall chart)\n\nPage 3: Customer Analysis\n- Customer Segmentation (Scatter plot)\n- Customer Lifetime Value (Tree map)\n- Ch",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 1
    },
    "id": "tejuu_bi_development_495"
  },
  {
    "text": " charts)\n- Win Rate by Product Category (Funnel chart)\n- Pipeline Analysis (Waterfall chart)\n\nPage 3: Customer Analysis\n- Customer Segmentation (Scatter plot)\n- Customer Lifetime Value (Tree map)\n- Churn Analysis (Line and column chart)\n- Customer Acquisition Cost (KPI visual)\n```\n\n### DAX Formulas and Calculations\n**Tejuu's DAX Knowledge:**\nDAX is essential for creating powerful calculations in Power BI. Here are some of my most-used formulas:\n\n**Time Intelligence:**\n```dax\n-- Year-to-Date Sales\nYTD Sales = \nTOTALYTD(\n    SUM(Sales[Amount]),\n    'Date'[Date]\n)\n\n-- Previous Year Sales\nPY Sales = \nCALCULATE(\n    SUM(Sales[Amount]),\n    SAMEPERIODLASTYEAR('Date'[Date])\n)\n\n-- Year-over-Year Growth\nYoY Growth % = \nDIVIDE(\n    [YTD Sales] - [PY Sales],\n    [PY Sales],\n    0\n)\n\n-- Moving Average (3 months)\n3M Moving Avg = \nAVERAGEX(\n    DATESINPERIOD(\n        'Date'[Date],\n        LASTDATE('Date'[Date]),\n        -3,\n        MONTH\n    ),\n    [Total Sales]\n)\n```\n\n**Customer Metrics:**\n```dax\n-",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 2
    },
    "id": "tejuu_bi_development_496"
  },
  {
    "text": " (3 months)\n3M Moving Avg = \nAVERAGEX(\n    DATESINPERIOD(\n        'Date'[Date],\n        LASTDATE('Date'[Date]),\n        -3,\n        MONTH\n    ),\n    [Total Sales]\n)\n```\n\n**Customer Metrics:**\n```dax\n-- Customer Count\nTotal Customers = \nDISTINCTCOUNT(Sales[CustomerID])\n\n-- New Customers\nNew Customers = \nCALCULATE(\n    DISTINCTCOUNT(Sales[CustomerID]),\n    FILTER(\n        ALL(Sales),\n        Sales[OrderDate] = \n        CALCULATE(\n            MIN(Sales[OrderDate]),\n            ALLEXCEPT(Sales, Sales[CustomerID])\n        )\n    )\n)\n\n-- Customer Retention Rate\nRetention Rate = \nVAR CurrentCustomers = [Total Customers]\nVAR PreviousCustomers = \n    CALCULATE(\n        [Total Customers],\n        DATEADD('Date'[Date], -1, MONTH)\n    )\nVAR RetainedCustomers = \n    CALCULATE(\n        DISTINCTCOUNT(Sales[CustomerID]),\n        FILTER(\n            ALL(Sales),\n            [CustomerID] IN VALUES(Sales[CustomerID]) &&\n            [OrderDate] >= EOMONTH(TODAY(), -2) &&\n            [OrderDate] < EOMONTH(TO",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 3
    },
    "id": "tejuu_bi_development_497"
  },
  {
    "text": "s[CustomerID]),\n        FILTER(\n            ALL(Sales),\n            [CustomerID] IN VALUES(Sales[CustomerID]) &&\n            [OrderDate] >= EOMONTH(TODAY(), -2) &&\n            [OrderDate] < EOMONTH(TODAY(), -1)\n        )\n    )\nRETURN\nDIVIDE(RetainedCustomers, PreviousCustomers, 0)\n```\n\n**Advanced Calculations:**\n```dax\n-- Running Total\nRunning Total = \nCALCULATE(\n    SUM(Sales[Amount]),\n    FILTER(\n        ALL('Date'[Date]),\n        'Date'[Date] <= MAX('Date'[Date])\n    )\n)\n\n-- Pareto Analysis (80/20 Rule)\nCumulative % = \nVAR CurrentRevenue = SUM(Sales[Amount])\nVAR TotalRevenue = \n    CALCULATE(\n        SUM(Sales[Amount]),\n        ALL(Products)\n    )\nVAR RunningTotal = \n    CALCULATE(\n        SUM(Sales[Amount]),\n        FILTER(\n            ALL(Products),\n            RANKX(\n                ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n                DESC\n            ) <= RANKX(\n                ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 4
    },
    "id": "tejuu_bi_development_498"
  },
  {
    "text": "ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n                DESC\n            ) <= RANKX(\n                ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n                DESC\n            )\n        )\n    )\nRETURN\nDIVIDE(RunningTotal, TotalRevenue, 0)\n\n-- Dynamic Ranking\nProduct Rank = \nRANKX(\n    ALL(Products[ProductName]),\n    [Total Sales],\n    ,\n    DESC,\n    DENSE\n)\n```\n\n### Data Modeling in Power BI\n**Tejuu's Data Modeling Approach:**\nGood data modeling is the foundation of a fast and reliable Power BI report. Here's my approach:\n\n**Star Schema Design:**\n```\nFact Tables:\n- Sales (OrderID, CustomerID, ProductID, DateKey, Amount, Quantity)\n- Inventory (ProductID, DateKey, StockLevel, WarehouseID)\n- Budget (DateKey, DepartmentID, BudgetAmount)\n\nDimension Tables:\n- Customers (CustomerID, Name, Segment, Region, City)\n- Products (ProductID, Name, Category, SubCategory, Price)\n- Date (DateKey, Date, Year, Quarter, Month, Week, Day)\n- Employees ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 5
    },
    "id": "tejuu_bi_development_499"
  },
  {
    "text": "\nDimension Tables:\n- Customers (CustomerID, Name, Segment, Region, City)\n- Products (ProductID, Name, Category, SubCategory, Price)\n- Date (DateKey, Date, Year, Quarter, Month, Week, Day)\n- Employees (EmployeeID, Name, Department, Manager, HireDate)\n```\n\n**Relationship Best Practices:**\n```\n1. Use Star Schema\n   - Fact tables in the center\n   - Dimension tables around it\n   - One-to-many relationships\n\n2. Create a Date Table\n   - Use CALENDAR or CALENDARAUTO\n   - Mark as date table\n   - Include all needed date attributes\n\n3. Avoid Bi-Directional Relationships\n   - Can cause ambiguity\n   - Impacts performance\n   - Use DAX instead when possible\n\n4. Hide Unnecessary Columns\n   - Foreign keys\n   - Technical columns\n   - Improves user experience\n\n5. Use Calculated Columns vs Measures Appropriately\n   - Calculated columns: Static, stored in model\n   - Measures: Dynamic, calculated at query time\n   - Prefer measures for aggregations\n```\n\n## Tableau Development\n\n### Building Interactive Dashbo",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 6
    },
    "id": "tejuu_bi_development_500"
  },
  {
    "text": "ely\n   - Calculated columns: Static, stored in model\n   - Measures: Dynamic, calculated at query time\n   - Prefer measures for aggregations\n```\n\n## Tableau Development\n\n### Building Interactive Dashboards\n**Tejuu's Tableau Experience:**\nI've worked with Tableau for 3+ years, building executive dashboards and self-service analytics solutions. Tableau's strength is its intuitive drag-and-drop interface and powerful visualization capabilities.\n\n**My Tableau Dashboard Structure:**\n```\n1. Overview Dashboard\n   - KPI Summary\n   - Trend Analysis\n   - Geographic Distribution\n   - Quick Filters\n\n2. Detailed Analysis\n   - Drill-down capabilities\n   - Parameter controls\n   - Calculated fields\n   - Reference lines and bands\n\n3. What-If Analysis\n   - Parameters for scenarios\n   - Calculated fields for projections\n   - Dynamic titles and labels\n```\n\n**Calculated Fields in Tableau:**\n```\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / LOOKUP(SUM([Sales]), -12)\n\n// Customer Lifet",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 7
    },
    "id": "tejuu_bi_development_501"
  },
  {
    "text": "projections\n   - Dynamic titles and labels\n```\n\n**Calculated Fields in Tableau:**\n```\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / LOOKUP(SUM([Sales]), -12)\n\n// Customer Lifetime Value\n{FIXED [Customer ID]: SUM([Sales])}\n\n// Cohort Analysis\nIF DATEDIFF('month', {FIXED [Customer ID]: MIN([Order Date])}, [Order Date]) = 0\nTHEN \"Month 0\"\nELSEIF DATEDIFF('month', {FIXED [Customer ID]: MIN([Order Date])}, [Order Date]) = 1\nTHEN \"Month 1\"\nELSE \"Month 2+\"\nEND\n\n// Dynamic Ranking\nRANK(SUM([Sales]), 'desc')\n\n// Moving Average\nWINDOW_AVG(SUM([Sales]), -2, 0)\n```\n\n## Excel for Business Intelligence\n\n### Advanced Excel Techniques\n**Tejuu's Excel Expertise:**\nEven with Power BI and Tableau, Excel remains a critical tool. I use it for ad-hoc analysis, data preparation, and quick reports.\n\n**Power Query (ETL in Excel):**\n```\nCommon Transformations:\n1. Remove Duplicates\n2. Fill Down/Up\n3. Pivot/Unpivot Columns\n4. Merge Queries (Joins)\n5. Append Queries (Union)\n6. Split Columns",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 8
    },
    "id": "tejuu_bi_development_502"
  },
  {
    "text": " reports.\n\n**Power Query (ETL in Excel):**\n```\nCommon Transformations:\n1. Remove Duplicates\n2. Fill Down/Up\n3. Pivot/Unpivot Columns\n4. Merge Queries (Joins)\n5. Append Queries (Union)\n6. Split Columns\n7. Change Data Types\n8. Add Custom Columns\n\nM Language Examples:\n// Add custom column\n= Table.AddColumn(#\"Previous Step\", \"Full Name\", \n    each [First Name] & \" \" & [Last Name])\n\n// Filter rows\n= Table.SelectRows(#\"Previous Step\", \n    each [Sales] > 1000)\n\n// Group by\n= Table.Group(#\"Previous Step\", {\"Region\"}, \n    {{\"Total Sales\", each List.Sum([Sales]), type number}})\n```\n\n**Power Pivot and Data Modeling:**\n```\nDAX in Excel:\n// Total Sales\nTotal Sales:=SUM(Sales[Amount])\n\n// Sales vs Budget Variance\nVariance:=[Total Sales]-[Total Budget]\n\n// Variance %\nVariance %:=DIVIDE([Variance],[Total Budget],0)\n\n// Top N Products\nTop 10 Products:=\nCALCULATE(\n    [Total Sales],\n    TOPN(10, ALL(Products[Product Name]), [Total Sales], DESC)\n)\n```\n\n**Advanced Formulas:**\n```excel\n// Dynamic Named R",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 9
    },
    "id": "tejuu_bi_development_503"
  },
  {
    "text": "l Budget],0)\n\n// Top N Products\nTop 10 Products:=\nCALCULATE(\n    [Total Sales],\n    TOPN(10, ALL(Products[Product Name]), [Total Sales], DESC)\n)\n```\n\n**Advanced Formulas:**\n```excel\n// Dynamic Named Ranges\n=OFFSET(Sheet1!$A$1,0,0,COUNTA(Sheet1!$A:$A),1)\n\n// Array Formulas (Ctrl+Shift+Enter)\n=SUM(IF(A2:A100=\"Yes\",B2:B100,0))\n\n// XLOOKUP (Modern Excel)\n=XLOOKUP(A2,Table1[ID],Table1[Name],\"Not Found\",0,1)\n\n// SUMIFS with Multiple Criteria\n=SUMIFS(Sales[Amount],Sales[Region],\"West\",Sales[Date],\">=\"&DATE(2023,1,1))\n\n// Dynamic Dashboard with CHOOSE and MATCH\n=CHOOSE(MATCH(B1,{\"Sales\",\"Profit\",\"Quantity\"},0),\n    SUM(Sales[Amount]),\n    SUM(Sales[Profit]),\n    SUM(Sales[Quantity]))\n```\n\n## Data Visualization Best Practices\n\n### Choosing the Right Chart\n**Tejuu's Chart Selection Guide:**\n```\nComparison:\n- Bar Chart: Compare values across categories\n- Column Chart: Show changes over time\n- Bullet Chart: Compare actual vs target\n\nComposition:\n- Pie Chart: Show parts of a whole (max 5 slices)\n- ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 10
    },
    "id": "tejuu_bi_development_504"
  },
  {
    "text": "arison:\n- Bar Chart: Compare values across categories\n- Column Chart: Show changes over time\n- Bullet Chart: Compare actual vs target\n\nComposition:\n- Pie Chart: Show parts of a whole (max 5 slices)\n- Stacked Bar: Show composition across categories\n- Treemap: Show hierarchical composition\n\nDistribution:\n- Histogram: Show frequency distribution\n- Box Plot: Show statistical distribution\n- Scatter Plot: Show correlation between variables\n\nTrend:\n- Line Chart: Show trends over time\n- Area Chart: Show cumulative trends\n- Waterfall: Show sequential changes\n\nRelationship:\n- Scatter Plot: Show correlation\n- Bubble Chart: Show 3 dimensions\n- Heat Map: Show patterns in matrix data\n```\n\n### Color Theory and Accessibility\n**My Color Guidelines:**\n```\n1. Use Color Purposefully\n   - Red: Negative, danger, stop\n   - Green: Positive, success, go\n   - Blue: Neutral, information\n   - Orange/Yellow: Warning, attention\n\n2. Ensure Accessibility\n   - Check color blindness compatibility\n   - Use patterns in a",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 11
    },
    "id": "tejuu_bi_development_505"
  },
  {
    "text": ", stop\n   - Green: Positive, success, go\n   - Blue: Neutral, information\n   - Orange/Yellow: Warning, attention\n\n2. Ensure Accessibility\n   - Check color blindness compatibility\n   - Use patterns in addition to colors\n   - Maintain sufficient contrast\n   - Test with grayscale\n\n3. Limit Color Palette\n   - Maximum 5-6 colors per dashboard\n   - Use shades for variations\n   - Keep brand colors consistent\n\n4. Highlight What Matters\n   - Use bright colors for important data\n   - Gray out less important information\n   - Use white space effectively\n```\n\n## ETL and Data Preparation\n\n### Data Quality Checks\n**Tejuu's Data Validation Process:**\n```\n1. Completeness Checks\n   - Missing values\n   - Null percentages\n   - Required fields populated\n\n2. Accuracy Checks\n   - Data type validation\n   - Range checks (min/max)\n   - Format validation (email, phone)\n\n3. Consistency Checks\n   - Cross-field validation\n   - Referential integrity\n   - Duplicate detection\n\n4. Timeliness Checks\n   - Data freshness\n ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 12
    },
    "id": "tejuu_bi_development_506"
  },
  {
    "text": "hecks (min/max)\n   - Format validation (email, phone)\n\n3. Consistency Checks\n   - Cross-field validation\n   - Referential integrity\n   - Duplicate detection\n\n4. Timeliness Checks\n   - Data freshness\n   - Update frequency\n   - Historical completeness\n\nSQL for Data Quality:\n-- Check for nulls\nSELECT \n    COUNT(*) as total_records,\n    COUNT(customer_id) as non_null_customers,\n    COUNT(*) - COUNT(customer_id) as null_count,\n    ROUND((COUNT(*) - COUNT(customer_id)) * 100.0 / COUNT(*), 2) as null_percentage\nFROM sales;\n\n-- Check for duplicates\nSELECT \n    order_id,\n    COUNT(*) as duplicate_count\nFROM orders\nGROUP BY order_id\nHAVING COUNT(*) > 1;\n\n-- Check data ranges\nSELECT \n    MIN(order_date) as earliest_date,\n    MAX(order_date) as latest_date,\n    MIN(amount) as min_amount,\n    MAX(amount) as max_amount,\n    AVG(amount) as avg_amount\nFROM orders;\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- Power BI dashboard development and DAX\n- Tableau visualization and calculated fie",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 13
    },
    "id": "tejuu_bi_development_507"
  },
  {
    "text": "t) as max_amount,\n    AVG(amount) as avg_amount\nFROM orders;\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- Power BI dashboard development and DAX\n- Tableau visualization and calculated fields\n- Advanced Excel (Power Query, Power Pivot)\n- SQL for data analysis\n- Data modeling and ETL\n- Data visualization best practices\n\n### Tools & Technologies:\n- **BI Tools**: Power BI, Tableau, Qlik Sense\n- **Databases**: SQL Server, Oracle, MySQL, PostgreSQL\n- **Excel**: Power Query, Power Pivot, VBA\n- **ETL**: SSIS, Alteryx, Informatica\n- **Cloud**: Azure, AWS, Google Cloud\n\n### Achievements:\n- Built 50+ Power BI dashboards used by 200+ users\n- Reduced report generation time from 2 days to 2 hours\n- Improved data accuracy by 95% through automated quality checks\n- Saved $100K annually by identifying cost optimization opportunities\n- Trained 30+ business users on self-service BI tools\n\n### Project Examples:\n- Sales Performance Dashboard (real-time tracking, 15+ KPIs)\n- Customer Analytics P",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 14
    },
    "id": "tejuu_bi_development_508"
  },
  {
    "text": "tifying cost optimization opportunities\n- Trained 30+ business users on self-service BI tools\n\n### Project Examples:\n- Sales Performance Dashboard (real-time tracking, 15+ KPIs)\n- Customer Analytics Platform (segmentation, churn prediction)\n- Financial Reporting Suite (P&L, balance sheet, cash flow)\n- Operational Metrics Dashboard (efficiency, productivity, quality)\n- Executive KPI Dashboard (company-wide metrics, drill-down)\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 15
    },
    "id": "tejuu_bi_development_509"
  },
  {
    "text": "---\ntags: [tableau, data-visualization, calculated-fields, parameters, dashboards, lod]\npersona: tejuu\n---\n\n# Tableau Expertise - Tejuu's Experience\n\n## Tableau Dashboard Development\n\n### Building Executive Dashboards\n**Tejuu's Approach:**\nSo I've built numerous executive dashboards in Tableau, and what I've learned is that executives want to see the story quickly. They don't have time to dig through data - they need insights at a glance.\n\n**My Executive Dashboard Structure:**\n```\nPage 1: Overview\n- KPI Summary Cards (Revenue, Profit, Growth %)\n- Trend Line (12-month rolling)\n- Geographic Heat Map\n- Top 10 Products/Customers\n- Quick Filters (Date, Region, Category)\n\nPage 2: Deep Dive\n- Detailed Tables with Drill-Down\n- Comparison Charts (Actual vs Target vs Prior Year)\n- Distribution Analysis\n- Cohort Analysis\n\nPage 3: What-If Analysis\n- Parameter Controls for Scenarios\n- Calculated Projections\n- Sensitivity Analysis\n```\n\n### Interactive Features I Implement\n**Navigation and Interactiv",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 16
    },
    "id": "tejuu_tableau_expertise_510"
  },
  {
    "text": "sis\n- Cohort Analysis\n\nPage 3: What-If Analysis\n- Parameter Controls for Scenarios\n- Calculated Projections\n- Sensitivity Analysis\n```\n\n### Interactive Features I Implement\n**Navigation and Interactivity:**\n\n```\n1. Dashboard Actions:\n   - Filter Actions: Click on region to filter all visuals\n   - Highlight Actions: Hover to highlight related data\n   - URL Actions: Link to external systems\n   - Parameter Actions: Dynamic parameter updates\n\n2. Navigation:\n   - Button navigation between pages\n   - Breadcrumb navigation\n   - Back button functionality\n   - Reset filters button\n\n3. Tooltips:\n   - Custom tooltip worksheets\n   - Show additional context\n   - Mini visualizations in tooltips\n   - Formatted text with key metrics\n```\n\n## Advanced Calculated Fields\n\n### LOD (Level of Detail) Expressions\n**Tejuu's Most-Used LOD Patterns:**\n\n```tableau\n// FIXED - Customer Lifetime Value\n{FIXED [Customer ID]: SUM([Sales])}\n\n// INCLUDE - Sales with Category Context\n{INCLUDE [Category]: AVG([Sales])}\n\n//",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 17
    },
    "id": "tejuu_tableau_expertise_511"
  },
  {
    "text": "\n**Tejuu's Most-Used LOD Patterns:**\n\n```tableau\n// FIXED - Customer Lifetime Value\n{FIXED [Customer ID]: SUM([Sales])}\n\n// INCLUDE - Sales with Category Context\n{INCLUDE [Category]: AVG([Sales])}\n\n// EXCLUDE - Overall Average Excluding Region\n{EXCLUDE [Region]: AVG([Sales])}\n\n// Customer First Purchase Date\n{FIXED [Customer ID]: MIN([Order Date])}\n\n// Customer Cohort Analysis\n// Cohort Month\n{FIXED [Customer ID]: \n    MIN(DATETRUNC('month', [Order Date]))}\n\n// Months Since First Purchase\nDATEDIFF('month', \n    {FIXED [Customer ID]: MIN([Order Date])},\n    [Order Date]\n)\n\n// Percent of Total by Category\nSUM([Sales]) / \n{FIXED [Category]: SUM([Sales])}\n\n// Running Total by Customer\nRUNNING_SUM(SUM([Sales]))\n\n// Rank within Category\nRANK(SUM([Sales]), 'desc')\n\n// Dense Rank for Top N\nRANK_DENSE(SUM([Sales]), 'desc')\n```\n\n### Table Calculations\n**My Frequently Used Table Calcs:**\n\n```tableau\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / \nLOOKUP(SUM([Sales]), -12)\n\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 18
    },
    "id": "tejuu_tableau_expertise_512"
  },
  {
    "text": "NSE(SUM([Sales]), 'desc')\n```\n\n### Table Calculations\n**My Frequently Used Table Calcs:**\n\n```tableau\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / \nLOOKUP(SUM([Sales]), -12)\n\n// Moving Average (3 months)\nWINDOW_AVG(SUM([Sales]), -2, 0)\n\n// Percent of Total\nSUM([Sales]) / TOTAL(SUM([Sales]))\n\n// Running Total\nRUNNING_SUM(SUM([Sales]))\n\n// Percent Difference from Average\n(SUM([Sales]) - WINDOW_AVG(SUM([Sales]))) / \nWINDOW_AVG(SUM([Sales]))\n\n// Index for Row Numbering\nINDEX()\n\n// First/Last Value in Partition\nFIRST() = 0  // First row\nLAST() = 0   // Last row\n\n// Lookup Previous Value\nLOOKUP(SUM([Sales]), -1)\n\n// Window Sum for Cumulative %\nRUNNING_SUM(SUM([Sales])) / \nTOTAL(SUM([Sales]))\n```\n\n### Complex Business Logic\n**Healthcare Analytics at Stryker:**\n\n```tableau\n// Patient Risk Stratification\nIF [Total Claims] > 50000 AND [Chronic Conditions] >= 3 THEN \"High Risk\"\nELSEIF [Total Claims] > 25000 OR [Chronic Conditions] >= 2 THEN \"Medium Risk\"\nELSE \"Low Risk\"\nE",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 19
    },
    "id": "tejuu_tableau_expertise_513"
  },
  {
    "text": "\n// Patient Risk Stratification\nIF [Total Claims] > 50000 AND [Chronic Conditions] >= 3 THEN \"High Risk\"\nELSEIF [Total Claims] > 25000 OR [Chronic Conditions] >= 2 THEN \"Medium Risk\"\nELSE \"Low Risk\"\nEND\n\n// Readmission Flag\nIF DATEDIFF('day', [Previous Discharge Date], [Admission Date]) <= 30\nTHEN \"30-Day Readmission\"\nELSE \"New Admission\"\nEND\n\n// Length of Stay Category\nCASE [Length of Stay]\n    WHEN 0 THEN \"Same Day\"\n    WHEN 1 THEN \"Overnight\"\n    WHEN 2 THEN \"2 Days\"\n    WHEN 3 THEN \"3 Days\"\n    ELSE \"Extended Stay (4+ days)\"\nEND\n\n// Cost per Day\n[Total Cost] / [Length of Stay]\n\n// Case Mix Adjusted Cost\n[Total Cost] / [Case Mix Index]\n\n// Utilization Rate\n[Actual Bed Days] / [Available Bed Days]\n```\n\n## Parameters and What-If Analysis\n\n### Dynamic Parameters\n**Tejuu's Parameter Strategies:**\n\n```tableau\n// Top N Parameter\nParameter: Top N (Integer, Range 5-20, Default 10)\n\nCalculated Field: Top N Filter\nRANK(SUM([Sales]), 'desc') <= [Top N Parameter]\n\n// Date Range Parameter\nParame",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 20
    },
    "id": "tejuu_tableau_expertise_514"
  },
  {
    "text": "ies:**\n\n```tableau\n// Top N Parameter\nParameter: Top N (Integer, Range 5-20, Default 10)\n\nCalculated Field: Top N Filter\nRANK(SUM([Sales]), 'desc') <= [Top N Parameter]\n\n// Date Range Parameter\nParameter: Date Range (String, List)\n- Last 7 Days\n- Last 30 Days\n- Last 90 Days\n- Last 12 Months\n- Year to Date\n- Custom\n\nCalculated Field: Date Filter\nCASE [Date Range Parameter]\n    WHEN \"Last 7 Days\" THEN [Order Date] >= TODAY() - 7\n    WHEN \"Last 30 Days\" THEN [Order Date] >= TODAY() - 30\n    WHEN \"Last 90 Days\" THEN [Order Date] >= TODAY() - 90\n    WHEN \"Last 12 Months\" THEN [Order Date] >= DATEADD('month', -12, TODAY())\n    WHEN \"Year to Date\" THEN YEAR([Order Date]) = YEAR(TODAY())\nEND\n\n// Metric Selector Parameter\nParameter: Select Metric (String, List)\n- Sales\n- Profit\n- Quantity\n- Avg Order Value\n\nCalculated Field: Selected Metric\nCASE [Select Metric Parameter]\n    WHEN \"Sales\" THEN SUM([Sales])\n    WHEN \"Profit\" THEN SUM([Profit])\n    WHEN \"Quantity\" THEN SUM([Quantity])\n    WHEN \"Av",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 21
    },
    "id": "tejuu_tableau_expertise_515"
  },
  {
    "text": "er Value\n\nCalculated Field: Selected Metric\nCASE [Select Metric Parameter]\n    WHEN \"Sales\" THEN SUM([Sales])\n    WHEN \"Profit\" THEN SUM([Profit])\n    WHEN \"Quantity\" THEN SUM([Quantity])\n    WHEN \"Avg Order Value\" THEN AVG([Order Value])\nEND\n\n// Growth Rate Parameter for Projections\nParameter: Growth Rate (Float, Range 0-0.5, Default 0.1)\n\nCalculated Field: Projected Sales\nSUM([Sales]) * (1 + [Growth Rate Parameter])\n```\n\n### Scenario Analysis\n**What-If Modeling:**\n\n```tableau\n// Price Elasticity Scenario\nParameter: Price Change % (Float, -50% to +50%)\n\nCalculated Field: New Price\n[Current Price] * (1 + [Price Change %])\n\nCalculated Field: Projected Demand\n// Assuming elasticity of -1.5\n[Current Demand] * POWER(\n    (1 + [Price Change %]),\n    -1.5\n)\n\nCalculated Field: Projected Revenue\n[New Price] * [Projected Demand]\n\n// Staffing Scenario\nParameter: Additional Staff (Integer, 0-50)\n\nCalculated Field: Total Staff\n[Current Staff] + [Additional Staff]\n\nCalculated Field: Projected Capac",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 22
    },
    "id": "tejuu_tableau_expertise_516"
  },
  {
    "text": "New Price] * [Projected Demand]\n\n// Staffing Scenario\nParameter: Additional Staff (Integer, 0-50)\n\nCalculated Field: Total Staff\n[Current Staff] + [Additional Staff]\n\nCalculated Field: Projected Capacity\n[Total Staff] * [Productivity per Staff]\n\nCalculated Field: Projected Cost\n([Current Staff] * [Current Wage]) + \n([Additional Staff] * [New Hire Wage])\n\n// Budget Allocation Scenario\nParameter: Marketing % (Float, 0-1)\n\nCalculated Field: Marketing Budget\n[Total Budget] * [Marketing % Parameter]\n\nCalculated Field: Operations Budget\n[Total Budget] * (1 - [Marketing % Parameter])\n```\n\n## Data Blending and Relationships\n\n### Cross-Database Joins\n**My Approach at CVS Health:**\n\n```\nPrimary Data Source: SQL Server (Sales Data)\nSecondary Data Source: Excel (Target Data)\n\nBlending Strategy:\n1. Define relationships on common fields\n2. Use primary data source for main viz\n3. Bring in secondary data as needed\n4. Aggregate appropriately to avoid duplication\n\nExample:\n- Primary: fact_sales (Order D",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 23
    },
    "id": "tejuu_tableau_expertise_517"
  },
  {
    "text": "ine relationships on common fields\n2. Use primary data source for main viz\n3. Bring in secondary data as needed\n4. Aggregate appropriately to avoid duplication\n\nExample:\n- Primary: fact_sales (Order Date, Product ID, Sales)\n- Secondary: targets (Month, Product ID, Target)\n- Relationship: Product ID\n- Calculation: Actual vs Target %\n```\n\n### Data Source Filters\n**Performance Optimization:**\n\n```tableau\n// Extract Filters\n1. Date Range: Last 2 years only\n2. Status: Exclude \"Cancelled\" orders\n3. Amount: > $0\n\n// Context Filters\n- Region (if analyzing specific region)\n- Date Range (before other filters)\n\n// Data Source Filters\n- Exclude test data\n- Filter out inactive records\n- Remove PII if not needed\n```\n\n## Dashboard Performance Optimization\n\n### Tejuu's Performance Checklist\n**What I Do to Speed Up Dashboards:**\n\n```\n1. Data Source Optimization:\n   - Use extracts instead of live connections\n   - Filter data at source\n   - Aggregate data when possible\n   - Remove unused fields\n   - Use ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 24
    },
    "id": "tejuu_tableau_expertise_518"
  },
  {
    "text": " Speed Up Dashboards:**\n\n```\n1. Data Source Optimization:\n   - Use extracts instead of live connections\n   - Filter data at source\n   - Aggregate data when possible\n   - Remove unused fields\n   - Use appropriate data types\n\n2. Calculation Optimization:\n   - Avoid complex calculations in viz\n   - Use table calcs instead of row-level calcs\n   - Materialize calculations in extract\n   - Use FIXED LODs sparingly\n   - Avoid nested LODs\n\n3. Visual Optimization:\n   - Limit number of marks (< 10K per sheet)\n   - Use aggregated data\n   - Avoid high-cardinality dimensions\n   - Use filters to reduce data\n   - Limit dashboard complexity (< 10 sheets)\n\n4. Dashboard Design:\n   - Use containers efficiently\n   - Minimize dashboard actions\n   - Use sheet-specific filters\n   - Hide unused sheets\n   - Optimize images and logos\n\n5. Extract Optimization:\n   - Schedule refreshes during off-hours\n   - Use incremental refresh\n   - Partition large extracts\n   - Remove historical data if not needed\n```\n\n### Extr",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 25
    },
    "id": "tejuu_tableau_expertise_519"
  },
  {
    "text": "e images and logos\n\n5. Extract Optimization:\n   - Schedule refreshes during off-hours\n   - Use incremental refresh\n   - Partition large extracts\n   - Remove historical data if not needed\n```\n\n### Extract Refresh Strategies\n**My Implementation:**\n\n```\nFull Refresh:\n- Weekly on Sunday night\n- For dimension tables\n- For small fact tables\n\nIncremental Refresh:\n- Daily for large fact tables\n- Filter: [Date] > MAX([Date]) - 7\n- Keeps last 7 days refreshable\n\nHybrid Approach:\n- Incremental daily (Mon-Sat)\n- Full refresh weekly (Sunday)\n- Ensures data consistency\n```\n\n## Advanced Visualizations\n\n### Custom Chart Types\n**Tejuu's Favorite Advanced Viz:**\n\n```\n1. Waterfall Chart\n   - Show sequential changes\n   - Revenue bridge analysis\n   - Budget variance analysis\n\n2. Bullet Chart\n   - KPI performance vs target\n   - Show good/satisfactory/poor ranges\n   - Compact executive view\n\n3. Funnel Chart\n   - Conversion analysis\n   - Sales pipeline stages\n   - Patient journey analysis\n\n4. Sankey Diagram\n ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 26
    },
    "id": "tejuu_tableau_expertise_520"
  },
  {
    "text": "ce vs target\n   - Show good/satisfactory/poor ranges\n   - Compact executive view\n\n3. Funnel Chart\n   - Conversion analysis\n   - Sales pipeline stages\n   - Patient journey analysis\n\n4. Sankey Diagram\n   - Flow analysis\n   - Customer journey\n   - Resource allocation\n\n5. Box and Whisker Plot\n   - Distribution analysis\n   - Outlier detection\n   - Statistical analysis\n\n6. Pareto Chart\n   - 80/20 analysis\n   - Prioritization\n   - ABC analysis\n```\n\n### Custom Shapes and Icons\n**Visual Enhancement:**\n\n```\nCustom Shapes for:\n- Geographic maps (custom territories)\n- Process flow diagrams\n- Organization charts\n- Network diagrams\n\nIcon Usage:\n- KPI indicators (↑ ↓ ■)\n- Status indicators (✓ ✗ ⚠)\n- Category icons\n- Action buttons\n```\n\n## Tableau Server Administration\n\n### Publishing and Permissions\n**My Governance Model:**\n\n```\nFolder Structure:\n/Executive Dashboards\n  - Finance\n  - Operations\n  - Sales\n/Departmental Reports\n  - HR\n  - Marketing\n  - IT\n/Sandbox\n  - Development\n  - Testing\n\nPermissio",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 27
    },
    "id": "tejuu_tableau_expertise_521"
  },
  {
    "text": " Governance Model:**\n\n```\nFolder Structure:\n/Executive Dashboards\n  - Finance\n  - Operations\n  - Sales\n/Departmental Reports\n  - HR\n  - Marketing\n  - IT\n/Sandbox\n  - Development\n  - Testing\n\nPermission Levels:\n- Viewer: Can view only\n- Interactor: Can view and interact\n- Editor: Can edit and publish\n- Publisher: Full control\n\nRow-Level Security:\n- User filters in data source\n- Dynamic user-based filtering\n- Integration with AD groups\n```\n\n### Monitoring and Maintenance\n**What I Track:**\n\n```\nPerformance Metrics:\n- Dashboard load times\n- Extract refresh duration\n- Failed refreshes\n- User adoption rates\n- Most viewed dashboards\n\nMaintenance Tasks:\n- Weekly: Review failed refreshes\n- Monthly: Clean up unused content\n- Quarterly: Performance audit\n- Annually: User access review\n\nAlerts:\n- Extract refresh failures\n- Dashboard load time > 10s\n- Disk space warnings\n- License usage threshold\n```\n\n## Tableau Prep for ETL\n\n### Data Preparation Workflows\n**Tejuu's Prep Flows:**\n\n```\nCommon Transf",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 28
    },
    "id": "tejuu_tableau_expertise_522"
  },
  {
    "text": "act refresh failures\n- Dashboard load time > 10s\n- Disk space warnings\n- License usage threshold\n```\n\n## Tableau Prep for ETL\n\n### Data Preparation Workflows\n**Tejuu's Prep Flows:**\n\n```\nCommon Transformations:\n1. Clean Data\n   - Remove nulls\n   - Fix data types\n   - Standardize formats\n   - Remove duplicates\n\n2. Join Data\n   - Left/Right/Inner joins\n   - Union multiple sources\n   - Pivot/Unpivot\n\n3. Aggregate Data\n   - Group by dimensions\n   - Calculate summaries\n   - Create rollups\n\n4. Calculate Fields\n   - Derived columns\n   - Conditional logic\n   - Date calculations\n\n5. Output\n   - Tableau extract (.hyper)\n   - Database table\n   - CSV file\n```\n\n### Prep Best Practices\n**My Workflow Design:**\n\n```\n1. Modular Design\n   - Separate flows for different sources\n   - Reusable cleaning steps\n   - Clear naming conventions\n\n2. Documentation\n   - Add descriptions to steps\n   - Document business logic\n   - Note data quality issues\n\n3. Error Handling\n   - Check for nulls\n   - Validate data type",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 29
    },
    "id": "tejuu_tableau_expertise_523"
  },
  {
    "text": "  - Clear naming conventions\n\n2. Documentation\n   - Add descriptions to steps\n   - Document business logic\n   - Note data quality issues\n\n3. Error Handling\n   - Check for nulls\n   - Validate data types\n   - Handle edge cases\n   - Log errors\n\n4. Performance\n   - Filter early\n   - Aggregate when possible\n   - Limit row sampling in dev\n   - Optimize joins\n```\n\n## Interview Talking Points\n\n### Technical Achievements\n- Built 30+ Tableau dashboards for healthcare and retail\n- Implemented complex LOD calculations for patient analytics\n- Optimized dashboard performance from 45s to 5s load time\n- Created reusable templates reducing development time by 50%\n- Trained 20+ users on Tableau best practices\n\n### Problem-Solving Examples\n**Performance Issue:**\n\"At Stryker, we had this patient dashboard that was super slow - taking 45 seconds to load. I analyzed it and found we were using live connection to a large database with no filters. I created an extract with appropriate filters, moved some calcu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 30
    },
    "id": "tejuu_tableau_expertise_524"
  },
  {
    "text": "t was super slow - taking 45 seconds to load. I analyzed it and found we were using live connection to a large database with no filters. I created an extract with appropriate filters, moved some calculations to the data source, and reduced the number of marks. Got it down to 5 seconds.\"\n\n**Complex Calculation:**\n\"At CVS Health, we needed to calculate patient cohort retention over time. This required LOD expressions to identify first purchase date, then table calculations to track behavior over subsequent months. I broke it down into smaller calculated fields, tested each piece, and documented the logic for the team.\"\n\n### Tools & Technologies\n- **Tableau**: Desktop, Server, Prep, Online\n- **Calculations**: LOD, Table Calcs, Parameters\n- **Data Sources**: SQL Server, Oracle, Excel, CSV\n- **Integration**: Tableau APIs, webhooks, embedded analytics\n- **Administration**: User management, permissions, monitoring\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 31
    },
    "id": "tejuu_tableau_expertise_525"
  },
  {
    "text": "Integration**: Tableau APIs, webhooks, embedded analytics\n- **Administration**: User management, permissions, monitoring\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 32
    },
    "id": "tejuu_tableau_expertise_526"
  },
  {
    "text": "---\ntags: [power-bi, dax, data-modeling, performance, rls, best-practices]\npersona: tejuu\n---\n\n# Advanced Power BI & DAX - Tejuu's Expertise\n\n## Power BI Data Modeling\n\n### Star Schema Design for Healthcare Analytics\n**Tejuu's Implementation at Stryker:**\nSo at Stryker, I designed this comprehensive star schema for our Medicaid and healthcare analytics. The key was understanding the business requirements first - what questions do clinicians, finance, and operations teams need to answer?\n\n**My Star Schema Structure:**\n```\nFact Tables:\n- fact_claims (claim_id, patient_key, provider_key, date_key, claim_amount, paid_amount, denied_amount)\n- fact_patient_visits (visit_id, patient_key, facility_key, date_key, visit_type, duration_minutes, cost)\n- fact_inventory (product_key, warehouse_key, date_key, quantity_on_hand, reorder_level, unit_cost)\n\nDimension Tables:\n- dim_patient (patient_key, patient_id, age_group, gender, state, insurance_type, risk_score)\n- dim_provider (provider_key, provide",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 33
    },
    "id": "tejuu_power_bi_advanced_527"
  },
  {
    "text": "_key, quantity_on_hand, reorder_level, unit_cost)\n\nDimension Tables:\n- dim_patient (patient_key, patient_id, age_group, gender, state, insurance_type, risk_score)\n- dim_provider (provider_key, provider_id, provider_name, specialty, network_status, region)\n- dim_facility (facility_key, facility_id, facility_name, facility_type, city, state, bed_count)\n- dim_product (product_key, product_id, product_name, category, subcategory, manufacturer)\n- dim_date (date_key, date, year, quarter, month, week, day_of_week, is_holiday, fiscal_period)\n- dim_insurance (insurance_key, payer_name, plan_type, coverage_level)\n```\n\n**Relationship Best Practices I Follow:**\n```\n1. Always use surrogate keys (integers) for relationships\n2. One-to-many relationships from dimension to fact\n3. Avoid bi-directional relationships (use DAX instead)\n4. Hide foreign keys from report view\n5. Mark date table properly\n6. Use role-playing dimensions sparingly\n```\n\n### Row-Level Security (RLS) Implementation\n**Tejuu's RLS St",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 34
    },
    "id": "tejuu_power_bi_advanced_528"
  },
  {
    "text": "tionships (use DAX instead)\n4. Hide foreign keys from report view\n5. Mark date table properly\n6. Use role-playing dimensions sparingly\n```\n\n### Row-Level Security (RLS) Implementation\n**Tejuu's RLS Strategy:**\nSo implementing RLS was critical at Central Bank and Stryker because different users needed access to different data. Here's how I approached it:\n\n**Role-Based Access Example:**\n```dax\n-- Regional Manager Role\n[Region] = USERPRINCIPALNAME()\n\n-- State-Level Access\n[State] IN {\n    LOOKUPVALUE(\n        user_access[State],\n        user_access[Email],\n        USERPRINCIPALNAME()\n    )\n}\n\n-- Dynamic Security with Bridge Table\nVAR UserEmail = USERPRINCIPALNAME()\nVAR UserRegions = \n    CALCULATETABLE(\n        VALUES(security_bridge[Region]),\n        security_bridge[UserEmail] = UserEmail\n    )\nRETURN\n    [Region] IN UserRegions\n\n-- Manager Hierarchy (see your team and below)\nVAR CurrentUser = USERPRINCIPALNAME()\nVAR UserLevel = \n    LOOKUPVALUE(\n        dim_employee[Level],\n        dim_",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 35
    },
    "id": "tejuu_power_bi_advanced_529"
  },
  {
    "text": "  )\nRETURN\n    [Region] IN UserRegions\n\n-- Manager Hierarchy (see your team and below)\nVAR CurrentUser = USERPRINCIPALNAME()\nVAR UserLevel = \n    LOOKUPVALUE(\n        dim_employee[Level],\n        dim_employee[Email],\n        CurrentUser\n    )\nRETURN\n    dim_employee[Manager_Email] = CurrentUser ||\n    dim_employee[Email] = CurrentUser\n```\n\n**Testing RLS:**\n```\n1. Create test users in Azure AD\n2. Use \"View as\" feature in Power BI Desktop\n3. Test each role with sample queries\n4. Document expected vs actual results\n5. Validate with actual users before production\n```\n\n## Advanced DAX Patterns\n\n### Time Intelligence Mastery\n**Tejuu's Most-Used Time Intelligence Patterns:**\n\n```dax\n-- Year-to-Date with Fiscal Calendar\nYTD Sales = \nCALCULATE(\n    SUM(fact_sales[Amount]),\n    DATESYTD(\n        dim_date[Date],\n        \"06/30\"  -- Fiscal year ends June 30\n    )\n)\n\n-- Prior Year Same Period\nPY Sales = \nCALCULATE(\n    [Total Sales],\n    SAMEPERIODLASTYEAR(dim_date[Date])\n)\n\n-- Year-over-Year Growt",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 36
    },
    "id": "tejuu_power_bi_advanced_530"
  },
  {
    "text": "_date[Date],\n        \"06/30\"  -- Fiscal year ends June 30\n    )\n)\n\n-- Prior Year Same Period\nPY Sales = \nCALCULATE(\n    [Total Sales],\n    SAMEPERIODLASTYEAR(dim_date[Date])\n)\n\n-- Year-over-Year Growth %\nYoY Growth % = \nVAR CurrentPeriod = [Total Sales]\nVAR PriorPeriod = [PY Sales]\nRETURN\n    DIVIDE(\n        CurrentPeriod - PriorPeriod,\n        PriorPeriod,\n        0\n    )\n\n-- Month-to-Date vs Prior Month-to-Date\nMTD vs PMTD = \nVAR MTD = \n    CALCULATE(\n        [Total Sales],\n        DATESMTD(dim_date[Date])\n    )\nVAR PMTD = \n    CALCULATE(\n        [Total Sales],\n        DATESMTD(\n            DATEADD(dim_date[Date], -1, MONTH)\n        )\n    )\nRETURN\n    MTD - PMTD\n\n-- Rolling 12 Months\nRolling 12M Sales = \nCALCULATE(\n    [Total Sales],\n    DATESINPERIOD(\n        dim_date[Date],\n        LASTDATE(dim_date[Date]),\n        -12,\n        MONTH\n    )\n)\n\n-- Same Period Last Year with Date Table\nSPLY Sales = \nCALCULATE(\n    [Total Sales],\n    DATEADD(dim_date[Date], -1, YEAR)\n)\n```\n\n### Complex",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 37
    },
    "id": "tejuu_power_bi_advanced_531"
  },
  {
    "text": "TDATE(dim_date[Date]),\n        -12,\n        MONTH\n    )\n)\n\n-- Same Period Last Year with Date Table\nSPLY Sales = \nCALCULATE(\n    [Total Sales],\n    DATEADD(dim_date[Date], -1, YEAR)\n)\n```\n\n### Complex Calculated Columns vs Measures\n**When I Use Each:**\n\n**Calculated Columns (Stored in Model):**\n```dax\n-- Age Group (Calculated Column)\nAge Group = \nSWITCH(\n    TRUE(),\n    dim_patient[Age] < 18, \"Pediatric\",\n    dim_patient[Age] < 65, \"Adult\",\n    \"Senior\"\n)\n\n-- Days Since Last Visit (Calculated Column)\nDays Since Last Visit = \nDATEDIFF(\n    dim_patient[Last_Visit_Date],\n    TODAY(),\n    DAY\n)\n\n-- Full Name (Calculated Column)\nFull Name = \ndim_patient[First_Name] & \" \" & dim_patient[Last_Name]\n```\n\n**Measures (Calculated at Query Time):**\n```dax\n-- Total Claims (Measure)\nTotal Claims = \nSUM(fact_claims[Claim_Amount])\n\n-- Average Claim Amount (Measure)\nAvg Claim Amount = \nAVERAGE(fact_claims[Claim_Amount])\n\n-- Distinct Patient Count (Measure)\nPatient Count = \nDISTINCTCOUNT(fact_claims[Pati",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 38
    },
    "id": "tejuu_power_bi_advanced_532"
  },
  {
    "text": "act_claims[Claim_Amount])\n\n-- Average Claim Amount (Measure)\nAvg Claim Amount = \nAVERAGE(fact_claims[Claim_Amount])\n\n-- Distinct Patient Count (Measure)\nPatient Count = \nDISTINCTCOUNT(fact_claims[Patient_Key])\n\n-- Claim Approval Rate (Measure)\nApproval Rate = \nDIVIDE(\n    CALCULATE([Total Claims], fact_claims[Status] = \"Approved\"),\n    [Total Claims],\n    0\n)\n```\n\n### Advanced DAX for Healthcare Analytics\n**Tejuu's Medicaid Analytics at Stryker:**\n\n```dax\n-- Patient Risk Score\nPatient Risk Score = \nVAR PatientVisits = \n    CALCULATE(\n        COUNTROWS(fact_patient_visits),\n        ALLEXCEPT(fact_patient_visits, fact_patient_visits[Patient_Key])\n    )\nVAR ChronicConditions = \n    CALCULATE(\n        DISTINCTCOUNT(fact_diagnosis[Condition_Code]),\n        fact_diagnosis[Is_Chronic] = TRUE()\n    )\nVAR HighCostFlag = \n    IF(\n        CALCULATE(\n            SUM(fact_claims[Paid_Amount]),\n            ALLEXCEPT(fact_claims, fact_claims[Patient_Key])\n        ) > 50000,\n        1,\n        0\n    )",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 39
    },
    "id": "tejuu_power_bi_advanced_533"
  },
  {
    "text": "  )\nVAR HighCostFlag = \n    IF(\n        CALCULATE(\n            SUM(fact_claims[Paid_Amount]),\n            ALLEXCEPT(fact_claims, fact_claims[Patient_Key])\n        ) > 50000,\n        1,\n        0\n    )\nRETURN\n    (PatientVisits * 0.3) + \n    (ChronicConditions * 0.5) + \n    (HighCostFlag * 0.2)\n\n-- Readmission Rate (30-day)\n30-Day Readmission Rate = \nVAR ReadmissionWindow = 30\nVAR Readmissions = \n    CALCULATE(\n        COUNTROWS(fact_patient_visits),\n        FILTER(\n            fact_patient_visits,\n            VAR CurrentVisitDate = fact_patient_visits[Visit_Date]\n            VAR PatientKey = fact_patient_visits[Patient_Key]\n            VAR PriorVisit = \n                CALCULATE(\n                    MAX(fact_patient_visits[Visit_Date]),\n                    ALLEXCEPT(fact_patient_visits, fact_patient_visits[Patient_Key]),\n                    fact_patient_visits[Visit_Date] < CurrentVisitDate\n                )\n            RETURN\n                DATEDIFF(PriorVisit, CurrentVisitDate, DAY)",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 40
    },
    "id": "tejuu_power_bi_advanced_534"
  },
  {
    "text": "ct_patient_visits[Patient_Key]),\n                    fact_patient_visits[Visit_Date] < CurrentVisitDate\n                )\n            RETURN\n                DATEDIFF(PriorVisit, CurrentVisitDate, DAY) <= ReadmissionWindow\n        )\n    )\nVAR TotalVisits = COUNTROWS(fact_patient_visits)\nRETURN\n    DIVIDE(Readmissions, TotalVisits, 0)\n\n-- Cost per Member per Month (PMPM)\nPMPM = \nVAR TotalCost = SUM(fact_claims[Paid_Amount])\nVAR MemberMonths = \n    SUMX(\n        VALUES(dim_patient[Patient_Key]),\n        CALCULATE(\n            DISTINCTCOUNT(dim_date[Month_Year])\n        )\n    )\nRETURN\n    DIVIDE(TotalCost, MemberMonths, 0)\n\n-- Length of Stay Analysis\nAvg Length of Stay = \nAVERAGEX(\n    fact_patient_visits,\n    DATEDIFF(\n        fact_patient_visits[Admission_Date],\n        fact_patient_visits[Discharge_Date],\n        DAY\n    )\n)\n\n-- Case Mix Index\nCase Mix Index = \nAVERAGEX(\n    VALUES(fact_patient_visits[Visit_ID]),\n    RELATED(dim_drg[Relative_Weight])\n)\n```\n\n### Performance Optimization ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 41
    },
    "id": "tejuu_power_bi_advanced_535"
  },
  {
    "text": "ischarge_Date],\n        DAY\n    )\n)\n\n-- Case Mix Index\nCase Mix Index = \nAVERAGEX(\n    VALUES(fact_patient_visits[Visit_ID]),\n    RELATED(dim_drg[Relative_Weight])\n)\n```\n\n### Performance Optimization Techniques\n**My DAX Optimization Rules:**\n\n```dax\n-- BEFORE: Slow (multiple context transitions)\nSlow Measure = \nSUMX(\n    fact_sales,\n    fact_sales[Quantity] * \n    RELATED(dim_product[Unit_Price])\n)\n\n-- AFTER: Fast (calculated column or import both fields)\nFast Measure = \nSUM(fact_sales[Line_Total])\n\n-- BEFORE: Slow (row context in measure)\nSlow Customer Count = \nCOUNTROWS(\n    FILTER(\n        dim_customer,\n        CALCULATE(SUM(fact_sales[Amount])) > 1000\n    )\n)\n\n-- AFTER: Fast (use CALCULATETABLE)\nFast Customer Count = \nCOUNTROWS(\n    CALCULATETABLE(\n        VALUES(dim_customer[Customer_Key]),\n        fact_sales[Amount] > 1000\n    )\n)\n\n-- Use Variables to Avoid Recalculation\nOptimized Measure = \nVAR TotalSales = SUM(fact_sales[Amount])\nVAR TotalCost = SUM(fact_sales[Cost])\nVAR Margin",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 42
    },
    "id": "tejuu_power_bi_advanced_536"
  },
  {
    "text": "Key]),\n        fact_sales[Amount] > 1000\n    )\n)\n\n-- Use Variables to Avoid Recalculation\nOptimized Measure = \nVAR TotalSales = SUM(fact_sales[Amount])\nVAR TotalCost = SUM(fact_sales[Cost])\nVAR Margin = TotalSales - TotalCost\nVAR MarginPct = DIVIDE(Margin, TotalSales, 0)\nRETURN\n    IF(MarginPct > 0.3, \"High\", \"Low\")\n```\n\n## Power BI Performance Tuning\n\n### Query Performance Optimization\n**Tejuu's Optimization Checklist:**\n\n```\n1. Data Model Optimization:\n   - Remove unused columns and tables\n   - Use integer keys instead of text\n   - Disable auto date/time hierarchy\n   - Use appropriate data types\n   - Avoid calculated columns when possible\n\n2. DAX Optimization:\n   - Use variables to store intermediate results\n   - Avoid row context in measures\n   - Use TREATAS instead of FILTER when possible\n   - Minimize use of CALCULATE\n   - Use SELECTEDVALUE instead of VALUES + HASONEVALUE\n\n3. Visual Optimization:\n   - Limit visuals per page (max 10-15)\n   - Use bookmarks for different views\n   - A",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 43
    },
    "id": "tejuu_power_bi_advanced_537"
  },
  {
    "text": "ble\n   - Minimize use of CALCULATE\n   - Use SELECTEDVALUE instead of VALUES + HASONEVALUE\n\n3. Visual Optimization:\n   - Limit visuals per page (max 10-15)\n   - Use bookmarks for different views\n   - Avoid high-cardinality fields in visuals\n   - Use aggregated data when possible\n   - Implement drill-through instead of showing all detail\n\n4. Data Refresh Optimization:\n   - Use incremental refresh for large tables\n   - Partition large tables by date\n   - Schedule refreshes during off-peak hours\n   - Use dataflows for common transformations\n```\n\n### Incremental Refresh Configuration\n**My Implementation at Stryker:**\n\n```\n1. Create RangeStart and RangeEnd parameters (Date/Time type)\n2. Filter source data:\n   = Table.SelectRows(Source, \n       each [Date] >= RangeStart and [Date] < RangeEnd)\n3. Configure incremental refresh policy:\n   - Archive data: 5 years\n   - Incremental refresh: 10 days\n   - Detect data changes: Yes\n   - Only refresh complete days: Yes\n```\n\n## Advanced Visualizations\n\n#",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 44
    },
    "id": "tejuu_power_bi_advanced_538"
  },
  {
    "text": "Configure incremental refresh policy:\n   - Archive data: 5 years\n   - Incremental refresh: 10 days\n   - Detect data changes: Yes\n   - Only refresh complete days: Yes\n```\n\n## Advanced Visualizations\n\n### Custom Visuals I Use\n**Tejuu's Favorite Custom Visuals:**\n\n```\n1. Zebra BI Tables - For variance analysis\n2. Drill Down Donut PRO - For hierarchical data\n3. Chiclet Slicer - For better filter experience\n4. Power KPI - For executive dashboards\n5. Sankey Diagram - For flow analysis\n6. Box and Whisker - For distribution analysis\n```\n\n### Conditional Formatting Patterns\n**My Advanced Formatting Techniques:**\n\n```dax\n-- Color Scale Based on Performance\nColor Measure = \nVAR Performance = [Actual] / [Target]\nRETURN\n    SWITCH(\n        TRUE(),\n        Performance >= 1.1, \"#00B050\",  -- Green\n        Performance >= 0.9, \"#FFC000\",  -- Yellow\n        \"#FF0000\"                        -- Red\n    )\n\n-- Icon Set Based on Trend\nTrend Icon = \nVAR CurrentMonth = [Sales This Month]\nVAR PriorMonth = [Sale",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 45
    },
    "id": "tejuu_power_bi_advanced_539"
  },
  {
    "text": "  Performance >= 0.9, \"#FFC000\",  -- Yellow\n        \"#FF0000\"                        -- Red\n    )\n\n-- Icon Set Based on Trend\nTrend Icon = \nVAR CurrentMonth = [Sales This Month]\nVAR PriorMonth = [Sales Last Month]\nVAR Change = DIVIDE(CurrentMonth - PriorMonth, PriorMonth, 0)\nRETURN\n    SWITCH(\n        TRUE(),\n        Change > 0.05, \"▲\",\n        Change < -0.05, \"▼\",\n        \"■\"\n    )\n\n-- Data Bars with Conditional Logic\nData Bar Value = \nVAR MaxValue = \n    CALCULATE(\n        MAX(fact_sales[Amount]),\n        ALLSELECTED(dim_product[Product_Name])\n    )\nVAR CurrentValue = SUM(fact_sales[Amount])\nRETURN\n    DIVIDE(CurrentValue, MaxValue, 0)\n```\n\n## Power BI Deployment & ALM\n\n### Application Lifecycle Management\n**Tejuu's ALM Strategy:**\n\n```\nDevelopment → Test → Production Pipeline\n\n1. Development Workspace:\n   - Individual developer workspaces\n   - Connect to Dev database\n   - Rapid iteration and testing\n   - Git integration for version control\n\n2. Test Workspace:\n   - Shared team worksp",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 46
    },
    "id": "tejuu_power_bi_advanced_540"
  },
  {
    "text": "pment Workspace:\n   - Individual developer workspaces\n   - Connect to Dev database\n   - Rapid iteration and testing\n   - Git integration for version control\n\n2. Test Workspace:\n   - Shared team workspace\n   - Connect to Test database\n   - UAT with business users\n   - Performance testing\n\n3. Production Workspace:\n   - Premium capacity\n   - Connect to Prod database\n   - Scheduled refreshes\n   - Monitoring and alerts\n\nDeployment Process:\n1. Export .pbix from Dev\n2. Update data source to Test\n3. Publish to Test workspace\n4. UAT sign-off\n5. Export from Test\n6. Update data source to Prod\n7. Publish to Prod workspace\n8. Configure refresh schedule\n9. Set up monitoring\n```\n\n### Version Control with Git\n**My Git Workflow:**\n\n```bash\n# Save Power BI file as PBIX\n# Use external tools to extract PBIT\n\n# Git workflow\ngit checkout -b feature/new-dashboard\n# Make changes in Power BI\ngit add .\ngit commit -m \"Add sales performance dashboard\"\ngit push origin feature/new-dashboard\n# Create pull request\n# ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 47
    },
    "id": "tejuu_power_bi_advanced_541"
  },
  {
    "text": "# Git workflow\ngit checkout -b feature/new-dashboard\n# Make changes in Power BI\ngit add .\ngit commit -m \"Add sales performance dashboard\"\ngit push origin feature/new-dashboard\n# Create pull request\n# Code review\n# Merge to main\n```\n\n### Deployment Pipelines\n**Configuration:**\n\n```\nPipeline Stages:\n1. Development\n   - Auto-deploy on commit to dev branch\n   - Run data quality tests\n   - Generate documentation\n\n2. Test\n   - Deploy on PR approval\n   - Run regression tests\n   - UAT validation\n\n3. Production\n   - Manual approval required\n   - Deploy during maintenance window\n   - Backup before deployment\n   - Rollback plan ready\n```\n\n## Power BI Best Practices\n\n### Naming Conventions\n**My Standard Naming:**\n\n```\nTables:\n- fact_sales, fact_claims, fact_inventory\n- dim_customer, dim_product, dim_date\n\nMeasures:\n- Total Sales, Avg Order Value, YTD Revenue\n- Use spaces, title case\n- Group related measures in folders\n\nColumns:\n- Use snake_case or PascalCase consistently\n- Prefix calculated column",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 48
    },
    "id": "tejuu_power_bi_advanced_542"
  },
  {
    "text": "ate\n\nMeasures:\n- Total Sales, Avg Order Value, YTD Revenue\n- Use spaces, title case\n- Group related measures in folders\n\nColumns:\n- Use snake_case or PascalCase consistently\n- Prefix calculated columns with \"calc_\"\n- Hide foreign keys\n\nRelationships:\n- Always name relationships descriptively\n- Document complex relationships\n```\n\n### Documentation Standards\n**What I Document:**\n\n```markdown\n# Dashboard Documentation\n\n## Purpose\nExecutive sales performance dashboard for regional managers\n\n## Data Sources\n- Azure SQL: sales_db.fact_sales\n- Refresh: Daily at 6 AM EST\n- Latency: T-1 day\n\n## Key Metrics\n- Total Sales: SUM of fact_sales[Amount]\n- YoY Growth: (Current - Prior Year) / Prior Year\n- Target Attainment: Actual / Target\n\n## Filters\n- Date Range: Last 12 months default\n- Region: Multi-select\n- Product Category: Single select\n\n## Row-Level Security\n- Regional managers see their region only\n- VPs see all regions\n- Executives see all data\n\n## Known Issues\n- Data quality issue with Regio",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 49
    },
    "id": "tejuu_power_bi_advanced_543"
  },
  {
    "text": "lect\n- Product Category: Single select\n\n## Row-Level Security\n- Regional managers see their region only\n- VPs see all regions\n- Executives see all data\n\n## Known Issues\n- Data quality issue with Region \"Unknown\" - investigating\n- Slow performance on Product Detail page - optimization in progress\n\n## Change Log\n- 2024-01-15: Added YoY comparison\n- 2024-01-10: Initial release\n```\n\n## Interview Talking Points\n\n### Technical Achievements\n- Built 50+ Power BI dashboards serving 200+ users\n- Implemented RLS for 10+ different security roles\n- Optimized DAX reducing query time from 30s to 3s\n- Designed star schemas for healthcare and sales analytics\n- Achieved 99% dashboard uptime in production\n\n### Problem-Solving Examples\n**Performance Issue:**\n\"At Stryker, we had this dashboard that was taking 30 seconds to load. I analyzed the DAX and found we were using calculated columns in measures, causing multiple context transitions. I refactored the measures to use variables and proper filter contex",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 50
    },
    "id": "tejuu_power_bi_advanced_544"
  },
  {
    "text": "30 seconds to load. I analyzed the DAX and found we were using calculated columns in measures, causing multiple context transitions. I refactored the measures to use variables and proper filter context, and got the load time down to 3 seconds.\"\n\n**Complex Business Logic:**\n\"At Central Bank, finance needed a complex calculation for regulatory reporting. It involved multiple date ranges, conditional logic, and aggregations across different grain levels. I broke it down into smaller measures, used variables for readability, and documented each step. The final measure was accurate and performant.\"\n\n### Tools & Technologies\n- **Power BI**: Desktop, Service, Premium, Embedded, Report Server\n- **DAX**: Advanced patterns, optimization, debugging\n- **Power Query**: M language, custom functions, data transformation\n- **Azure**: Synapse, Data Factory, SQL Database\n- **Version Control**: Git, Azure DevOps\n- **ALM**: Deployment pipelines, testing, documentation\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 51
    },
    "id": "tejuu_power_bi_advanced_545"
  },
  {
    "text": "a transformation\n- **Azure**: Synapse, Data Factory, SQL Database\n- **Version Control**: Git, Azure DevOps\n- **ALM**: Deployment pipelines, testing, documentation\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 52
    },
    "id": "tejuu_power_bi_advanced_546"
  }
]