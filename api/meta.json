[
  {
    "id": "mlops_pipeline_0",
    "text": "# MLOps Pipeline Architecture\n\n## Overview\nMLOps (Machine Learning Operations) encompasses the practices and tools needed to deploy, monitor, and maintain machine learning models in production environments.\n\n## Core Components\n\n### 1. Model Development\n- **Experiment Tracking**: Use MLflow, Weights & Biases, or TensorBoard\n- **Version Control**: Track code, data, and model versions\n- **Reproducibility**: Ensure consistent environments and dependencies\n\n### 2. Model Training Pipeline\n- **Data Pipeline**: Automated data ingestion, validation, and preprocessing\n- **Training Orchestration**: Use Airflow, Prefect, or Kubeflow Pipelines\n- **Hyperparameter Optimization**: Automated tuning with Optuna or Ray Tune\n- **Model Validation**: Automated testing and performance evaluation\n\n### 3. Model Deployment\n- **Containerization**: Package models in Docker containers\n- **Orchestration**: Deploy on Kubernetes or cloud services\n- **API Gateway**: Expose models through REST or GraphQL APIs\n- **Load Balancing**: Distribute traffic across multiple model instances\n\n### 4. Model Monitoring\n- **Performance Monitoring**: Track accuracy, latency, and throughput\n- **Data Drift Detection**: Monitor input data distribution changes\n- **Model Drift Detection**: Track model performance degradation\n- **Alerting**: Set up notifications for critical issues\n\n## MLflow Integration\n\n### Experiment Tracking\n```python\nimport mlflow\n\n# Start experiment\nmlflow.set_experiment(\"customer-churn-prediction\")\n\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_param(\"epochs\", 100)\n    \n    # Train model\n    model = train_model(X_train, y_train)\n    \n    # Log metrics\n    accuracy = evaluate_model(model, X_test, y_test)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    \n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n```\n\n### Model Registry\n- **Model Versioning**: Track different model versions\n- **Staging Workflow**: Promote models through dev → staging → prod\n- **Model Metadata**: Store model descriptions, tags, and lineage\n- **Automated Deployment**: Trigger deployments based on model performance\n\n## Kubernetes Deployment\n\n### Model Serving with KServe\n```yaml\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: customer-churn-model\nspec:\n  predictor:\n    sklearn:\n      storageUri: gs://model-registry/churn-model/1\n      resources:\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        limits:\n          cpu: \"2\"\n          memory: \"4Gi\"\n```\n\n### Auto-scaling Configuration\n- **Horizontal Pod Autoscaler**: Scale based on CPU/memory usage\n- **Vertical Pod Autoscaler**: Optimize resource allocation\n- **Custom Metrics**: Scale based on request latency or queue depth\n\n## Data Pipeline Integration\n\n### Feature Engineering Pipeline\n- **Feature Store**: Centralized feature management (Feast, Tecton)\n- **Feature Validation**: Ensure feature quality and consistency\n- **Feature Serving**: Low-latency feature retrieval for inference\n- **Feature Lineage**: Track feature dependencies and transformations\n\n### Data Validation\n- **Schema Validation**: Ensure data conforms to expected schema\n- **Statistical Validation**: Check data distributions and anomalies\n- **Temporal Validation**: Monitor data freshness and completeness\n- **Quality Metrics**: Track data quality scores over time\n\n## Monitoring and Observability\n\n### Performance Metrics\n- **Latency**: Track prediction response times\n- **Throughput**: Monitor requests per second\n- **Error Rates**: Track prediction failures and exceptions\n- **Resource Usage**: Monitor CPU, memory, and GPU utilization\n\n### Model Quality Metrics\n- **Accuracy**: Track model performance on validation data\n- **Precision/Recall**: Monitor classification metrics\n- **A/B Testing**: Compare model versions in production\n- **Statistical Significance**: Ensure metric differences are meaningful\n\n### Data Drift Detection\n- **Statistical Tests**: Use KS test, PSI, or KL divergence\n- **Feature Importance**: Track changes in feature importance\n- **Distribution Monitoring**: Visualize feature distributions over time\n- **Alerting Thresholds**: Set up alerts for significant drift\n\n## Best Practices\n\n### Model Lifecycle Management\n- **Automated Retraining**: Trigger retraining based on performance degradation\n- **Gradual Rollout**: Deploy new models to small traffic percentages first\n- **Rollback Strategy**: Quick rollback to previous model version\n- **Blue-Green Deployment**: Zero-downtime model updates\n\n### Security and Compliance\n- **Model Encryption**: Encrypt models at rest and in transit\n- **Access Control**: Implement role-based access to models and data",
    "metadata": {
      "tags": [
        "mlops",
        "mlflow",
        "deployment",
        "monitoring"
      ],
      "persona": "ai",
      "file_path": "ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md"
    }
  },
  {
    "id": "mlops_pipeline_1",
    "text": "performance on validation data\n- **Precision/Recall**: Monitor classification metrics\n- **A/B Testing**: Compare model versions in production\n- **Statistical Significance**: Ensure metric differences are meaningful\n\n### Data Drift Detection\n- **Statistical Tests**: Use KS test, PSI, or KL divergence\n- **Feature Importance**: Track changes in feature importance\n- **Distribution Monitoring**: Visualize feature distributions over time\n- **Alerting Thresholds**: Set up alerts for significant drift\n\n## Best Practices\n\n### Model Lifecycle Management\n- **Automated Retraining**: Trigger retraining based on performance degradation\n- **Gradual Rollout**: Deploy new models to small traffic percentages first\n- **Rollback Strategy**: Quick rollback to previous model version\n- **Blue-Green Deployment**: Zero-downtime model updates\n\n### Security and Compliance\n- **Model Encryption**: Encrypt models at rest and in transit\n- **Access Control**: Implement role-based access to models and data\n- **Audit Logging**: Track all model access and predictions\n- **GDPR Compliance**: Handle data privacy and right to be forgotten\n\n### Cost Optimization\n- **Resource Right-sizing**: Optimize CPU/memory allocation\n- **Spot Instances**: Use spot instances for training workloads\n- **Model Compression**: Reduce model size with quantization or pruning\n- **Caching**: Cache predictions for repeated queries\n\n## Tools and Platforms\n\n### Open Source\n- **MLflow**: Experiment tracking and model registry\n- **Kubeflow**: End-to-end ML workflow orchestration\n- **Seldon Core**: Model serving and deployment\n- **Evidently**: Model monitoring and drift detection\n\n### Cloud Platforms\n- **Azure ML**: Managed ML platform with MLOps capabilities\n- **AWS SageMaker**: End-to-end ML platform\n- **Google Vertex AI**: Managed ML platform with AutoML\n- **Databricks**: Unified analytics platform for ML\n\n## Implementation Strategy\n\n### Phase 1: Foundation\n- Set up experiment tracking and model registry\n- Implement basic CI/CD pipeline\n- Deploy models as REST APIs\n\n### Phase 2: Automation\n- Automate model training and validation\n- Implement automated deployment pipelines\n- Add basic monitoring and alerting\n\n### Phase 3: Advanced Features\n- Implement advanced monitoring and drift detection\n- Add A/B testing and gradual rollout capabilities\n- Optimize for cost and performance",
    "metadata": {
      "tags": [
        "mlops",
        "mlflow",
        "deployment",
        "monitoring"
      ],
      "persona": "ai",
      "file_path": "ai_ml/mlops_pipeline.md",
      "file_name": "mlops_pipeline.md"
    }
  },
  {
    "id": "rag_systems_0",
    "text": "# RAG Systems Architecture\n\n## Overview\nRetrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to provide more accurate and up-to-date responses.\n\n## Key Components\n\n### 1. Document Ingestion Pipeline\n- **Text Chunking**: Split documents into overlapping chunks (typically 512-1024 tokens)\n- **Embedding Generation**: Use models like `text-embedding-3-small` or `text-embedding-ada-002`\n- **Vector Storage**: Store embeddings in vector databases (Pinecone, Weaviate, FAISS)\n\n### 2. Retrieval Strategy\n- **Similarity Search**: Use cosine similarity or dot product for semantic matching\n- **Hybrid Search**: Combine semantic search with keyword-based BM25\n- **Re-ranking**: Apply cross-encoder models to improve relevance\n\n### 3. Generation Process\n- **Context Injection**: Include retrieved chunks in the prompt\n- **Prompt Engineering**: Structure prompts to leverage retrieved information\n- **Response Synthesis**: Generate answers grounded in retrieved context\n\n## Implementation Best Practices\n\n### Chunking Strategies\n- **Fixed-size chunks**: Simple but may split important information\n- **Semantic chunking**: Use sentence transformers to find natural boundaries\n- **Hierarchical chunking**: Store both fine and coarse-grained representations\n\n### Embedding Models\n- **General purpose**: OpenAI embeddings work well for most use cases\n- **Domain-specific**: Fine-tune embeddings on your specific domain\n- **Multilingual**: Consider models like `paraphrase-multilingual-MiniLM-L12-v2`\n\n### Vector Database Selection\n- **Pinecone**: Managed service, good for production\n- **Weaviate**: Open source, supports hybrid search\n- **FAISS**: Facebook's library, good for research and prototyping\n- **Chroma**: Lightweight, easy to get started\n\n## Performance Optimization\n\n### Query Processing\n- **Query expansion**: Generate multiple query variations\n- **Query routing**: Direct queries to specialized indexes\n- **Caching**: Cache frequent queries and their results\n\n### Retrieval Tuning\n- **Top-k selection**: Experiment with different numbers of retrieved chunks\n- **Score thresholds**: Filter out low-relevance results\n- **Diversity sampling**: Ensure retrieved chunks cover different aspects\n\n## Common Challenges\n\n### Information Retrieval Issues\n- **Semantic mismatch**: Query and documents use different terminology\n- **Context loss**: Important information split across chunks\n- **Hallucination**: Model generates information not in retrieved context\n\n### Solutions\n- **Query preprocessing**: Expand queries with synonyms and related terms\n- **Overlapping chunks**: Ensure important information appears in multiple chunks\n- **Grounding verification**: Check if generated content is supported by retrieved chunks\n\n## Evaluation Metrics\n\n### Retrieval Quality\n- **Precision@K**: Fraction of retrieved items that are relevant\n- **Recall@K**: Fraction of relevant items that are retrieved\n- **MRR**: Mean reciprocal rank of first relevant item\n\n### Generation Quality\n- **Faithfulness**: Generated content is supported by retrieved context\n- **Relevance**: Generated content answers the user's question\n- **Completeness**: Generated content covers all aspects of the question\n\n## Production Considerations\n\n### Scalability\n- **Batch processing**: Process large document collections efficiently\n- **Incremental updates**: Add new documents without rebuilding entire index\n- **Load balancing**: Distribute queries across multiple instances\n\n### Monitoring\n- **Query latency**: Track response times for different query types\n- **Retrieval quality**: Monitor precision and recall metrics\n- **Generation quality**: Track user satisfaction and feedback\n\n### Security\n- **Access control**: Restrict access to sensitive documents\n- **Query sanitization**: Prevent injection attacks through user queries\n- **Audit logging**: Track all queries and responses for compliance",
    "metadata": {
      "tags": [
        "rag",
        "retrieval",
        "embeddings",
        "vector-db"
      ],
      "persona": "ai",
      "file_path": "ai_ml/rag_systems.md",
      "file_name": "rag_systems.md"
    }
  },
  {
    "id": "ai_ml_gen_ai_0",
    "text": "# 📄 OG Resume – Krishna Sathvik (AI/ML & GenAI Version)\n\n---\n\n## Project Intro (Real-World AI/ML/GenAI Example)\n\nIn my recent AI/ML project at Walgreens, I worked as an AI/ML Engineer to design and deploy a **GenAI-powered knowledge assistant** for pharmacy operations. The solution integrated a **Retrieval-Augmented Generation (RAG) pipeline** where customer FAQs and policy documents were ingested, chunked, and stored in a **vector database (Pinecone)**. We built embeddings using **OpenAI + Hugging Face models**, enabling pharmacists to query compliance or medication rules in natural language with citations. The ingestion pipelines were built in **Databricks + PySpark**, transforming structured and unstructured data (text, PDFs, call transcripts) into embedding-friendly formats.\n\nFor orchestration, we leveraged **Airflow** to automate nightly ingestion, retraining, and re-indexing jobs. The inference pipeline was deployed via **Azure Kubernetes Service (AKS)** with scalable APIs. We used **MLflow** for experiment tracking and **Azure DevOps CI/CD** to deploy models into production with rollback safety. Governance was enforced with **Unity Catalog + custom PII scrubbing** before embedding sensitive data. Performance tuning included prompt optimization, few-shot examples, and caching embeddings for high-frequency queries, which reduced inference latency by 40%.\n\nThis project gave me **end-to-end GenAI + ML Ops experience**: ingestion, embeddings, vector search, model deployment, monitoring, and governance. It also showcased the ability to bridge **data engineering foundations with AI/ML innovation**, aligning with enterprise needs for compliance, scalability, and cost efficiency.\n\n---\n\n### 📄 OG Resume Summary – AI/ML & GenAI Version\n\nAI/ML Engineer with **5+ years of experience** building end-to-end machine learning and generative AI solutions. Skilled in developing data pipelines, training and deploying ML models, and implementing RAG/LLM systems with embeddings and vector databases. Proven ability to deliver cost-efficient, production-ready AI platforms that scale across domains including healthcare, retail, and finance.\n\n## Experience\n\n**AI/ML Engineer | TCS (Walgreens, USA) | Feb 2022 – Present**\n\nDesigned and deployed RAG pipeline with Databricks + Pinecone + OpenAI, enabling pharmacists to query compliance docs in natural language with source-cited answers\n\nEngineered ingestion of structured/unstructured data (PDFs, transcripts) via PySpark pipelines, transforming data into embeddings and cutting manual search effort by 60%\n\nAutomated retraining and re-indexing workflows using Airflow, improving model freshness and reducing knowledge gaps across pharmacy operations\n\nDeployed scalable inference APIs on AKS with CI/CD in Azure DevOps, ensuring zero-downtime rollouts and 35% latency reduction with optimized caching + prompts\n\nImplemented governance with Unity Catalog and PII scrubbing, securing sensitive patient data while maintaining HIPAA compliance in embeddings and responses\n\n**ML Engineer | CVS Health (USA) | Jan 2021 – Jan 2022**\n\nBuilt demand forecasting models in PySpark + TensorFlow predicting sales and supply trends, improving procurement accuracy and saving \\$15M annually\n\nEngineered feature pipelines with dbt + Databricks for model-ready datasets, cutting preparation time by 40% and ensuring consistency across teams\n\nTracked experiments with MLflow and automated retraining pipelines, boosting model performance by 18% over baseline\n\nDeployed models to production via Azure ML, implementing monitoring for drift and automating retraining triggers\n\n**Data Science Intern | McKesson (USA) | May 2020 – Dec 2020**\n\nDeveloped ETL + ML scripts in Python reducing ingestion latency 50%, powering dashboards for executive decisions on financial integrity\n\nBuilt regression + time series models forecasting patient demand, preventing supply mismatches and reducing stockouts by 22%\n\nProduced compliance-focused ML insights aligning with audit requirements and informing leadership’s strategic reviews\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nBuilt backend APIs and optimized SQL queries for ERP modules, strengthening transactional accuracy and improving response times by 35%\n\nDesigned reporting modules surfacing anomalies in contracts and payments, reducing manual reconciliation workload\n\n---\n\n## Skills\n\n- **AI/ML & GenAI:** PyTorch, TensorFlow, Hugging Face, OpenAI API, LangChain, MLflow, RAG pipelines, Vector DBs (Pinecone, FAISS, Weaviate)\n- **Data Engineering & ELT:** PySpark, Databricks, Airflow, dbt\n- **Cloud Platforms:** Azure (ML, Data Factory, AKS, DevOps), AWS (SageMaker, Lambda, S3)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle,",
    "metadata": {
      "persona": "ai",
      "file_path": "ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md"
    }
  },
  {
    "id": "ai_ml_gen_ai_1",
    "text": "22%\n\nProduced compliance-focused ML insights aligning with audit requirements and informing leadership’s strategic reviews\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nBuilt backend APIs and optimized SQL queries for ERP modules, strengthening transactional accuracy and improving response times by 35%\n\nDesigned reporting modules surfacing anomalies in contracts and payments, reducing manual reconciliation workload\n\n---\n\n## Skills\n\n- **AI/ML & GenAI:** PyTorch, TensorFlow, Hugging Face, OpenAI API, LangChain, MLflow, RAG pipelines, Vector DBs (Pinecone, FAISS, Weaviate)\n- **Data Engineering & ELT:** PySpark, Databricks, Airflow, dbt\n- **Cloud Platforms:** Azure (ML, Data Factory, AKS, DevOps), AWS (SageMaker, Lambda, S3)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle, Vector DBs\n- **Analytics & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Governance:** Azure DevOps, GitHub Actions, Docker, Kubernetes, Unity Catalog, PII Masking, Model Monitoring\n\n---\n\n# 📑 Resume Optimization Framework – AI/ML/GenAI Version\n\n## 1. Structure & Bullet Rules\n\n**5-4-3-2 Rule**\n\n- Walgreens → 5 bullets | Present tense | Outcome + metrics | Core tech (RAG, GenAI, PySpark, Databricks)\n- CVS → 4 bullets | Past tense | ML pipelines + forecasting | TensorFlow, Databricks, dbt\n- McKesson → 3 bullets | Past tense | Forecasting + compliance outcomes | Python + ML models\n- Inditek → 2 bullets | Past tense | Developer foundation | APIs + SQL\n\n**Character Constraint Rule**\n\n- Each bullet = **220–240 characters**\n- Never <215, never >240\n\n---\n\n## 2. Domain Adaptation Layer\n\nAdapt **vocabulary + emphasis** per JD:\n\n- GenAI/LLMs → RAG, vector DBs, embeddings, LangChain, prompt optimization\n- ML Engineering → model training, feature pipelines, MLflow, deployment\n- Data + AI → hybrid roles bridging ingestion, ELT, and ML ops\n- Compliance/Healthcare → HIPAA, lineage, governance for AI outputs\n\n---\n\n## 3. Skills Optimization Layer\n\n**Always keep 6 categories:**\n\n1. AI/ML & GenAI\n2. Data Engineering & ELT\n3. Cloud Platforms\n4. Databases & Storage\n5. Analytics & BI\n6. DevOps & Governance\n\n**JD Alignment:**\n\n- Re-order tools inside categories (AWS ML before Azure ML if JD says AWS)\n- Highlight GenAI (LangChain, OpenAI, vector DBs) when relevant\n- Strictly ATS-optimized, no jargon\n\n---\n\n## 4. Verb Rotation Rule\n\nRotate verbs: **Engineer, Optimize, Automate, Deploy, Develop, Train, Fine-tune, Implement, Orchestrate, Scale, Monitor**\n\n---\n\n## 5. Metrics Bank Rule\n\nEvery bullet ties to measurable outcome:\n\n- % improvements (latency -40%, accuracy +18%)\n- Scale (10TB+, 200+ models tracked, 1M+ embeddings)\n- Cost savings (\\$15M+ savings, 30% infra cost cut)\n- Risk/Compliance outcomes (HIPAA alignment, audit-ready insights)\n- User impact (pharmacists saved 60% manual search time)\n\n---\n\n# 📋 JD → Resume Adaptation Checklist\n\n1. **Read JD closely** → Look for AI/ML vs GenAI emphasis (RAG, embeddings, ML pipelines).\n2. **Apply Domain Layer** → Mirror vocabulary (e.g., LLM + LangChain vs forecasting + TensorFlow).\n3. **Reorder Skills** → Keep 6 categories, move JD-priority tools up front.\n4. **Rewrite Bullets** → Use 5-4-3-2, adjust tech names, keep measurable outcomes.\n5. **Rotate Verbs** → Ensure diversity, no repetition.\n6. **Check Character Count** → 220–240 characters each.\n7. **Final Scan** → ATS keywords aligned, outcomes present, tense correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**GenAI/LLM-Heavy JD**\\\nAI/ML Engineer with 6+ years’ experience delivering RAG pipelines, vector database search, and LLM-powered assistants. Skilled in Databricks, PySpark, OpenAI, and Hugging Face with proven impact on compliance, scalability, and latency reduction.\n\n**ML Engineering JD**\\\nMachine Learning Engineer with 6+ years’ experience designing feature pipelines, training models",
    "metadata": {
      "persona": "ai",
      "file_path": "ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md"
    }
  },
  {
    "id": "ai_ml_gen_ai_2",
    "text": "LangChain vs forecasting + TensorFlow).\n3. **Reorder Skills** → Keep 6 categories, move JD-priority tools up front.\n4. **Rewrite Bullets** → Use 5-4-3-2, adjust tech names, keep measurable outcomes.\n5. **Rotate Verbs** → Ensure diversity, no repetition.\n6. **Check Character Count** → 220–240 characters each.\n7. **Final Scan** → ATS keywords aligned, outcomes present, tense correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**GenAI/LLM-Heavy JD**\\\nAI/ML Engineer with 6+ years’ experience delivering RAG pipelines, vector database search, and LLM-powered assistants. Skilled in Databricks, PySpark, OpenAI, and Hugging Face with proven impact on compliance, scalability, and latency reduction.\n\n**ML Engineering JD**\\\nMachine Learning Engineer with 6+ years’ experience designing feature pipelines, training models in TensorFlow/PyTorch, and deploying production systems via Azure ML & SageMaker. Strong background in Databricks, MLflow, and cost-optimized ML Ops.\n\n**Hybrid Data+AI JD**\\\nData & AI Engineer with 6+ years bridging ELT pipelines and ML/GenAI systems. Skilled in PySpark, dbt, Databricks, and OpenAI APIs with experience in feature engineering, governance, and deploying ML + GenAI solutions to production at scale.\n\n---",
    "metadata": {
      "persona": "ai",
      "file_path": "ai_ml/ai_ml_gen_ai.md",
      "file_name": "ai_ml_gen_ai.md"
    }
  },
  {
    "id": "databricks_pyspark_0",
    "text": "# Databricks and PySpark Optimization\n\n## Overview\nDatabricks provides a unified analytics platform built on Apache Spark, offering optimized performance for big data processing with Delta Lake for ACID transactions and data versioning.\n\n## PySpark Performance Optimization\n\n### 1. Data Partitioning Strategies\n```python\n# Partition by date for time-series data\ndf.write.partitionBy(\"date\").mode(\"overwrite\").parquet(\"path/to/data\")\n\n# Partition by business key for joins\ndf.write.partitionBy(\"customer_id\").mode(\"overwrite\").parquet(\"path/to/data\")\n\n# Coalesce to reduce number of files\ndf.coalesce(10).write.mode(\"overwrite\").parquet(\"path/to/data\")\n```\n\n### 2. Caching and Persistence\n```python\n# Cache frequently accessed DataFrames\ndf.cache()\n\n# Persist with different storage levels\ndf.persist(StorageLevel.MEMORY_AND_DISK_SER)\n\n# Unpersist when done\ndf.unpersist()\n```\n\n### 3. Broadcast Joins\n```python\n# Broadcast small tables for joins\nfrom pyspark.sql.functions import broadcast\n\nlarge_df.join(broadcast(small_df), \"key\", \"inner\")\n```\n\n## Delta Lake Best Practices\n\n### 1. Delta Table Management\n```python\n# Create Delta table with partitioning\ndf.write.format(\"delta\").partitionBy(\"date\").saveAsTable(\"sales_data\")\n\n# Optimize Delta table\nspark.sql(\"OPTIMIZE sales_data ZORDER BY (customer_id, product_id)\")\n\n# Vacuum old files\nspark.sql(\"VACUUM sales_data RETAIN 168 HOURS\")\n```\n\n### 2. Time Travel and Versioning\n```python\n# Read from specific version\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 5).table(\"sales_data\")\n\n# Read from specific timestamp\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2023-01-01\").table(\"sales_data\")\n\n# Show table history\nspark.sql(\"DESCRIBE HISTORY sales_data\").show()\n```\n\n### 3. Merge Operations\n```python\n# Upsert data using merge\nfrom delta.tables import DeltaTable\n\ndelta_table = DeltaTable.forName(spark, \"sales_data\")\n\ndelta_table.alias(\"target\").merge(\n    updates_df.alias(\"source\"),\n    \"target.id = source.id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n## Cluster Configuration\n\n### 1. Cluster Sizing\n- **Driver Node**: 2-4 cores, 8-16GB RAM for small to medium workloads\n- **Worker Nodes**: 4-8 cores, 16-32GB RAM per node\n- **Storage**: Use instance stores for temporary data, EBS for persistent data\n- **Network**: Enable enhanced networking for better performance\n\n### 2. Spark Configuration\n```python\n# Optimize for large datasets\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n\n# Optimize for joins\nspark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n```\n\n### 3. Dynamic Allocation\n```python\n# Enable dynamic allocation\nspark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\nspark.conf.set(\"spark.dynamicAllocation.minExecutors\", \"2\")\nspark.conf.set(\"spark.dynamicAllocation.maxExecutors\", \"20\")\nspark.conf.set(\"spark.dynamicAllocation.initialExecutors\", \"5\")\n```\n\n## Data Processing Patterns\n\n### 1. Streaming Data Processing\n```python\n# Read from Kafka\ndf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").load()\n\n# Process streaming data\nprocessed_df = df.select(\n    col(\"value\").cast(\"string\").alias(\"json\"),\n    col(\"timestamp\")\n).select(\n    from_json(col(\"json\"), schema).alias(\"data\"),\n    col(\"timestamp\")\n).select(\"data.*\", \"timestamp\")\n\n# Write to Delta Lake\nprocessed_df.writeStream.format(\"delta\").option(\"checkpointLocation\", \"/path/to/checkpoint\").start()\n```\n\n### 2. Batch Processing with Window Functions\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, lag, lead\n\n# Define window specification\nwindow_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n\n# Apply window functions\ndf.withColumn(\"row_num\", row_number().over(window_spec)) \\\n  .withColumn(\"prev_order_date\", lag(\"order_date\").over(window_spec)) \\\n  .withColumn(\"order_rank\", rank().over",
    "metadata": {
      "tags": [
        "databricks",
        "pyspark",
        "delta-lake",
        "big-data"
      ],
      "persona": "de",
      "file_path": "data_eng/databricks_pyspark.md",
      "file_name": "databricks_pyspark.md"
    }
  },
  {
    "id": "databricks_pyspark_1",
    "text": ":9092\").load()\n\n# Process streaming data\nprocessed_df = df.select(\n    col(\"value\").cast(\"string\").alias(\"json\"),\n    col(\"timestamp\")\n).select(\n    from_json(col(\"json\"), schema).alias(\"data\"),\n    col(\"timestamp\")\n).select(\"data.*\", \"timestamp\")\n\n# Write to Delta Lake\nprocessed_df.writeStream.format(\"delta\").option(\"checkpointLocation\", \"/path/to/checkpoint\").start()\n```\n\n### 2. Batch Processing with Window Functions\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, lag, lead\n\n# Define window specification\nwindow_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n\n# Apply window functions\ndf.withColumn(\"row_num\", row_number().over(window_spec)) \\\n  .withColumn(\"prev_order_date\", lag(\"order_date\").over(window_spec)) \\\n  .withColumn(\"order_rank\", rank().over(window_spec))\n```\n\n### 3. Data Quality Checks\n```python\n# Define data quality rules\nquality_rules = [\n    col(\"customer_id\").isNotNull(),\n    col(\"order_date\").isNotNull(),\n    col(\"amount\") > 0,\n    col(\"amount\") < 10000\n]\n\n# Apply quality checks\nvalid_df = df.filter(reduce(lambda a, b: a & b, quality_rules))\ninvalid_df = df.filter(~reduce(lambda a, b: a & b, quality_rules))\n\n# Log quality issues\ninvalid_df.write.mode(\"append\").saveAsTable(\"data_quality_issues\")\n```\n\n## Monitoring and Observability\n\n### 1. Spark UI Monitoring\n- **Jobs Tab**: Monitor job execution and stages\n- **Stages Tab**: Analyze stage performance and bottlenecks\n- **Storage Tab**: Check DataFrame caching and persistence\n- **Environment Tab**: Review Spark configuration\n\n### 2. Custom Metrics\n```python\n# Track custom metrics\nfrom pyspark.sql.functions import count, sum\n\n# Record processing metrics\nmetrics = df.agg(\n    count(\"*\").alias(\"total_records\"),\n    sum(\"amount\").alias(\"total_amount\")\n).collect()[0]\n\n# Log metrics to external system\nlog_metrics({\n    \"total_records\": metrics[\"total_records\"],\n    \"total_amount\": metrics[\"total_amount\"],\n    \"timestamp\": datetime.now()\n})\n```\n\n### 3. Error Handling\n```python\n# Implement robust error handling\ntry:\n    result_df = process_data(input_df)\n    result_df.write.mode(\"overwrite\").saveAsTable(\"processed_data\")\nexcept Exception as e:\n    # Log error details\n    error_details = {\n        \"error\": str(e),\n        \"timestamp\": datetime.now(),\n        \"input_record_count\": input_df.count()\n    }\n    \n    # Write error to error table\n    error_df = spark.createDataFrame([error_details])\n    error_df.write.mode(\"append\").saveAsTable(\"processing_errors\")\n    \n    # Re-raise exception\n    raise e\n```\n\n## Data Lakehouse Architecture\n\n### 1. Bronze Layer (Raw Data)\n```python\n# Ingest raw data\nraw_df = spark.read.format(\"json\").load(\"path/to/raw/data\")\n\n# Store in Bronze layer\nraw_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronze.raw_events\")\n```\n\n### 2. Silver Layer (Cleaned Data)\n```python\n# Clean and validate data\ncleaned_df = raw_df.filter(\n    col(\"event_id\").isNotNull() &\n    col(\"timestamp\").isNotNull() &\n    col(\"user_id\").isNotNull()\n).withColumn(\"processed_date\", current_timestamp())\n\n# Store in Silver layer\ncleaned_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"silver.cleaned_events\")\n```\n\n### 3. Gold Layer (Business Logic)\n```python\n# Apply business logic and aggregations\ngold_df = cleaned_df.groupBy(\"user_id\", \"date\").agg(\n    count(\"*\").alias(\"event_count\"),\n    sum(\"value\").alias(\"total_value\"),\n    avg(\"value\").alias(\"avg_value\")\n)\n\n# Store in Gold layer\ngold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.user_daily_metrics\")\n```\n\n## Cost Optimization\n\n### 1. Cluster Management\n- **Auto-scaling**: Use Databricks auto-scaling for variable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Cluster Pools**: Pre-allocate clusters for faster startup\n- **Termination Policies**: Auto-terminate idle clusters\n\n### 2. Storage Optimization\n- **Compression**: Use Parquet with Snappy compression\n- **Partitioning**: Partition large tables by date or key columns\n- **File Sizing**: Optimize file sizes (128MB-1GB per",
    "metadata": {
      "tags": [
        "databricks",
        "pyspark",
        "delta-lake",
        "big-data"
      ],
      "persona": "de",
      "file_path": "data_eng/databricks_pyspark.md",
      "file_name": "databricks_pyspark.md"
    }
  },
  {
    "id": "databricks_pyspark_2",
    "text": "business logic and aggregations\ngold_df = cleaned_df.groupBy(\"user_id\", \"date\").agg(\n    count(\"*\").alias(\"event_count\"),\n    sum(\"value\").alias(\"total_value\"),\n    avg(\"value\").alias(\"avg_value\")\n)\n\n# Store in Gold layer\ngold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.user_daily_metrics\")\n```\n\n## Cost Optimization\n\n### 1. Cluster Management\n- **Auto-scaling**: Use Databricks auto-scaling for variable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Cluster Pools**: Pre-allocate clusters for faster startup\n- **Termination Policies**: Auto-terminate idle clusters\n\n### 2. Storage Optimization\n- **Compression**: Use Parquet with Snappy compression\n- **Partitioning**: Partition large tables by date or key columns\n- **File Sizing**: Optimize file sizes (128MB-1GB per file)\n- **Delta Optimization**: Regular OPTIMIZE and VACUUM operations\n\n### 3. Query Optimization\n- **Predicate Pushdown**: Use filters early in the query\n- **Column Pruning**: Select only needed columns\n- **Join Optimization**: Use broadcast joins for small tables\n- **Caching**: Cache frequently accessed DataFrames\n\n## Security and Governance\n\n### 1. Access Control\n- **Unity Catalog**: Centralized data governance\n- **Table ACLs**: Fine-grained access control\n- **Column-level Security**: Mask sensitive columns\n- **Row-level Security**: Filter data based on user context\n\n### 2. Data Lineage\n- **Lineage Tracking**: Track data transformations\n- **Metadata Management**: Document table schemas and business logic\n- **Impact Analysis**: Understand downstream dependencies\n- **Compliance**: Meet regulatory requirements\n\n### 3. Audit and Monitoring\n- **Query Logging**: Log all data access\n- **Performance Monitoring**: Track query performance\n- **Resource Usage**: Monitor compute and storage usage\n- **Cost Tracking**: Track costs by user and project",
    "metadata": {
      "tags": [
        "databricks",
        "pyspark",
        "delta-lake",
        "big-data"
      ],
      "persona": "de",
      "file_path": "data_eng/databricks_pyspark.md",
      "file_name": "databricks_pyspark.md"
    }
  },
  {
    "id": "krishna_intro_0",
    "text": "# Krishna - Data Engineer Profile & Experience\n\n## Professional Overview\n\nKrishna is a Data Engineer with extensive experience at Walgreens, specializing in Azure and Databricks technologies. He has built and optimized large-scale data pipelines processing over 10TB of data monthly across multiple business domains.\n\n## Current Role & Responsibilities\n\n### **Primary Focus Areas**\n- **Large-Scale Data Processing**: Handling 10TB+ monthly data volume\n- **Modern Data Architecture**: Implementing medallion architecture (Bronze, Silver, Gold)\n- **Cross-Domain Expertise**: Sales, customer loyalty, pharmacy, and supply chain data\n- **Legacy Migration**: Transitioning from traditional systems to cloud-native solutions\n\n### **Technical Leadership**\n- **Subject Matter Expert**: Databricks and PySpark specialist\n- **Team Mentorship**: Guiding offshore developers on best practices\n- **Code Review**: Ensuring quality and performance standards\n- **Best Practices**: Driving data engineering standards across teams\n\n## Technical Expertise\n\n### **Core Technologies**\n- **Azure Data Factory (ADF)**: Orchestration and workflow management\n- **Databricks**: Notebook development and cluster management\n- **PySpark**: Large-scale data transformations and processing\n- **Delta Lake**: ACID transactions, schema enforcement, time travel\n- **Unity Catalog**: Data governance and access control\n- **Azure DevOps**: CI/CD pipeline implementation\n\n### **Data Architecture Patterns**\n- **Medallion Architecture**: Bronze (raw), Silver (standardized), Gold (modeled)\n- **Delta Lake Features**: Schema evolution, ACID compliance, time travel\n- **Partitioning Strategies**: Date-based, state-based, product-based\n- **Data Quality Frameworks**: Validation, monitoring, and alerting\n\n## Project Experience\n\n### **Walgreens Data Platform Migration**\n**Scope**: End-to-end data engineering for enterprise migration\n**Scale**: 10TB+ monthly processing across multiple business domains\n**Architecture**: Modern medallion architecture implementation\n\n#### **Bronze Layer (Raw Data Ingestion)**\n- Ingested raw data from multiple sources\n- Implemented schema-on-read for flexibility\n- Handled various data formats and structures\n- Set up automated ingestion pipelines\n\n#### **Silver Layer (Standardized Data)**\n- Applied schema enforcement and validation\n- Implemented deduplication logic\n- Data cleansing and standardization\n- Quality checks and error handling\n\n#### **Gold Layer (Business Models)**\n- Built fact and dimension tables\n- Created KPI and reporting models\n- Optimized for business consumption\n- Integrated with Power BI dashboards\n\n## Performance Optimization Achievements\n\n### **Significant Performance Improvements**\n- **Runtime Reduction**: Cut job execution time from 2+ hours to under 40 minutes\n- **Cost Optimization**: 30% reduction in processing costs\n- **SLA Compliance**: Improved pipeline reliability and performance\n\n### **Optimization Techniques**\n- **Partitioning**: Strategic partitioning by date, state, and product\n- **Skew Resolution**: Implemented salting for skewed data joins\n- **Join Optimization**: Broadcast joins for small lookup tables\n- **File Compaction**: Optimized Delta file sizes for better performance\n- **Resource Tuning**: Executor memory and CPU optimization\n\n## Data Governance & Security\n\n### **Unity Catalog Implementation**\n- **Role-Based Access Control**: Granular permissions management\n- **PII Protection**: Data masking and tokenization\n- **Lineage Tracking**: Complete data lineage for audits\n- **Compliance**: Meeting regulatory requirements\n\n### **Security Measures**\n- **Data Masking**: Sensitive field protection (emails, customer IDs)\n- **Tokenization**: Secure handling of PII data\n- **Access Controls**: Business teams access only anonymized data\n- **Audit Trails**: Complete tracking of data access and changes\n\n## CI/CD & DevOps\n\n### **Automated Deployment Pipeline**\n- **Version Control**: Git-based notebook and pipeline management\n- **Environment Management**: Automated deployments across dev, test, prod\n- **Error Reduction**: Eliminated manual deployment errors\n- **Repeatability**: Consistent deployment processes\n\n### **Quality Assurance**\n- **Automated Testing**: CI/CD integration with validation checks\n- **Code Review**: Systematic review processes\n- **Deployment Validation**: Automated testing in each environment\n\n## Collaboration & Mentoring\n\n### **Team Leadership**\n- **Offshore Team Mentorship**: Guided international developers\n- **Technical Training**: PySpark and ADF best practices\n- **Code Reviews**: Ensuring quality and performance standards\n- **Knowledge Sharing**: Documented processes and procedures\n\n### **Stakeholder Engagement**\n- **Business Translation**: Converting KPI requirements to data models\n- **Cross-Team Collaboration**: Working with business and IT teams\n- **Requirements Gathering**: Understanding business needs and priorities\n\n## Monitoring & Data Quality\n\n### **",
    "metadata": {
      "tags": [
        "krishna",
        "walgreens",
        "data-engineer",
        "azure",
        "databricks",
        "pyspark",
        "medallion-architecture",
        "delta-lake",
        "azure-data-factory",
        "unity-catalog",
        "performance-optimization"
      ],
      "persona": "de",
      "file_path": "data_eng/krishna_intro.md",
      "file_name": "krishna_intro.md"
    }
  },
  {
    "id": "krishna_intro_1",
    "text": "Control**: Git-based notebook and pipeline management\n- **Environment Management**: Automated deployments across dev, test, prod\n- **Error Reduction**: Eliminated manual deployment errors\n- **Repeatability**: Consistent deployment processes\n\n### **Quality Assurance**\n- **Automated Testing**: CI/CD integration with validation checks\n- **Code Review**: Systematic review processes\n- **Deployment Validation**: Automated testing in each environment\n\n## Collaboration & Mentoring\n\n### **Team Leadership**\n- **Offshore Team Mentorship**: Guided international developers\n- **Technical Training**: PySpark and ADF best practices\n- **Code Reviews**: Ensuring quality and performance standards\n- **Knowledge Sharing**: Documented processes and procedures\n\n### **Stakeholder Engagement**\n- **Business Translation**: Converting KPI requirements to data models\n- **Cross-Team Collaboration**: Working with business and IT teams\n- **Requirements Gathering**: Understanding business needs and priorities\n\n## Monitoring & Data Quality\n\n### **Data Quality Framework**\n- **Validation Checks**: Automated data quality monitoring\n- **Error Logging**: Comprehensive error tracking and reporting\n- **Monitoring Tables**: Centralized quality metrics storage\n- **Real-Time Dashboards**: Power BI integration for business visibility\n\n### **Operational Excellence**\n- **Pipeline Health**: Real-time monitoring of data pipeline status\n- **Business Visibility**: Dashboards for business stakeholders\n- **Proactive Alerting**: Early detection of data quality issues\n- **Performance Tracking**: Continuous optimization monitoring\n\n## Key Achievements & Impact\n\n### **Technical Achievements**\n- **Scale**: Successfully handling 10TB+ monthly data processing\n- **Performance**: 75% reduction in job runtime (2+ hours to 40 minutes)\n- **Cost**: 30% reduction in processing costs through optimization\n- **Reliability**: Improved SLA compliance and system stability\n\n### **Business Impact**\n- **Modernization**: Successfully migrated legacy systems\n- **Governance**: Implemented enterprise-grade data governance\n- **Collaboration**: Enhanced cross-team productivity\n- **Quality**: Improved data quality and business confidence\n\n## Skills Summary\n\n### **Technical Skills**\n- **Big Data**: PySpark, Databricks, Delta Lake\n- **Cloud**: Azure Data Factory, Azure DevOps\n- **Governance**: Unity Catalog, data lineage, security\n- **Performance**: Optimization, tuning, monitoring\n- **DevOps**: CI/CD, version control, automation\n\n### **Soft Skills**\n- **Leadership**: Team mentoring and technical guidance\n- **Communication**: Cross-functional collaboration\n- **Problem Solving**: Complex technical challenges\n- **Project Management**: End-to-end delivery responsibility\n\n## Professional Philosophy\n\nKrishna believes in building reliable, scalable, and maintainable data solutions that serve both technical and business needs. His approach combines hands-on technical expertise with strong collaboration skills to drive successful data platform implementations.\n\n---\n\n*This profile represents Krishna's comprehensive experience in modern data engineering, combining technical depth with business impact and team leadership.*",
    "metadata": {
      "tags": [
        "krishna",
        "walgreens",
        "data-engineer",
        "azure",
        "databricks",
        "pyspark",
        "medallion-architecture",
        "delta-lake",
        "azure-data-factory",
        "unity-catalog",
        "performance-optimization"
      ],
      "persona": "de",
      "file_path": "data_eng/krishna_intro.md",
      "file_name": "krishna_intro.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_0",
    "text": "# Comprehensive Data Engineering Interview Preparation\n\n## Latest Interview Questions & Topics\n\nOver the past few months, I've been actively preparing for Data Engineering interviews, and I thought of sharing some of the latest questions I came across. These focus on Spark, SQL, Python, and Databricks, the core skills for most modern data engineering roles.\n\n### 🔹 Spark Questions\n\n**Explain Spark architecture – how do driver, executors, and cluster manager interact?**\n- Driver program coordinates the job execution and communicates with cluster manager\n- Cluster manager (YARN, Mesos, or standalone) allocates resources across the cluster\n- Executors run tasks and store data in memory/disk, report back to driver\n- Driver sends tasks to executors, collects results, and manages the overall job\n\n**Difference between narrow vs. wide transformations with examples.**\n- Narrow transformations: map(), filter(), union() - no data movement between partitions\n- Wide transformations: groupBy(), join(), distinct() - requires shuffling data across partitions\n- Wide transformations are expensive and should be minimized for performance\n\n**How do you optimize Spark jobs (partitioning, caching, broadcast joins)?**\n- Partition data appropriately based on query patterns (usually date-based)\n- Cache frequently accessed DataFrames in memory\n- Use broadcast joins for small lookup tables\n- Tune spark.sql.shuffle.partitions based on cluster size\n- Use coalesce() instead of repartition() when reducing partitions\n\n**What happens when you run df.explain() in Spark?**\n- Shows the logical and physical execution plans\n- Displays optimization decisions made by Catalyst optimizer\n- Helps identify bottlenecks, unnecessary shuffles, or missing optimizations\n- Can use df.explain(true) for extended physical plan details\n\n### 🔹 SQL Questions\n\n**Write a query to get the second highest salary without using TOP or LIMIT.**\n```sql\n-- Using window function\nSELECT salary \nFROM (\n    SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) as rn\n    FROM employees\n) ranked\nWHERE rn = 2;\n\n-- Using subquery\nSELECT MAX(salary) \nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n\n-- Using self-join\nSELECT DISTINCT e1.salary\nFROM employees e1, employees e2\nWHERE e1.salary < e2.salary\nGROUP BY e1.salary\nHAVING COUNT(*) = 1;\n```\n\n**Explain QUALIFY with an example.**\n```sql\n-- QUALIFY filters results of window functions without subquery\nSELECT name, salary, department,\n       ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rn\nFROM employees\nQUALIFY rn <= 3;  -- Top 3 earners per department\n```\n\n**Difference between INNER JOIN, LEFT JOIN, FULL JOIN, and CROSS JOIN.**\n- INNER JOIN: Only matching rows from both tables\n- LEFT JOIN: All rows from left table + matching rows from right table\n- FULL JOIN: All rows from both tables (union of LEFT and RIGHT)\n- CROSS JOIN: Cartesian product of all rows from both tables\n\n**How do you handle incremental loads in SQL?**\n- Use watermark columns (modified_date, created_date)\n- Implement CDC (Change Data Capture) patterns\n- Use MERGE statements for upsert operations\n- Track last processed timestamp in control tables\n\n### 🔹 Python Questions\n\n**How would you reverse a string without using built-in functions?**\n```python\ndef reverse_string(s):\n    result = \"\"\n    for i in range(len(s) - 1, -1, -1):\n        result += s[i]\n    return result\n\n# Alternative approach\ndef reverse_string_slicing(s):\n    return s[::-1]\n```\n\n**Write code to find duplicates in a list.**\n```python\ndef find_duplicates(lst):\n    seen = set()\n    duplicates = set()\n    for item in lst:\n        if item in seen:\n            duplicates.add(item)\n        else:\n            seen.add(item)\n    return list(duplicates)\n\n# Using collections.Counter\nfrom collections import Counter\ndef find_duplicates_counter(lst):\n    counts = Counter(lst)\n    return [item for item, count in counts.items() if count > 1]\n```\n\n**Difference between @staticmethod, @classmethod, and instance methods.**\n- Instance methods: Take self, access instance attributes\n- Class methods: Take cls, access class attributes, can create new instances\n- Static methods: Don't take self or cls, utility functions that don't need class/instance data\n\n**Explain Python's GIL (Global Interpreter Lock).**\n- GIL prevents multiple threads from executing Python bytecode simultaneously\n- Only one thread can execute Python code at a time\n- Affects CPU-bound tasks but not I/O-bound tasks\n- Can be bypassed using multiprocessing or C extensions\n\n### 🔹 Databricks",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_1",
    "text": "in lst:\n        if item in seen:\n            duplicates.add(item)\n        else:\n            seen.add(item)\n    return list(duplicates)\n\n# Using collections.Counter\nfrom collections import Counter\ndef find_duplicates_counter(lst):\n    counts = Counter(lst)\n    return [item for item, count in counts.items() if count > 1]\n```\n\n**Difference between @staticmethod, @classmethod, and instance methods.**\n- Instance methods: Take self, access instance attributes\n- Class methods: Take cls, access class attributes, can create new instances\n- Static methods: Don't take self or cls, utility functions that don't need class/instance data\n\n**Explain Python's GIL (Global Interpreter Lock).**\n- GIL prevents multiple threads from executing Python bytecode simultaneously\n- Only one thread can execute Python code at a time\n- Affects CPU-bound tasks but not I/O-bound tasks\n- Can be bypassed using multiprocessing or C extensions\n\n### 🔹 Databricks Questions\n\n**How do you implement Bronze-Silver-Gold architecture in Databricks?**\n- Bronze: Raw ingested data with schema flexibility\n- Silver: Cleaned, validated, deduplicated data with enforced schemas\n- Gold: Business-ready aggregated and modeled data for analytics\n- Use Delta Lake for ACID transactions and schema evolution\n\n**Difference between Delta Lake and traditional parquet tables.**\n- Delta Lake: ACID transactions, schema enforcement, time travel, upserts\n- Parquet: Immutable, append-only, no built-in transaction support\n- Delta Lake provides better reliability and governance capabilities\n\n**How do you manage incremental data load using Delta Lake?**\n```python\n# Using MERGE for upserts\ndf.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/path/to/table\")\n\n# Or using merge operation\nfrom delta.tables import DeltaTable\ndeltaTable = DeltaTable.forPath(spark, \"/path/to/table\")\ndeltaTable.alias(\"target\").merge(\n    source_df.alias(\"source\"),\n    \"target.id = source.id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n```\n\n**Explain Unity Catalog and its role in governance.**\n- Centralized metadata store for data governance\n- Provides fine-grained access control at table/column level\n- Enables data lineage tracking and discovery\n- Supports PII masking and row-level security\n- Integrates with external identity providers\n\n## 🔍 Why Most People Struggle in Data Engineering Interviews\n\nI've seen this pattern again and again — candidates know the tools, but the moment the interviewer shifts from \"what is Spark?\" to \"design a pipeline for streaming data,\" things start to fall apart.\n\n**The truth?** Data Engineering interviews test how you think, not just what you know.\n\n### Mental Framework for Interviews:\n\n1️⃣ **SQL is your foundation** → If you can't join, aggregate, and optimize queries, nothing else will stand.\n\n2️⃣ **Model the data** → Understand when to use star schema vs. snowflake, OLTP vs. OLAP, and how to handle Slowly Changing Dimensions.\n\n3️⃣ **Think scale** → Spark, partitioning, shuffles, streaming late-arrival data — these aren't buzzwords, they're the backbone of real-world DE systems.\n\n4️⃣ **Design pipelines, not scripts** → Batch vs. streaming, orchestration with Airflow, partitioning strategies — show that you can think like an architect.\n\n5️⃣ **Zoom out** → Cloud (S3, Redshift, BigQuery, Synapse), data lake vs. warehouse vs. lakehouse — interviews often end with \"how would you design this end-to-end?\"\n\n## Databricks-Specific Interview Preparation\n\nDatabricks is the home of Apache Spark and the Lakehouse Platform. Engineers here focus on real-time data, ML pipelines, and large-scale analytics.\n\n### 10 Key Databricks Questions:\n\n1️⃣ **Explain how Delta Lake improves reliability over a traditional data lake.**\n- ACID transactions ensure data consistency\n- Schema enforcement prevents data corruption\n- Time travel enables data recovery and auditing\n- Upsert capabilities support CDC patterns\n\n2️⃣ **How would you design a streaming pipeline with Structured Streaming in Spark?**\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder.appName(\"StreamingPipeline\").getOrCreate()\n\n# Read from Kafka\nstreaming_df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"events\") \\\n    .load()\n\n# Process and write to Delta Lake\nprocessed_df = streaming_df.select(\n    from_json(col(\"value\").cast(\"string\"),",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_2",
    "text": "real-time data, ML pipelines, and large-scale analytics.\n\n### 10 Key Databricks Questions:\n\n1️⃣ **Explain how Delta Lake improves reliability over a traditional data lake.**\n- ACID transactions ensure data consistency\n- Schema enforcement prevents data corruption\n- Time travel enables data recovery and auditing\n- Upsert capabilities support CDC patterns\n\n2️⃣ **How would you design a streaming pipeline with Structured Streaming in Spark?**\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder.appName(\"StreamingPipeline\").getOrCreate()\n\n# Read from Kafka\nstreaming_df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"events\") \\\n    .load()\n\n# Process and write to Delta Lake\nprocessed_df = streaming_df.select(\n    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\nquery = processed_df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\n    .outputMode(\"append\") \\\n    .start(\"/delta/events\")\n```\n\n3️⃣ **Walk me through optimizing a Spark job handling terabytes of data.**\n- Profile with Spark UI to identify bottlenecks\n- Optimize partitioning strategy (usually date-based)\n- Use broadcast joins for small lookup tables\n- Implement salting for skewed joins\n- Enable Adaptive Query Execution (AQE)\n- Cache frequently accessed DataFrames\n- Use columnar formats like Parquet/Delta\n\n4️⃣ **How would you enable ACID transactions in a big data pipeline?**\n- Use Delta Lake for ACID compliance\n- Implement proper partitioning strategies\n- Use MERGE operations for upserts\n- Configure appropriate isolation levels\n- Handle concurrent writes with optimistic concurrency control\n\n5️⃣ **Explain how you'd manage schema evolution with Delta Lake.**\n```python\n# Enable schema evolution\ndf.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .save(\"/delta/table\")\n\n# Or manually evolve schema\nspark.sql(\"ALTER TABLE delta_table ADD COLUMN new_column STRING\")\n```\n\n6️⃣ **What's your strategy for scaling ML model training pipelines in Databricks?**\n- Use MLflow for experiment tracking and model versioning\n- Implement distributed training with Spark MLlib\n- Use GPU clusters for deep learning workloads\n- Cache feature engineering results\n- Implement model serving with MLflow Model Registry\n\n7️⃣ **How would you integrate Databricks with external BI tools like Tableau or Power BI?**\n- Create SQL endpoints for direct query access\n- Use JDBC/ODBC connectors for BI tools\n- Implement data marts with optimized schemas\n- Use Delta Lake for consistent data access\n- Configure appropriate access controls\n\n8️⃣ **Describe how you'd secure a data lakehouse in a multi-tenant environment.**\n- Implement Unity Catalog for centralized governance\n- Use workspace-level isolation\n- Configure fine-grained access controls\n- Implement PII masking and encryption\n- Use Azure Active Directory integration\n\n9️⃣ **How do you debug performance bottlenecks in Spark?**\n- Use Spark UI to analyze DAG and stage execution\n- Check for data skew in shuffle operations\n- Monitor memory usage and garbage collection\n- Profile with Spark History Server\n- Use query plans to identify optimization opportunities\n\n🔟 **Tell me about a time you balanced speed vs. cost in large-scale data jobs.**\n- Implemented incremental processing instead of full loads\n- Used spot instances for non-critical workloads\n- Optimized partitioning to reduce shuffle overhead\n- Implemented auto-scaling based on workload patterns\n- Used columnar storage formats for better compression\n\n## Advanced Data Engineering Interview Questions\n\n### 20 Comprehensive Questions:\n\n1️⃣ **Write an SQL query to find the second highest salary from an employee table.**\n```sql\nSELECT MAX(salary) as second_highest_salary\nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n```\n\n2️⃣ **How do you handle NULL values in SQL joins while ensuring data integrity?**\n- Use COALESCE() or ISNULL() to provide default values\n- Implement proper NULL handling in business logic\n- Use INNER JOIN to exclude NULL matches if appropriate\n- Consider LEFT JOIN with NULL checks for data validation\n\n3️⃣ **Write an SQL query to calculate customer churn rate over the last 6 months.**\n```sql\nWITH customer_activity AS (\n    SELECT customer_id,\n           MAX(order_date) as last_order_date,\n           CASE WHEN MAX(order_date)",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_3",
    "text": "on workload patterns\n- Used columnar storage formats for better compression\n\n## Advanced Data Engineering Interview Questions\n\n### 20 Comprehensive Questions:\n\n1️⃣ **Write an SQL query to find the second highest salary from an employee table.**\n```sql\nSELECT MAX(salary) as second_highest_salary\nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n```\n\n2️⃣ **How do you handle NULL values in SQL joins while ensuring data integrity?**\n- Use COALESCE() or ISNULL() to provide default values\n- Implement proper NULL handling in business logic\n- Use INNER JOIN to exclude NULL matches if appropriate\n- Consider LEFT JOIN with NULL checks for data validation\n\n3️⃣ **Write an SQL query to calculate customer churn rate over the last 6 months.**\n```sql\nWITH customer_activity AS (\n    SELECT customer_id,\n           MAX(order_date) as last_order_date,\n           CASE WHEN MAX(order_date) < DATE_SUB(CURRENT_DATE, INTERVAL 6 MONTH) \n                THEN 1 ELSE 0 END as is_churned\n    FROM orders\n    GROUP BY customer_id\n)\nSELECT \n    COUNT(*) as total_customers,\n    SUM(is_churned) as churned_customers,\n    ROUND(SUM(is_churned) * 100.0 / COUNT(*), 2) as churn_rate_percentage\nFROM customer_activity;\n```\n\n4️⃣ **Design a fact table for an e-commerce platform – what dimensions and measures would you include?**\n- **Dimensions**: Date, Customer, Product, Store, Payment Method\n- **Measures**: Sales Amount, Quantity, Discount, Tax, Shipping Cost\n- **Grain**: One row per transaction line item\n- **Design**: Star schema with fact table in center\n\n5️⃣ **Explain the difference between star schema and snowflake schema and when to choose each.**\n- **Star Schema**: Single fact table with denormalized dimensions\n- **Snowflake Schema**: Normalized dimensions with multiple levels\n- **Choose Star**: When query performance is priority, simpler to understand\n- **Choose Snowflake**: When storage efficiency matters, complex hierarchies\n\n6️⃣ **Write a Python script to validate data quality and detect anomalies before loading.**\n```python\ndef validate_data_quality(df):\n    validation_results = {}\n    \n    # Check for nulls in critical columns\n    critical_columns = ['customer_id', 'order_date', 'amount']\n    for col in critical_columns:\n        null_count = df.filter(col(col).isNull()).count()\n        validation_results[f'{col}_nulls'] = null_count\n    \n    # Check for duplicates\n    duplicate_count = df.count() - df.dropDuplicates().count()\n    validation_results['duplicates'] = duplicate_count\n    \n    # Check for outliers (amount > 3 standard deviations)\n    amount_stats = df.select(mean('amount'), stddev('amount')).collect()[0]\n    outlier_count = df.filter(col('amount') > (amount_stats[0] + 3 * amount_stats[1])).count()\n    validation_results['outliers'] = outlier_count\n    \n    return validation_results\n```\n\n7️⃣ **In PySpark, how would you efficiently join two very large DataFrames to avoid skew?**\n```python\n# Method 1: Broadcast small DataFrame\nfrom pyspark.sql.functions import broadcast\nresult = large_df.join(broadcast(small_df), \"key\")\n\n# Method 2: Salting for skewed keys\ndef salt_key(df, salt_buckets=10):\n    return df.withColumn(\"salt\", (rand() * salt_buckets).cast(\"int\")) \\\n             .withColumn(\"salted_key\", concat(col(\"key\"), lit(\"_\"), col(\"salt\")))\n\n# Method 3: Repartition before join\ndf1_repartitioned = df1.repartition(200, \"join_key\")\ndf2_repartitioned = df2.repartition(200, \"join_key\")\nresult = df1_repartitioned.join(df2_repartitioned, \"join_key\")\n```\n\n8️⃣ **Write PySpark code to find the top 3 customers by revenue per region.**\n```python\nfrom pyspark.sql.functions import sum, rank, col\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"region\").orderBy(col(\"total_revenue\").desc())\n\ntop_customers = df.groupBy(\"customer_id\", \"region\") \\\n    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n    .withColumn(\"rank\", rank().over(window_spec)) \\\n    .filter(col(\"rank\") <= 3) \\\n    .orderBy(\"region\", \"rank\")\n```\n\n9️⃣ **You are processing real-time data from Event Hub into Delta tables. How would you implement",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_4",
    "text": ".repartition(200, \"join_key\")\ndf2_repartitioned = df2.repartition(200, \"join_key\")\nresult = df1_repartitioned.join(df2_repartitioned, \"join_key\")\n```\n\n8️⃣ **Write PySpark code to find the top 3 customers by revenue per region.**\n```python\nfrom pyspark.sql.functions import sum, rank, col\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"region\").orderBy(col(\"total_revenue\").desc())\n\ntop_customers = df.groupBy(\"customer_id\", \"region\") \\\n    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n    .withColumn(\"rank\", rank().over(window_spec)) \\\n    .filter(col(\"rank\") <= 3) \\\n    .orderBy(\"region\", \"rank\")\n```\n\n9️⃣ **You are processing real-time data from Event Hub into Delta tables. How would you implement this?**\n```python\n# Structured Streaming from Event Hub\nstreaming_df = spark.readStream \\\n    .format(\"eventhubs\") \\\n    .options(**event_hub_config) \\\n    .load()\n\n# Process and write to Delta Lake\nprocessed_stream = streaming_df.select(\n    from_json(col(\"body\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\nquery = processed_stream.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\n    .outputMode(\"append\") \\\n    .trigger(processingTime='10 seconds') \\\n    .start(\"/delta/real_time_data\")\n```\n\n🔟 **How do you implement schema evolution in Delta Lake without breaking existing jobs?**\n- Use `mergeSchema=true` option when writing\n- Add new columns with default values\n- Use `ALTER TABLE` for explicit schema changes\n- Test schema changes in development first\n- Use version control for schema definitions\n\n1️⃣1️⃣ **How would you implement Slowly Changing Dimensions (SCD Type 2) in a data warehouse?**\n```python\n# SCD Type 2 implementation with Delta Lake\ndef implement_scd_type2(current_df, historical_df, business_key):\n    # Add versioning columns\n    current_with_version = current_df.withColumn(\"effective_date\", current_date()) \\\n                                   .withColumn(\"end_date\", lit(None)) \\\n                                   .withColumn(\"is_current\", lit(True))\n    \n    # Merge logic\n    delta_table = DeltaTable.forPath(spark, historical_table_path)\n    delta_table.alias(\"target\").merge(\n        current_with_version.alias(\"source\"),\n        f\"target.{business_key} = source.{business_key}\"\n    ).whenMatchedUpdate(\n        condition=\"target.is_current = true AND target.hash_key != source.hash_key\",\n        set={\n            \"end_date\": current_date(),\n            \"is_current\": \"false\"\n        }\n    ).whenNotMatchedInsertAll().execute()\n```\n\n1️⃣2️⃣ **Late-arriving data is detected in a batch pipeline – how do you ensure correctness of historical reporting?**\n- Implement watermarking with configurable retention periods\n- Use Delta Lake's time travel for reprocessing\n- Design fact tables to handle late-arriving facts\n- Implement data quality monitoring for late arrivals\n- Use surrogate keys for dimension lookups\n\n1️⃣3️⃣ **How do you design a pipeline that supports both batch and streaming workloads simultaneously?**\n- Use Lambda architecture with batch and speed layers\n- Implement unified data models in Delta Lake\n- Use Structured Streaming for real-time processing\n- Batch layer handles historical data and corrections\n- Merge both layers for complete analytics\n\n1️⃣4️⃣ **What is your approach to building incremental data loads in Azure Data Factory pipelines?**\n- Use watermark columns to track last processed data\n- Implement lookup activities to retrieve watermarks\n- Use conditional activities for incremental vs. full loads\n- Store watermarks in Azure SQL Database or Key Vault\n- Implement error handling and retry policies\n\n1️⃣5️⃣ **Your PySpark job is failing due to skewed joins. Walk through your debugging and optimization steps.**\n1. Identify skew using Spark UI (look for tasks taking much longer)\n2. Analyze data distribution on join keys\n3. Apply salting technique for skewed keys\n4. Use broadcast joins for small tables\n5. Repartition data before joins\n6. Enable Adaptive Query Execution (AQE)\n\n1️⃣6️⃣ **Explain the architecture of Azure Databricks integrated with Delta Lake in a production environment.**\n- Databricks workspace with Unity Catalog for governance\n- Azure Data Lake Storage Gen2 for data",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "interview_prep_comprehensive_5",
    "text": "Azure Data Factory pipelines?**\n- Use watermark columns to track last processed data\n- Implement lookup activities to retrieve watermarks\n- Use conditional activities for incremental vs. full loads\n- Store watermarks in Azure SQL Database or Key Vault\n- Implement error handling and retry policies\n\n1️⃣5️⃣ **Your PySpark job is failing due to skewed joins. Walk through your debugging and optimization steps.**\n1. Identify skew using Spark UI (look for tasks taking much longer)\n2. Analyze data distribution on join keys\n3. Apply salting technique for skewed keys\n4. Use broadcast joins for small tables\n5. Repartition data before joins\n6. Enable Adaptive Query Execution (AQE)\n\n1️⃣6️⃣ **Explain the architecture of Azure Databricks integrated with Delta Lake in a production environment.**\n- Databricks workspace with Unity Catalog for governance\n- Azure Data Lake Storage Gen2 for data persistence\n- Azure Key Vault for secrets management\n- Azure Active Directory for authentication\n- Azure Monitor for logging and monitoring\n- CI/CD pipeline with Azure DevOps\n\n1️⃣7️⃣ **How do you optimize query performance and concurrency in a Synapse dedicated SQL pool?**\n- Implement proper distribution strategies (hash, round-robin, replicated)\n- Use columnstore indexes for analytical workloads\n- Optimize statistics and create covering indexes\n- Implement workload management with resource classes\n- Use result set caching for repeated queries\n\n1️⃣8️⃣ **Your data lake contains sensitive PII data. What best practices do you follow to secure data and manage secrets?**\n- Implement encryption at rest and in transit\n- Use Azure Key Vault for secret management\n- Apply row-level security and column-level encryption\n- Implement data masking for non-production environments\n- Use Azure Purview for data classification and lineage\n- Regular security audits and access reviews\n\n1️⃣9️⃣ **How do you establish end-to-end data lineage and governance using Microsoft Purview?**\n- Register data sources (Azure Data Lake, SQL databases, etc.)\n- Configure automated scanning for schema discovery\n- Set up data classification and sensitivity labels\n- Implement access policies and data retention rules\n- Create lineage maps showing data flow\n- Enable compliance reporting and auditing\n\n2️⃣0️⃣ **Design an end-to-end analytics pipeline that ingests from Event Hub, processes data in Databricks, stores it in Synapse, and powers dashboards in Power BI.**\n```\nEvent Hub → Azure Data Factory → Databricks (Streaming) → Delta Lake → \nSynapse (via PolyBase/Copy) → Power BI → Business Users\n\nComponents:\n- Event Hub: Real-time data ingestion\n- ADF: Orchestration and scheduling\n- Databricks: Stream processing and ML\n- Delta Lake: ACID transactions and schema evolution\n- Synapse: Data warehouse and analytics\n- Power BI: Visualization and reporting\n- Azure Key Vault: Secrets management\n- Azure Monitor: Monitoring and alerting\n```\n\n## Key Success Factors for Data Engineering Interviews\n\n1. **Technical Depth**: Understand the \"why\" behind tools and technologies\n2. **System Design**: Think about scalability, reliability, and maintainability\n3. **Problem Solving**: Break down complex problems into manageable pieces\n4. **Communication**: Explain technical concepts clearly and concisely\n5. **Real-world Experience**: Draw from actual project experiences and challenges\n6. **Continuous Learning**: Stay updated with latest trends and best practices\n\nRemember: Interviewers want to see how you think through problems, not just memorized answers. Focus on demonstrating your analytical approach and practical experience.",
    "metadata": {
      "tags": [
        "interview",
        "preparation",
        "comprehensive",
        "databricks",
        "sql",
        "spark",
        "python"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_prep_comprehensive.md",
      "file_name": "interview_prep_comprehensive.md"
    }
  },
  {
    "id": "infa_migration_examples_0",
    "text": "# Informatica Migration Examples\n\n## Example: Expression + Filter + Lookup → PySpark\n\n```python\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nsrc = (spark.read.format(\"jdbc\")\n  .option(\"url\", jdbc_url)\n  .option(\"dbtable\", \"SRC_ORDERS\")\n  .load())\n\nlkp = (spark.read.format(\"jdbc\")\n  .option(\"url\", jdbc_url)\n  .option(\"dbtable\", \"DIM_PRODUCT\")\n  .load())\n\n# Expression (IIF) & standardization\ncur = (src\n  .withColumn(\"TRIM_SKU\", F.trim(\"SKU\"))\n  .withColumn(\"ORDER_DT\", F.to_timestamp(\"ORDER_TS\"))\n  .withColumn(\"IS_PRIORITY\",\n              F.when(F.col(\"PRIORITY_FLAG\") == \"Y\", F.lit(1)).otherwise(F.lit(0)))\n  .filter(\"STATUS = 'COMPLETE'\"))\n\n# Lookup (broadcast) + join\ncur = (cur.join(F.broadcast(lkp.select(\"SKU\",\"PRODUCT_ID\")), on=\"SKU\", how=\"left\")\n          .fillna({\"PRODUCT_ID\": -1}))\n\n# Surrogate key (sequence-like)\nw = Window.orderBy(F.monotonically_increasing_id())\ncur = cur.withColumn(\"LINE_ID\", F.row_number().over(w))\n\n# Final projection\nout_df = cur.select(\"LINE_ID\", \"ORDER_ID\", \"PRODUCT_ID\", \"ORDER_DT\", \"IS_PRIORITY\")\n```\n\n## Example: Write to Snowflake via Connector\n\n```python\nsfOptions = {\n  \"sfURL\": SF_URL,\n  \"sfUser\": SF_USER,\n  \"sfPassword\": SF_PWD,     # or OAuth\n  \"sfDatabase\": \"SALES\",\n  \"sfSchema\": \"CURATED\",\n  \"sfWarehouse\": \"WH_XL\",\n  \"sfRole\": \"SYSADMIN\"\n}\n(out_df\n  .write.format(\"snowflake\")\n  .options(**sfOptions)\n  .option(\"dbtable\", \"ORDERS_STAGE\")\n  .mode(\"append\")\n  .save())\n```\n\n## Example: Stage + COPY Pattern\n\n* Write partitioned Parquet to ADLS/S3 (e.g., by `order_dt`).\n* Call Snowflake:\n\n  ```sql\n  COPY INTO SALES.CURATED.ORDERS\n  FROM @ext_stage/orders/\n  FILE_FORMAT=(TYPE=PARQUET)\n  MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n  ON_ERROR=ABORT_STATEMENT;\n  ```\n* Wrap in task/pipe for automation.\n\n## Example: Aggregator → PySpark\n\n```python\n# Informatica Aggregator becomes:\nagg_df = (src_df\n  .groupBy(\"STORE_ID\", \"PRODUCT_CATEGORY\")\n  .agg(\n    F.sum(\"SALES_AMOUNT\").alias(\"TOTAL_SALES\"),\n    F.count(\"*\").alias(\"TRANSACTION_COUNT\"),\n    F.avg(\"UNIT_PRICE\").alias(\"AVG_PRICE\"),\n    F.max(\"SALE_DATE\").alias(\"LAST_SALE\")\n  )\n  .filter(\"TOTAL_SALES > 1000\"))  # Having clause\n```\n\n## Example: Router → Multiple Outputs\n\n```python\n# Router with multiple conditions\nhigh_value = df.filter(\"SALES_AMOUNT > 10000\")\nmedium_value = df.filter(\"SALES_AMOUNT BETWEEN 1000 AND 10000\")\nlow_value = df.filter(\"SALES_AMOUNT < 1000\")\n\n# Write to different targets\nhigh_value.write.mode(\"append\").saveAsTable(\"high_value_sales\")\nmedium_value.write.mode(\"append\").saveAsTable(\"medium_value_sales\")\nlow_value.write.mode(\"append\").saveAsTable(\"low_value_sales\")\n```\n\n## Example: Sequence Generator → Window Functions\n\n```python\n# Informatica sequence becomes:\nw = Window.partitionBy(\"ORDER_ID\").orderBy(\"LINE_NUMBER\")\ndf_with_seq = df.withColumn(\"SEQUENCE_ID\", F.row_number().over(w))\n\n# Or for Snowflake sequences on write:\ndf_with_seq = df.withColumn(\"SEQUENCE_ID\", F.expr(\"NEXTVAL('order_seq')\"))\n```",
    "metadata": {
      "tags": [
        "data-eng",
        "examples",
        "mapping",
        "pyspark",
        "snowflake"
      ],
      "persona": "de",
      "file_path": "data_eng/infa_migration_examples.md",
      "file_name": "infa_migration_examples.md"
    }
  },
  {
    "id": "sql_interview_questions_0",
    "text": "# SQL Interview Questions & Solutions\n\n## Finding Second Highest Salary Without TOP/LIMIT\n\n### Method 1: Using Subquery\n```sql\nSELECT MAX(salary) \nFROM employees \nWHERE salary < (SELECT MAX(salary) FROM employees);\n```\n\n### Method 2: Using Window Functions (RANK)\n```sql\nSELECT salary \nFROM (\n    SELECT salary, RANK() OVER (ORDER BY salary DESC) as salary_rank\n    FROM employees\n) ranked_salaries \nWHERE salary_rank = 2;\n```\n\n### Method 3: Using Window Functions (ROW_NUMBER)\n```sql\nSELECT salary \nFROM (\n    SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) as salary_rank\n    FROM employees\n) ranked_salaries \nWHERE salary_rank = 2;\n```\n\n### Method 4: Using Self Join\n```sql\nSELECT DISTINCT e1.salary\nFROM employees e1\nWHERE 1 = (SELECT COUNT(DISTINCT e2.salary) \n           FROM employees e2 \n           WHERE e2.salary > e1.salary);\n```\n\n## Common SQL Interview Patterns\n\n### Nth Highest Salary (Generic)\n```sql\nSELECT salary \nFROM (\n    SELECT salary, DENSE_RANK() OVER (ORDER BY salary DESC) as salary_rank\n    FROM employees\n) ranked_salaries \nWHERE salary_rank = N;\n```\n\n### Find Duplicates\n```sql\nSELECT column_name, COUNT(*)\nFROM table_name\nGROUP BY column_name\nHAVING COUNT(*) > 1;\n```\n\n### Delete Duplicates (Keep One)\n```sql\nDELETE e1 FROM employees e1\nINNER JOIN employees e2 \nWHERE e1.id > e2.id AND e1.email = e2.email;\n```\n\n### Find Employees with Same Salary\n```sql\nSELECT e1.name, e1.salary\nFROM employees e1\nINNER JOIN employees e2 ON e1.salary = e2.salary AND e1.id != e2.id;\n```",
    "metadata": {
      "tags": [
        "sql",
        "interview",
        "queries",
        "window-functions",
        "subqueries"
      ],
      "persona": "de",
      "file_path": "data_eng/sql_interview_questions.md",
      "file_name": "sql_interview_questions.md"
    }
  },
  {
    "id": "data_engineer_questions_0",
    "text": "# Data Engineering Interview Questions & Answers\n\n## Core PySpark + Databricks Questions\n\n### Q1. What is your experience with PySpark in your current project?\n**A:** In my Walgreens project, I built end-to-end PySpark pipelines in Databricks that processed over 10TB monthly. I handled ingestion of raw data into Bronze, applied transformations and schema validation in Silver, and modeled fact/dimension tables in Gold for reporting. I frequently used PySpark DataFrame APIs, window functions, UDFs, and joins. For example, I implemented deduplication logic using `dropDuplicates()` and window functions, ensuring the most recent transaction record is retained.\n\n### Q2. How do you handle large datasets in PySpark efficiently?\n**A:** First, I design partitioning strategies at ingestion (date-based partitions). Second, I tune Spark configurations like `spark.sql.shuffle.partitions` and executor memory. Third, I use optimizations such as broadcast joins for small lookup tables and salting for skewed keys. For large jobs, I enable Adaptive Query Execution. In one case, these changes reduced a pipeline runtime from 2+ hours to 40 minutes.\n\n### Q3. How do you debug PySpark jobs when they fail?\n**A:** I start with Spark UI to analyze DAGs, stages, and tasks. I check for skew (few tasks taking much longer) or memory errors. Then I check logs in Databricks for stack traces. I usually replay the job with a smaller dataset to isolate the failing transformation. For example, when a job failed due to a corrupt record, I added `_corrupt_record` handling and moved bad rows into a quarantine table.\n\n### Q4. How do you handle nested JSON in PySpark?\n**A:** I define a `StructType` schema, use `from_json` to parse the string column, and `explode` for arrays. Then I flatten with `withColumn` to extract nested attributes. For deeply nested JSON, I use recursive functions to normalize. Finally, I store it as a clean Delta table in Silver for downstream consumption.\n\n### Q5. How do you handle duplicates in PySpark?\n**A:** For simple duplicates, I use `dropDuplicates()`. For business-specific deduplication, I use window functions with `row_number()` ordered by timestamp, keeping only the latest row. In Walgreens, this was used for handling multiple prescription updates from pharmacy systems.\n\n### Q6. How do you handle schema drift in PySpark pipelines?\n**A:** In Bronze, I ingest with permissive mode to avoid job failure. In Silver, I enforce strict schema. If new columns appear, I add them with defaults in Silver after business validation. For Delta tables, I use `mergeSchema` in controlled deployments, never blindly. This allows flexibility but avoids breaking downstream queries.\n\n## Advanced PySpark & Delta Lake Questions\n\n### Q7. How do you design incremental loads in PySpark?\n**A:** I use watermarking (modified_date column) or surrogate keys. ADF passes parameters to notebooks for last processed date. In PySpark, I filter only incremental rows and apply Delta merge to update/inserts. This reduced daily runs from processing 10TB full to ~300GB delta, saving cost and runtime.\n\n### Q8. Can you explain time travel in Delta Lake? How have you used it?\n**A:** Time travel lets me query data at a specific version or timestamp. At Walgreens, one job overwrote 2 days of data. Using `VERSION AS OF` in Delta, I restored the table to its previous state in minutes without reloading raw files.\n\n### Q9. How do you handle slowly changing dimensions (SCD) in Databricks?\n**A:** I used Delta merge for Type 2. Old record is closed with an end date, new record inserted with active flag = 1. This keeps historical changes. For example, when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use PERMISSIVE mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.\n\n### Q13",
    "metadata": {
      "tags": [
        "pyspark",
        "databricks",
        "data-engineering",
        "etl",
        "dataframe",
        "delta-lake",
        "json",
        "schema-drift",
        "optimization",
        "debugging",
        "walgreens",
        "azure-data-factory"
      ],
      "persona": "de",
      "file_path": "data_eng/data_engineer_questions.md",
      "file_name": "data_engineer_questions.md"
    }
  },
  {
    "id": "data_engineer_questions_1",
    "text": "when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use PERMISSIVE mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.\n\n### Q13. How do you test PySpark pipelines?\n**A:** Unit tests validate transformations with small sample data. Row count reconciliation checks ingestion completeness. Schema validation checks enforce consistency. We automated these checks in CI/CD pipelines.\n\n### Q14. How do you manage dependencies across notebooks in Databricks?\n**A:** I modularize common logic (like validations, schema enforcement) in utility notebooks or .py files stored in repos. Then I import them into main notebooks. This avoids code duplication and keeps pipelines maintainable.\n\n### Q15. How do you handle late arriving data?\n**A:** I use watermarking in Delta tables, so late data is still merged if within X days. If outside retention, we load them manually after business approval.\n\n## Performance Optimization & Best Practices\n\n### Q16. How do you handle large joins across multiple datasets?\n**A:** First, partition both datasets on the join key. If one is small, broadcast it. If skew occurs, apply salting. If still heavy, break into smaller joins and cache intermediate results.\n\n### Q17. How do you manage PySpark code for reusability?\n**A:** I follow modular design: separate ingestion, transformation, validation, and load functions. I store configs in parameter files, not hardcoded. Reusable frameworks allowed offshore to easily plug in new sources with minimal code.\n\n### Q18. How do you optimize PySpark DataFrame transformations?\n**A:** Avoid wide transformations until necessary, use select instead of *, cache intermediate results when reused, and avoid UDFs unless unavoidable. Vectorized operations (pandas UDFs) are faster than row-wise ones.\n\n### Q19. How do you manage error handling in PySpark?\n**A:** I wrap critical transformations with try/except. Failures are logged into error tables. In ADF, we configure retries and failure alerts. This ensures job doesn't fail silently.\n\n### Q20. What's the biggest challenge you solved with PySpark at Walgreens?\n**A:** Optimizing a 10TB sales fact pipeline that originally took 2+ hours. By tuning partitioning, salting skewed joins, and compacting files, I reduced runtime to 40 minutes. This improved SLA compliance and cut costs by 30%.\n\n## Key Takeaways\n\n- **Performance**: Proper partitioning, broadcast joins, and adaptive query execution are crucial\n- **Data Quality**: Implement comprehensive error handling and data validation\n- **Monitoring**: Log metrics and create dashboards for pipeline health\n- **Testing**: Unit tests and automated validation ensure pipeline reliability\n- **Code Organization**: Modular design improves maintainability and reusability\n- **Delta Lake**: Leverage time travel and merge capabilities for data management",
    "metadata": {
      "tags": [
        "pyspark",
        "databricks",
        "data-engineering",
        "etl",
        "dataframe",
        "delta-lake",
        "json",
        "schema-drift",
        "optimization",
        "debugging",
        "walgreens",
        "azure-data-factory"
      ],
      "persona": "de",
      "file_path": "data_eng/data_engineer_questions.md",
      "file_name": "data_engineer_questions.md"
    }
  },
  {
    "id": "informatica_to_pyspark_0",
    "text": "# Informatica → PySpark Migration Playbook\n\n## Overall Approach\n1) **Inventory & lineage**: export mapping XML / repository report; list source/target tables, lookups, joins, aggregations, filters, expressions, sequences.\n2) **Semantics parity**: for each mapping, capture row-order assumptions, null-handling, case-sensitivity, timezone, numeric precision/scale, surrogate keys.\n3) **Transform translation**: map each Informatica transformation → PySpark function (see table).\n4) **Framework**: build a small PySpark \"operator\" layer so pipelines are declarative (config-driven).\n5) **Testing**: golden-data tests (row/column counts, nulls, min/max, checksums), sample-based parity, KPI comparisons.\n6) **Orchestration**: ADF or Databricks Workflows; parameterize envs; promote via DevOps CI/CD.\n7) **Performance**: partitioning, broadcast/AQE, file compaction; pushdown where possible.\n8) **Snowflake load**: choose **Spark → Snowflake connector** (batch) or **stage + COPY** (Snowpipe/auto-ingest) for high-throughput and decoupling.\n\n## Common Transformation Mapping\n- **Source Qualifier** → `spark.read` with predicates; pushdown via JDBC filter if possible.\n- **Expression** (IIF/DECODE/SUBSTR/UPPER) → `when/otherwise`, `expr()`, `substring`, `upper`, `coalesce`.\n- **Filter** → `df.filter(\"...\")`.\n- **Lookup** → left join with a broadcast-hint; handle not-found as default/null + `when` logic.\n- **Joiner** → `df.join(other, keys, \"inner/left/right/full\")`; use `broadcast()` for small tables.\n- **Aggregator** → `groupBy(...).agg(...)` with careful null-handling and data types.\n- **Router** → multiple filters writing to multiple sinks.\n- **Sequence Generator** → window functions with `row_number()` or Snowflake sequences on write.\n- **Union** → `unionByName` with `allowMissingColumns=True`.\n- **Sorter/Rank** → `orderBy`, window specs with `row_number`, `dense_rank`.\n\n## Null & Datatype Rules\n- Match Informatica's null rules explicitly; use `coalesce`, cast to target schema, standardize timestamp TZ and string trimming.\n\n## Incremental Patterns\n- Watermark on `last_update_ts`; Bronze → Silver Delta with `MERGE INTO`; or CDC using file-based change tables.\n- For Snowflake targets, prefer **idempotent upserts** using stage + `COPY INTO` to temp table + `MERGE`.\n\n## Migration Strategy\nStart with simple mappings (single source, basic transformations), validate parity, then tackle complex joins and aggregations. Use a staging approach where you run both systems in parallel for validation before switching over.",
    "metadata": {
      "tags": [
        "data-eng",
        "informatica",
        "migration",
        "pyspark",
        "mapping",
        "transformations",
        "snowflake"
      ],
      "persona": "de",
      "file_path": "data_eng/informatica_to_pyspark.md",
      "file_name": "informatica_to_pyspark.md"
    }
  },
  {
    "id": "snowflake_load_patterns_0",
    "text": "# Snowflake Load Patterns\n\n## A) Spark → Snowflake Connector (Simple)\n- Use `net.snowflake:spark-snowflake` + `net.snowflake:snowflake-jdbc`.\n- Write mode: `append` to a staging table; finalize with Snowflake-side `MERGE`.\n- Good for medium volumes; keeps pipeline in one place (Databricks).\n\n**Example write:**\n- options: `sfURL`, `sfUser`, `sfPassword`/`sfOAuth`, `sfDatabase`, `sfSchema`, `sfWarehouse`, `sfRole`.\n- `df.write.format(\"snowflake\").options(**sfOptions).option(\"dbtable\",\"STAGE_TABLE\").mode(\"append\").save()`\n\n## B) External Stage + COPY INTO (High-throughput / Decoupled)\n1) Spark writes Parquet to cloud storage (ADLS/S3) with partitioning.\n2) Snowflake `COPY INTO <table>` from the external stage with file pattern.\n3) Automate with **Snowpipe** or orchestration tool; track file manifests / checkpoints.\n\n**Pros:** better parallelism, resilient to retries, clear separation of compute concerns.\n**Cons:** more moving parts (stages, pipes, notifications).\n\n## Performance & Cost Tips\n- Use `COPY INTO` with `ON_ERROR='CONTINUE'` to isolate bad files; quarantine & reprocess.\n- Cluster keys only where pruning helps. Avoid over-clustering.\n- Compress & partition data output; align Snowflake micro-partitions with query patterns.\n- Prefer `MERGE` with small change sets; if very large upsert, consider swap: load to temp + swap/rename.\n\n## Load Patterns by Use Case\n- **Batch loads**: Spark connector for simplicity, stage+COPY for high volume\n- **Real-time**: Snowpipe with auto-ingest from cloud storage\n- **Upserts**: Stage temp table + MERGE for idempotency\n- **Historical reloads**: Direct COPY INTO with file pattern matching\n\n## Monitoring & Validation\n- Track file ingestion status via Snowflake metadata views\n- Monitor COPY performance and error rates\n- Validate row counts and data quality post-load\n- Set up alerts for failed loads or SLA breaches",
    "metadata": {
      "tags": [
        "data-eng",
        "snowflake",
        "spark-connector",
        "copy-into",
        "snowpipe",
        "performance"
      ],
      "persona": "de",
      "file_path": "data_eng/snowflake_load_patterns.md",
      "file_name": "snowflake_load_patterns.md"
    }
  },
  {
    "id": "azure_data_factory_0",
    "text": "# Azure Data Factory (ADF) Best Practices\n\n## Overview\nAzure Data Factory is a cloud-based data integration service that allows you to create, schedule, and manage data pipelines for moving and transforming data.\n\n## Core Concepts\n\n### 1. Data Factory Architecture\n- **Linked Services**: Connection strings to external data stores\n- **Datasets**: Structure of data within linked services\n- **Pipelines**: Logical grouping of activities that perform a task\n- **Activities**: Individual steps in a pipeline\n- **Triggers**: Schedule or event-based pipeline execution\n\n### 2. Data Movement Patterns\n- **Copy Activity**: Move data between data stores\n- **Data Flow**: Visual data transformation using Spark\n- **Custom Activities**: Execute custom code or scripts\n- **Stored Procedure Activity**: Execute SQL stored procedures\n\n## Pipeline Design Patterns\n\n### ETL vs ELT Architecture\n- **ETL (Extract-Transform-Load)**: Transform data before loading\n- **ELT (Extract-Load-Transform)**: Load raw data, then transform\n- **Hybrid Approach**: Use both patterns based on data characteristics\n\n### Incremental Data Processing\n```json\n{\n  \"source\": {\n    \"type\": \"SqlServerSource\",\n    \"sqlReaderQuery\": \"SELECT * FROM Orders WHERE ModifiedDate > '@{pipeline().parameters.WatermarkValue}'\"\n  },\n  \"sink\": {\n    \"type\": \"SqlServerSink\",\n    \"tableName\": \"Orders_Staging\"\n  }\n}\n```\n\n### Error Handling and Retry Logic\n- **Retry Policy**: Configure automatic retries for transient failures\n- **Error Output**: Route failed records to error tables\n- **Dead Letter Queue**: Handle permanently failed records\n- **Monitoring**: Set up alerts for pipeline failures\n\n## Data Flow Best Practices\n\n### Performance Optimization\n- **Partitioning**: Use appropriate partitioning strategies\n- **Caching**: Enable caching for frequently accessed data\n- **Data Skew**: Handle data skew in transformations\n- **Resource Allocation**: Right-size cluster resources\n\n### Transformation Patterns\n```sql\n-- Derived Column transformation\nCASE \n    WHEN [OrderDate] < DATEADD(day, -30, GETDATE()) THEN 'Historical'\n    WHEN [OrderDate] < DATEADD(day, -7, GETDATE()) THEN 'Recent'\n    ELSE 'Current'\nEND AS [OrderCategory]\n```\n\n### Data Quality Checks\n- **Null Value Handling**: Define strategies for missing data\n- **Data Type Validation**: Ensure data types match expectations\n- **Range Validation**: Check data falls within expected ranges\n- **Referential Integrity**: Validate foreign key relationships\n\n## Integration Patterns\n\n### Real-time Data Processing\n- **Event Grid**: Trigger pipelines on data changes\n- **Stream Analytics**: Process streaming data\n- **Change Data Capture**: Capture incremental changes\n- **IoT Hub Integration**: Process IoT device data\n\n### Batch Processing\n- **Schedule Triggers**: Regular batch processing schedules\n- **Tumbling Windows**: Fixed-size time windows\n- **Sliding Windows**: Overlapping time windows\n- **Session Windows**: Activity-based windows\n\n## Monitoring and Troubleshooting\n\n### Pipeline Monitoring\n- **Activity Runs**: Monitor individual activity execution\n- **Pipeline Runs**: Track overall pipeline performance\n- **Trigger Runs**: Monitor trigger-based executions\n- **Data Flow Runs**: Track data transformation performance\n\n### Performance Metrics\n- **Data Volume**: Track data processed per pipeline run\n- **Execution Time**: Monitor pipeline duration\n- **Success Rate**: Track pipeline success/failure rates\n- **Resource Utilization**: Monitor compute resource usage\n\n### Alerting and Notifications\n- **Email Alerts**: Notify on pipeline failures\n- **Azure Monitor**: Integrate with monitoring dashboards\n- **Custom Metrics**: Define business-specific KPIs\n- **Log Analytics**: Centralized logging and analysis\n\n## Security and Governance\n\n### Access Control\n- **Managed Identity**: Use Azure AD managed identities\n- **Role-Based Access**: Implement least privilege access\n- **Key Vault Integration**: Secure credential storage\n- **Private Endpoints**: Secure network connectivity\n\n### Data Lineage\n- **Data Catalog**: Track data lineage and dependencies\n- **Metadata Management**: Document data definitions\n- **Impact Analysis**: Understand downstream effects\n- **Compliance Tracking**: Meet regulatory requirements\n\n## Cost Optimization\n\n### Resource Management\n- **Auto-scaling**: Scale resources based on demand\n- **Reserved Capacity**: Use reserved instances for predictable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Resource Tagging**: Track costs by project or department\n\n### Pipeline Optimization\n- **Incremental Processing**: Process only changed data\n- **Parallel Execution**: Run independent activities in parallel\n- **Data Compression**: Compress data in transit and at rest\n- **Efficient Transformations**: Optimize transformation logic\n\n## Common Use Cases\n\n### Data",
    "metadata": {
      "tags": [
        "adf",
        "azure",
        "etl",
        "data-pipeline"
      ],
      "persona": "de",
      "file_path": "data_eng/azure_data_factory.md",
      "file_name": "azure_data_factory.md"
    }
  },
  {
    "id": "azure_data_factory_1",
    "text": "Azure AD managed identities\n- **Role-Based Access**: Implement least privilege access\n- **Key Vault Integration**: Secure credential storage\n- **Private Endpoints**: Secure network connectivity\n\n### Data Lineage\n- **Data Catalog**: Track data lineage and dependencies\n- **Metadata Management**: Document data definitions\n- **Impact Analysis**: Understand downstream effects\n- **Compliance Tracking**: Meet regulatory requirements\n\n## Cost Optimization\n\n### Resource Management\n- **Auto-scaling**: Scale resources based on demand\n- **Reserved Capacity**: Use reserved instances for predictable workloads\n- **Spot Instances**: Use spot instances for non-critical workloads\n- **Resource Tagging**: Track costs by project or department\n\n### Pipeline Optimization\n- **Incremental Processing**: Process only changed data\n- **Parallel Execution**: Run independent activities in parallel\n- **Data Compression**: Compress data in transit and at rest\n- **Efficient Transformations**: Optimize transformation logic\n\n## Common Use Cases\n\n### Data Lake Ingestion\n- **Multi-source Ingestion**: Ingest from various data sources\n- **Schema Evolution**: Handle changing data schemas\n- **Data Validation**: Ensure data quality during ingestion\n- **Metadata Extraction**: Capture and store metadata\n\n### Data Warehouse Loading\n- **Dimension Loading**: Load slowly changing dimensions\n- **Fact Table Loading**: Load fact tables with proper partitioning\n- **Aggregation Processing**: Pre-compute aggregations\n- **Data Mart Creation**: Create subject-specific data marts\n\n### Data Migration\n- **On-premises to Cloud**: Migrate from on-premises systems\n- **Cloud to Cloud**: Move between cloud providers\n- **Database Migration**: Migrate between database systems\n- **Application Migration**: Support application modernization\n\n## Troubleshooting Guide\n\n### Common Issues\n- **Connection Timeouts**: Check network connectivity and firewall rules\n- **Memory Issues**: Optimize data flow transformations\n- **Performance Degradation**: Review partitioning and caching strategies\n- **Data Quality Issues**: Implement data validation checks\n\n### Debugging Techniques\n- **Activity Run Details**: Review detailed execution logs\n- **Data Flow Debug**: Use debug mode for data flow development\n- **Sample Data**: Use sample data for testing\n- **Incremental Testing**: Test with small data subsets\n\n## Migration Strategies\n\n### Legacy System Integration\n- **API Integration**: Connect to legacy APIs\n- **File-based Integration**: Process legacy file formats\n- **Database Integration**: Connect to legacy databases\n- **Message Queue Integration**: Process legacy message queues\n\n### Cloud Migration\n- **Lift and Shift**: Move existing pipelines to cloud\n- **Cloud-native Redesign**: Redesign for cloud-native patterns\n- **Hybrid Approach**: Maintain some on-premises components\n- **Gradual Migration**: Migrate incrementally over time",
    "metadata": {
      "tags": [
        "adf",
        "azure",
        "etl",
        "data-pipeline"
      ],
      "persona": "de",
      "file_path": "data_eng/azure_data_factory.md",
      "file_name": "azure_data_factory.md"
    }
  },
  {
    "id": "interview_voice_0",
    "text": "# Interview Voice & Communication Style\n\n## Interview Cadence\nStart with context (one line), state approach (one line), give 2–3 concrete steps, note a trade-off, close with how to validate/monitor. Avoid jargon dumps. If pressed, go one level deeper (config, code primitive).\n\n## Response Structure\n1. **Context**: Brief understanding of the problem\n2. **Approach**: High-level strategy in one sentence\n3. **Steps**: 2-3 concrete implementation steps\n4. **Trade-offs**: Mention one key consideration or alternative\n5. **Validation**: How to verify or monitor the solution\n\n## Tone Guidelines\n- **Confident**: Direct, decisive language with clear recommendations\n- **Collaborative**: \"We can...\" language, asking for input\n- **Cautious**: \"I'd typically...\" with hedging and validation steps\n\n## Depth Levels\n- **Short**: 2-3 sentences, core approach only\n- **Mid**: 4-6 sentences with key steps and one trade-off\n- **Deep**: Detailed implementation with multiple options and validation\n\n## Common Patterns\n- Lead with the business impact or technical constraint\n- Use specific tools/technologies (PySpark, ADF, Delta Lake)\n- Mention concrete metrics (runtime, cost, SLA)\n- Reference Spark UI, lineage, or monitoring for validation\n- Acknowledge when multiple approaches exist and pick one with reasoning",
    "metadata": {
      "tags": [
        "style",
        "interview",
        "cadence",
        "communication"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_voice.md",
      "file_name": "interview_voice.md"
    }
  },
  {
    "id": "data enginner_0",
    "text": "# 📄 OG Resume – Krishna Sathvik (v2)\n\n---\n\n## Summary\n\nData Engineer with 6+ years’ experience evolving from backend development into advanced data engineering. Skilled in Databricks, PySpark, SQL, and Azure, with proven success delivering 10TB+ scale pipelines, medallion architectures, and governance frameworks. Recognized for performance tuning, cost optimization, and enabling analytics that improve decision-making across healthcare, retail, and finance domains.\n\n---\n\n## Experience\n\n**Data Engineer | TCS (Walgreens, USA) | Feb 2022 – Present**\n\nEngineered Delta Lakehouse pipelines in Databricks + PySpark, processing 10TB+ monthly data across sales, pharmacy, and supply chain, enabling real-time analytics and reducing ad-hoc reporting by 32%\n\nDesigned medallion architecture flows (Bronze, Silver, Gold) with schema enforcement, deduplication, and fact/dimension modeling, improving KPI reporting accuracy 30%\n\nAutomated orchestration with ADF triggering Databricks notebooks, implementing dynamic parameterization and CI/CD via Azure DevOps to achieve zero-downtime deployments\n\nImplemented Unity Catalog for role-based access, PII masking, and lineage tracking, ensuring compliance and secure access for regulated data and anonymized reporting\n\nOptimized partitioning strategies, resolved skew with salting and broadcast joins, and compacted small Delta files—reducing critical job runtime from 2+ hours to under 40 minutes\n\n**Analytics Engineer | CVS Health (USA) | Jan 2021 – Jan 2022**\n\nBuilt ingestion pipelines in ADF + Databricks integrating supply chain and sales data, producing audit-ready datasets powering KPI dashboards across 200+ retail sites\n\nModeled fact/dimension schemas in SQL + dbt, improving reconciliation agility and accuracy 20% while supporting finance and operations decision-making\n\nCollaborated with stakeholders to define business rules for replenishment and billing, surfacing 20+ KPIs leveraged by leadership for operational strategy\n\nOptimized Databricks jobs with incremental models and clustering, cutting compute costs 18% and improving pipeline performance 25%\n\n**Data Science Intern | McKesson (USA) | May 2020 – Dec 2020**\n\nAutomated ETL scripts in Python + SQL reducing ingestion latency 50%, accelerating delivery of compliance dashboards for executive monitoring\n\nBuilt forecasting models aligning patient demand with supply capacity, preventing mismatches and driving \\$20M+ savings in procurement\n\nProduced insights from claims and sales data supporting compliance reviews and informing leadership’s cost-recovery initiatives\n\n**Software Developer | Inditek Pioneer Solutions (India) | 2017 – 2019**\n\nDeveloped backend APIs and optimized SQL queries for ERP modules, improving system response 35% and strengthening transactional accuracy in client billing\n\nDesigned reporting modules surfacing missed payments and contract discrepancies, cutting manual reconciliation and improving transparency in logistics workflows\n\n---\n\n## Skills\n\n- **Data Engineering & ELT:** Databricks, PySpark, dbt, Airflow, ADF\n- **Cloud Platforms:** Azure (Synapse, Data Factory, DevOps), Snowflake, AWS (Glue, Lambda)\n- **Databases & Storage:** SQL Server, PostgreSQL, Delta Lake, Oracle\n- **Analytics & BI:** Power BI, Tableau, KPI Dashboards\n- **DevOps & Orchestration:** GitHub Actions, Azure DevOps, Docker, Kubernetes\n- **Governance & Security:** Data Quality Validation, PII Masking, Unity Catalog\n\n---\n\n# 📑 Resume Optimization Framework – v2\n\n## 1. Structure & Bullet Rules\n\n**5-4-3-2 Rule**\n\n- Walgreens → 5 bullets | Present tense | Outcome + metrics | Core tech (Databricks, PySpark, dbt, Azure)\n- CVS → 4 bullets | Past tense | Analytics enablement focus + optimization | ADF + dbt + SQL\n- McKesson → 3 bullets | Past tense | Forecasting + audit outcomes | Python + SQL + models\n- Inditek → 2 bullets | Past tense | ERP/API outcomes | SQL + APIs | Efficiency gains\n\n**Character Constraint Rule**\n\n- Each bullet = **220–240 characters**\n- Never <215, never >240\n\n---\n\n## 2. Domain Adaptation Layer\n\nAdapt **vocabulary + emphasis** per JD:\n\n- Healthcare/Compliance → audit-ready datasets, regulatory workflows, compliance monitoring\n- Retail/Audit → financial integrity, recovery audits, supplier agreements, reconciliation\n- Tech/Data Platforms → streaming pipelines, real-time analytics, Kafka/Event Hubs, ML integration\n- Finance/AI/ML → PySpark models, AWS Glue, risk compliance, forecasting, ML deployment\n\n---\n\n## 3. Skills Optimization Layer\n\n**Always keep 6 OG categories:**\n\n1. Data Engineering",
    "metadata": {
      "persona": "de",
      "file_path": "data_eng/data enginner.md",
      "file_name": "data enginner.md"
    }
  },
  {
    "id": "data enginner_1",
    "text": "+ optimization | ADF + dbt + SQL\n- McKesson → 3 bullets | Past tense | Forecasting + audit outcomes | Python + SQL + models\n- Inditek → 2 bullets | Past tense | ERP/API outcomes | SQL + APIs | Efficiency gains\n\n**Character Constraint Rule**\n\n- Each bullet = **220–240 characters**\n- Never <215, never >240\n\n---\n\n## 2. Domain Adaptation Layer\n\nAdapt **vocabulary + emphasis** per JD:\n\n- Healthcare/Compliance → audit-ready datasets, regulatory workflows, compliance monitoring\n- Retail/Audit → financial integrity, recovery audits, supplier agreements, reconciliation\n- Tech/Data Platforms → streaming pipelines, real-time analytics, Kafka/Event Hubs, ML integration\n- Finance/AI/ML → PySpark models, AWS Glue, risk compliance, forecasting, ML deployment\n\n---\n\n## 3. Skills Optimization Layer\n\n**Always keep 6 OG categories:**\n\n1. Data Engineering & ELT\n2. Cloud Platforms\n3. Databases & Storage\n4. Analytics & BI\n5. DevOps & Orchestration\n6. Governance & Security\n\n**JD Alignment Examples:**\n\n- **Azure-Heavy JD**\n\n  - Data Engineering & ELT: Databricks, PySpark, ADF, dbt, Airflow\n  - Cloud Platforms: Azure (Synapse, Data Factory, DevOps), Snowflake\n  - Databases & Storage: SQL Server, Delta Lake, PostgreSQL\n  - Analytics & BI: Power BI, Tableau\n  - DevOps & Orchestration: Azure DevOps, GitHub Actions, Docker\n  - Governance & Security: Unity Catalog, PII Masking, Data Quality Validation\n\n- **AWS-Heavy JD**\n\n  - Data Engineering & ELT: PySpark, dbt, Airflow, AWS Glue\n  - Cloud Platforms: AWS (Glue, Lambda, MSK/Kafka, S3), Snowflake\n  - Databases & Storage: PostgreSQL, SQL Server, Redshift\n  - Analytics & BI: Tableau, Power BI\n  - DevOps & Orchestration: GitHub Actions, Docker, Kubernetes\n  - Governance & Security: Data Lineage, PII Masking, Compliance Frameworks\n\n- **BI/Analytics-Heavy JD**\n\n  - Data Engineering & ELT: SQL, dbt, ADF, Databricks (light emphasis)\n  - Cloud Platforms: Azure (Synapse, Data Factory), AWS (basic S3/Glue)\n  - Databases & Storage: SQL Server, Delta Lake\n  - Analytics & BI: Power BI, Tableau, KPI Dashboards\n  - DevOps & Orchestration: Azure DevOps, GitHub Actions\n  - Governance & Security: Data Quality Validation, Anonymization, Lineage Tracking\n\n---\n\n## 4. Verb Rotation Rule\n\nRotate verbs to avoid repetition: **Engineer, Optimize, Automate, Deploy, Collaborate, Design, Implement, Develop, Create, Scale, Orchestrate, Modernize**\n\n---\n\n## 5. Metrics Bank Rule\n\nEvery bullet ties to outcome + metric:\n\n- % improvements (22% cost reduction, 30% reliability boost)\n- Scale (10TB+, 1.2B rows)\n- Time savings (30+ hours weekly)\n- Financial impact (\\$20M+ savings)\n- Adoption (200+ sites, 50+ pipelines)\n\n---\n\n# 📋 JD → Resume Adaptation Checklist\n\n1. **Read JD closely** → Highlight cloud platform (AWS vs Azure), pipeline type (batch vs streaming), domain (healthcare, retail, finance).\n2. **Apply Domain Layer** → Swap vocabulary to mirror JD (audit, compliance, ML, streaming, etc).\n3. **Reorder Skills** → Keep 6 categories but move JD-priority tools to the front.\n4. **Rewrite Bullets** → Use 5-4-3-2 rule, adjust tech names, keep impact + metrics fixed.\n5. **Rotate Verbs** → Swap starting verbs per bullet to keep flow strong.\n6. **Check Character Count** → Ensure 220–240 characters per bullet.\n7. **Final Scan** → ATS keywords match JD, measurable outcomes present, tense alignment correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**Azure-Heavy JD (Walgreens, CVS, Microsoft ecosystem)**\\\nData Engineer with 6+ years’ experience designing and scaling pipelines on Azure and Databricks. Skilled in PySpark, SQL, ADF, and medallion architecture with proven success optimizing costs, enforcing governance, and enabling real-time analytics at enterprise scale.\n\n**AWS-Heavy JD (Moody’s, Possible Finance, startups)**\\\nData Engineer with 6+ years’ experience building ELT pipelines on AWS using Glue,",
    "metadata": {
      "persona": "de",
      "file_path": "data_eng/data enginner.md",
      "file_name": "data enginner.md"
    }
  },
  {
    "id": "data enginner_2",
    "text": "4. **Rewrite Bullets** → Use 5-4-3-2 rule, adjust tech names, keep impact + metrics fixed.\n5. **Rotate Verbs** → Swap starting verbs per bullet to keep flow strong.\n6. **Check Character Count** → Ensure 220–240 characters per bullet.\n7. **Final Scan** → ATS keywords match JD, measurable outcomes present, tense alignment correct.\n\n---\n\n# 🎯 JD-Specific Summary Templates\n\n**Azure-Heavy JD (Walgreens, CVS, Microsoft ecosystem)**\\\nData Engineer with 6+ years’ experience designing and scaling pipelines on Azure and Databricks. Skilled in PySpark, SQL, ADF, and medallion architecture with proven success optimizing costs, enforcing governance, and enabling real-time analytics at enterprise scale.\n\n**AWS-Heavy JD (Moody’s, Possible Finance, startups)**\\\nData Engineer with 6+ years’ experience building ELT pipelines on AWS using Glue, Lambda, MSK/Kafka, and S3. Expertise in PySpark, dbt, and SQL with a strong record of optimizing performance, reducing costs, and delivering audit-ready data platforms across finance and retail.\n\n**BI/Analytics-Heavy JD (Analytics Engineer / BI Dev focus)**\\\nAnalytics Engineer with 6+ years’ experience bridging data engineering and BI. Designed SQL + dbt models, KPI frameworks, and automated ETL pipelines powering Power BI/Tableau dashboards. Skilled in Databricks, Python, and governance frameworks to deliver accurate, business-ready insights.\n\n---",
    "metadata": {
      "persona": "de",
      "file_path": "data_eng/data enginner.md",
      "file_name": "data enginner.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_0",
    "text": "# Comprehensive Data Engineering Q&A - 120 Questions\n\n## Section A: Core PySpark + Databricks (20 Questions)\n\n### Q1. What is your experience with PySpark in your current project?\n**A:** In my Walgreens project, I built end-to-end PySpark pipelines in Databricks that processed over 10TB monthly. I handled ingestion of raw data into Bronze, applied transformations and schema validation in Silver, and modeled fact/dimension tables in Gold for reporting. I frequently used PySpark DataFrame APIs, window functions, UDFs, and joins. For example, I implemented deduplication logic using `dropDuplicates()` and window functions, ensuring the most recent transaction record is retained.\n\n### Q2. How do you handle large datasets in PySpark efficiently?\n**A:** First, I design partitioning strategies at ingestion (date-based partitions). Second, I tune Spark configurations like `spark.sql.shuffle.partitions` and executor memory. Third, I use optimizations such as broadcast joins for small lookup tables and salting for skewed keys. For large jobs, I enable Adaptive Query Execution. In one case, these changes reduced a pipeline runtime from 2+ hours to 40 minutes.\n\n### Q3. How do you debug PySpark jobs when they fail?\n**A:** I start with Spark UI to analyze DAGs, stages, and tasks. I check for skew (few tasks taking much longer) or memory errors. Then I check logs in Databricks for stack traces. I usually replay the job with a smaller dataset to isolate the failing transformation. For example, when a job failed due to a corrupt record, I added `_corrupt_record` handling and moved bad rows into a quarantine table.\n\n### Q4. How do you handle nested JSON in PySpark?\n**A:** I define a StructType schema, use `from_json` to parse the string column, and `explode` for arrays. Then I flatten with `withColumn` to extract nested attributes. For deeply nested JSON, I use recursive functions to normalize. Finally, I store it as a clean Delta table in Silver for downstream consumption.\n\n### Q5. How do you handle duplicates in PySpark?\n**A:** For simple duplicates, I use `dropDuplicates()`. For business-specific deduplication, I use window functions with `row_number()` ordered by timestamp, keeping only the latest row. In Walgreens, this was used for handling multiple prescription updates from pharmacy systems.\n\n### Q6. How do you handle schema drift in PySpark pipelines?\n**A:** In Bronze, I ingest with permissive mode to avoid job failure. In Silver, I enforce strict schema. If new columns appear, I add them with defaults in Silver after business validation. For Delta tables, I use `mergeSchema` in controlled deployments, never blindly. This allows flexibility but avoids breaking downstream queries.\n\n### Q7. How do you design incremental loads in PySpark?\n**A:** I use watermarking (modified_date column) or surrogate keys. ADF passes parameters to notebooks for last processed date. In PySpark, I filter only incremental rows and apply Delta `merge` to update/inserts. This reduced daily runs from processing 10TB full to ~300GB delta, saving cost and runtime.\n\n### Q8. Can you explain time travel in Delta Lake? How have you used it?\n**A:** Time travel lets me query data at a specific version or timestamp. At Walgreens, one job overwrote 2 days of data. Using `VERSION AS OF` in Delta, I restored the table to its previous state in minutes without reloading raw files.\n\n### Q9. How do you handle slowly changing dimensions (SCD) in Databricks?\n**A:** I used Delta `merge` for Type 2. Old record is closed with an end_date, new record inserted with active_flag = 1. This keeps historical changes. For example, when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use `PERMISSIVE` mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_1",
    "text": ", when product pricing changed, our dimension table kept both old and new versions for accurate reporting.\n\n### Q10. How do you monitor PySpark jobs?\n**A:** I log metadata like job ID, start/end time, row counts, and error counts into monitoring tables. ADF sends failure alerts. Additionally, I surface monitoring dashboards in Power BI so IT and business both see pipeline health.\n\n### Q11. How do you implement joins in PySpark for performance?\n**A:** For large-large joins, I repartition on join keys to avoid skew. For small-large joins, I use `broadcast()`. For very skewed joins, I salt keys. I always monitor shuffle size in Spark UI.\n\n### Q12. How do you handle corrupt records in ingestion?\n**A:** I use `PERMISSIVE` mode in PySpark read, which places bad records in `_corrupt_record`. I redirect them into a quarantine Delta table for manual review, while valid data continues processing.\n\n### Q13. How do you test PySpark pipelines?\n**A:** Unit tests validate transformations with small sample data. Row count reconciliation checks ingestion completeness. Schema validation checks enforce consistency. We automated these checks in CI/CD pipelines.\n\n### Q14. How do you manage dependencies across notebooks in Databricks?\n**A:** I modularize common logic (like validations, schema enforcement) in utility notebooks or .py files stored in repos. Then I import them into main notebooks. This avoids code duplication and keeps pipelines maintainable.\n\n### Q15. How do you handle late arriving data?\n**A:** I use watermarking in Delta tables, so late data is still merged if within X days. If outside retention, we load them manually after business approval.\n\n### Q16. How do you handle large joins across multiple datasets?\n**A:** First, partition both datasets on the join key. If one is small, broadcast it. If skew occurs, apply salting. If still heavy, break into smaller joins and cache intermediate results.\n\n### Q17. How do you manage PySpark code for reusability?\n**A:** I follow modular design: separate ingestion, transformation, validation, and load functions. I store configs in parameter files, not hardcoded. Reusable frameworks allowed offshore to easily plug in new sources with minimal code.\n\n### Q18. How do you optimize PySpark DataFrame transformations?\n**A:** Avoid wide transformations until necessary, use `select` instead of `*`, cache intermediate results when reused, and avoid UDFs unless unavoidable. Vectorized operations (pandas UDFs) are faster than row-wise ones.\n\n### Q19. How do you manage error handling in PySpark?\n**A:** I wrap critical transformations with try/except. Failures are logged into error tables. In ADF, we configure retries and failure alerts. This ensures job doesn't fail silently.\n\n### Q20. What's the biggest challenge you solved with PySpark at Walgreens?\n**A:** Optimizing a 10TB sales fact pipeline that originally took 2+ hours. By tuning partitioning, salting skewed joins, and compacting files, I reduced runtime to 40 minutes. This improved SLA compliance and cut costs by 30%.\n\n## Section B: Azure Data Factory (ADF) & Orchestration (20 Questions)\n\n### Q21. How did you use ADF in your Walgreens project?\n**A:** I used ADF mainly for orchestration and scheduling of Databricks notebooks. ADF pipelines triggered ingestion of raw files into ADLS, parameterized notebook runs with date filters, and coordinated dependencies across multiple jobs. For example, when pharmacy and sales data needed to be processed together, ADF ensured ingestion → transformation → validation were executed in sequence with retries on failure.\n\n### Q22. How do you parameterize ADF pipelines?\n**A:** I use ADF global parameters and dataset parameters to make pipelines dynamic. For example, the source file path is parameterized with date and source name, so the same pipeline ingests multiple sources. Databricks notebook activities accept parameters for start_date and end_date, allowing incremental processing without hardcoding.\n\n### Q23. How do you handle scheduling in ADF?\n**A:** I use triggers — primarily tumbling window triggers for periodic jobs like daily loads, and event triggers for file arrival. For critical pipelines, we used tumbling window with retry policies. Business-critical reports were scheduled nightly at 2am, ensuring that Silver/Gold tables were refreshed before business started.\n\n### Q24. How do you handle failure in ADF pipelines?\n**A:** I configure retry policies on activities and failure paths to send alerts. Failed activities log errors in monitoring tables. For example, if a Databricks notebook failed due to schema mismatch, ADF retried once, and on repeated failure, triggered a Logic App alert to teams.\n\n### Q25. How do you design",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_2",
    "text": ", the source file path is parameterized with date and source name, so the same pipeline ingests multiple sources. Databricks notebook activities accept parameters for start_date and end_date, allowing incremental processing without hardcoding.\n\n### Q23. How do you handle scheduling in ADF?\n**A:** I use triggers — primarily tumbling window triggers for periodic jobs like daily loads, and event triggers for file arrival. For critical pipelines, we used tumbling window with retry policies. Business-critical reports were scheduled nightly at 2am, ensuring that Silver/Gold tables were refreshed before business started.\n\n### Q24. How do you handle failure in ADF pipelines?\n**A:** I configure retry policies on activities and failure paths to send alerts. Failed activities log errors in monitoring tables. For example, if a Databricks notebook failed due to schema mismatch, ADF retried once, and on repeated failure, triggered a Logic App alert to teams.\n\n### Q25. How do you design ADF pipelines for scalability?\n**A:** I avoid building one massive pipeline. Instead, I create modular pipelines — ingestion, transformations, validations — and link them with pipeline chaining. I also parameterize everything, so the same pipeline works across multiple sources. This reduced maintenance and improved reusability for offshore team.\n\n### Q26. How do you handle dependencies between ADF pipelines?\n**A:** I use dependency triggers and pipeline chaining. For example, Silver pipeline starts only after Bronze ingestion completes successfully. If multiple datasets must be ready before Gold processing, I use \"Wait on Activity\" or success/failure dependencies.\n\n### Q27. How do you integrate ADF with Databricks?\n**A:** ADF has a \"Databricks Notebook\" activity. I configured linked services with Key Vault for secrets, so credentials aren't hardcoded. Each notebook call passes parameters like file_date, source system, or load_type. This ensured pipelines remained dynamic and secure.\n\n### Q28. How do you handle incremental data loads with ADF?\n**A:** I store last run watermark in a metadata table. ADF fetches this value, passes it to Databricks as parameter. Databricks then queries only new/updated data and merges it into Delta. After successful load, ADF updates the watermark. This ensured daily loads were efficient.\n\n### Q29. How do you monitor ADF pipelines?\n**A:** I use ADF monitoring dashboard for real-time status. In addition, I log metadata like run_id, row counts, and errors into custom Delta tables. Failures are reported via email alerts. Power BI dashboards show historical success/failure trends, which helped management track SLA adherence.\n\n### Q30. How do you secure ADF pipelines?\n**A:** Secrets like database passwords and storage keys are stored in Azure Key Vault. ADF uses managed identities to connect to ADLS and Databricks. This eliminated hardcoding sensitive info.\n\n### Q31. How do you design ADF for reusability?\n**A:** I use parameterized datasets and pipeline templates. For example, one generic ingestion pipeline handled all CSV sources by passing schema, file_path, and delimiter as parameters. Offshore team just configured metadata, no code changes.\n\n### Q32. How do you handle event-driven ingestion in ADF?\n**A:** For real-time scenarios, I set up Event Grid triggers so ADF pipelines started as soon as new files landed in ADLS. This reduced latency from hours to minutes for near-real-time data availability.\n\n### Q33. How do you integrate ADF with CI/CD?\n**A:** ADF JSON definition files are stored in Git. Azure DevOps pipelines deploy these JSONs across dev, test, and prod environments. Parameters like connection strings are environment-specific and replaced during deployment using ARM templates.\n\n### Q34. How do you deal with long-running pipelines?\n**A:** For long pipelines, I break them into smaller pipelines with checkpoints. This ensures partial success is saved and we don't restart everything on failure. For example, ingestion pipeline completed successfully even if transformation pipeline failed, allowing us to restart only transformations.\n\n### Q35. How do you manage data validation with ADF?\n**A:** After ingestion, I run validation notebooks triggered by ADF. These check row counts, null ratios, and duplicates. ADF logs validation status into Delta tables. Alerts are raised if thresholds are violated.\n\n### Q36. How do you manage metadata in ADF pipelines?\n**A:** I store pipeline configs (file paths, schema, business rules) in metadata tables. ADF reads metadata at runtime and applies ingestion accordingly. This approach made pipelines completely dynamic — new sources onboarded without code changes.\n\n### Q37. How do you optimize ADF performance?\n**A:** I configure parallel copy in copy activities, use staging in ADLS/Blob, and partition large files. For example, one large file was split into",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_3",
    "text": "with checkpoints. This ensures partial success is saved and we don't restart everything on failure. For example, ingestion pipeline completed successfully even if transformation pipeline failed, allowing us to restart only transformations.\n\n### Q35. How do you manage data validation with ADF?\n**A:** After ingestion, I run validation notebooks triggered by ADF. These check row counts, null ratios, and duplicates. ADF logs validation status into Delta tables. Alerts are raised if thresholds are violated.\n\n### Q36. How do you manage metadata in ADF pipelines?\n**A:** I store pipeline configs (file paths, schema, business rules) in metadata tables. ADF reads metadata at runtime and applies ingestion accordingly. This approach made pipelines completely dynamic — new sources onboarded without code changes.\n\n### Q37. How do you optimize ADF performance?\n**A:** I configure parallel copy in copy activities, use staging in ADLS/Blob, and partition large files. For example, one large file was split into multiple blocks and ingested in parallel, reducing ingestion time from 40 minutes to under 10.\n\n### Q38. How do you orchestrate multiple technologies with ADF?\n**A:** ADF orchestrated ADLS, Databricks, Synapse, and Snowflake in my project. For example, ingestion from APIs landed in ADLS, processed in Databricks, exported to Snowflake, and then visualized in Power BI. ADF coordinated the entire workflow end-to-end.\n\n### Q39. How do you handle SLA in ADF pipelines?\n**A:** I tracked expected runtime and row counts. If pipelines exceeded thresholds, alerts triggered. For pharmacy data, SLA was 6am reporting availability — ADF was scheduled at 2am with monitoring, so if job failed, support team had time to re-run before SLA breach.\n\n### Q40. What challenges did you face with ADF and how did you solve them?\n**A:** One challenge was schema drift causing failures in ingestion. I solved it by using schema drift-tolerant ingestion at Bronze and enforcing strict schema at Silver. Another was long ingestion times, solved with parallel copy and partitioning. I also improved security by integrating Key Vault and managed identities, removing all hardcoded secrets.\n\n## Section C: Delta Lake, Schema, Data Modeling (20 Questions)\n\n### Q41. How did you design the medallion architecture in Walgreens?\n**A:** I followed the medallion architecture — Bronze, Silver, and Gold layers. Bronze captured raw ingested data exactly as it arrived from sources, tolerant to schema drift. Silver applied schema enforcement, deduplication, and standardization, making the data clean and query-ready. Gold was modeled into fact and dimension tables optimized for reporting. This layered approach made pipelines robust, reusable, and easy for business teams to consume.\n\n### Q42. How do you enforce schema in Delta Lake?\n**A:** I applied explicit schemas during ingestion, not relying on inference in production. For schema evolution, I used `mergeSchema` in controlled updates. For example, when a new column was introduced in sales data, I validated it in dev, updated downstream logic, and then enabled schema merge. This prevented silent failures.\n\n### Q43. How do you handle schema drift?\n**A:** Bronze is flexible — it accepts extra columns and quarantines bad rows. Silver enforces schema strictly. Any new column is validated in dev first. If valid, I add it with defaults and update documentation. This two-layer enforcement avoids unexpected breakages.\n\n### Q44. How do you design fact and dimension tables in Gold?\n**A:** I model fact tables to capture transactional data like sales, and dimension tables to capture master data like product, store, and customer. Facts include foreign keys to dimensions. I also denormalize selectively for performance. KPIs like revenue per store were modeled in Gold for Power BI dashboards.\n\n### Q45. How do you design SCD (Slowly Changing Dimensions)?\n**A:** I implemented Type 2 SCD with Delta merge. When a dimension attribute changes, I close the old record with an end_date and insert a new record with active_flag = 1. This preserved historical accuracy. Example: product price changes required Type 2 so reports showed past sales at old prices.\n\n### Q46. How do you manage historical versions of data?\n**A:** Delta Lake's time travel feature allowed querying older versions of tables. For example, when an accidental overwrite occurred, I restored previous version using `VERSION AS OF`. This was faster than reprocessing from raw files.\n\n### Q47. How do you deal with null values in schema enforcement?\n**A:** I use PySpark `fillna` or business rules to assign defaults. In Silver, nulls in critical columns are flagged in validation tables. Business-approved defaults like \"Unknown\" for missing store_id ensure pipelines don't break.\n\n### Q48",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_4",
    "text": "How do you design SCD (Slowly Changing Dimensions)?\n**A:** I implemented Type 2 SCD with Delta merge. When a dimension attribute changes, I close the old record with an end_date and insert a new record with active_flag = 1. This preserved historical accuracy. Example: product price changes required Type 2 so reports showed past sales at old prices.\n\n### Q46. How do you manage historical versions of data?\n**A:** Delta Lake's time travel feature allowed querying older versions of tables. For example, when an accidental overwrite occurred, I restored previous version using `VERSION AS OF`. This was faster than reprocessing from raw files.\n\n### Q47. How do you deal with null values in schema enforcement?\n**A:** I use PySpark `fillna` or business rules to assign defaults. In Silver, nulls in critical columns are flagged in validation tables. Business-approved defaults like \"Unknown\" for missing store_id ensure pipelines don't break.\n\n### Q48. How do you validate transformations across layers?\n**A:** I reconcile row counts and key distributions from Bronze → Silver → Gold. I log validation results into Delta monitoring tables. For example, I checked that deduplication didn't drop more records than expected.\n\n### Q49. How do you design partitioning in Delta tables?\n**A:** I usually partition large datasets by date, as most queries are time-based. For multi-dimensional queries, I use ZORDER indexing. Example: sales fact was partitioned by transaction_date, with ZORDER on store_id and product_id for efficient pruning.\n\n### Q50. How do you manage small file problems in Delta?\n**A:** I use `OPTIMIZE` in Databricks to compact small Parquet files into larger ones. I also control batch size during ingestion in ADF to avoid excessive tiny files. For Gold, I scheduled weekly compaction jobs to keep query performance high.\n\n### Q51. How do you design data models for reporting?\n**A:** I followed Kimball principles: fact tables for metrics, dimension tables for descriptive attributes, and star schema design. I ensured measures like sales_amount were additive and dimensions like date, product, and store enabled slice-and-dice in Power BI.\n\n### Q52. How do you manage data lineage?\n**A:** Unity Catalog and Purview tracked lineage across layers. For example, lineage view showed pharmacy source files → Silver clean table → Gold fact → Power BI dashboard. This transparency helped in audits and troubleshooting.\n\n### Q53. How do you handle late-arriving facts in modeling?\n**A:** I used Delta merge to insert or update late-arriving facts. If fact arrived after dimension change, I ensured it still mapped correctly using surrogate keys. For example, late prescription transactions still mapped to the correct product dimension.\n\n### Q54. How do you manage surrogate keys in dimensions?\n**A:** I generated surrogate keys in Silver using hash functions on natural keys. These surrogate keys became dimension table primary keys. Fact tables stored foreign keys referencing them. This approach ensured consistency even if natural keys changed.\n\n### Q55. How do you validate fact/dimension consistency?\n**A:** I ran referential integrity checks — ensuring every fact foreign key matched a valid dimension key. Invalid records were flagged in a quarantine table for review.\n\n### Q56. How do you manage incremental loads into Gold models?\n**A:** I used Delta `merge` for upserts. Facts were loaded incrementally based on modified_date. Dimensions were updated using SCD logic. This kept models fresh without reprocessing full history.\n\n### Q57. How do you optimize Gold layer models for BI?\n**A:** I pre-aggregated summary tables for common KPIs, reduced joins by denormalizing small dimensions, and compacted files. This ensured Power BI dashboards loaded in seconds instead of minutes.\n\n### Q58. How do you handle multi-source integration in modeling?\n**A:** I standardized schemas across sources in Silver. Then I conformed them into unified dimensions. Example: pharmacy and sales sources had different store codes. I standardized codes and built a single store dimension in Gold.\n\n### Q59. How do you document data models?\n**A:** I maintained data dictionaries in Confluence, showing column definitions, lineage, and business rules. Unity Catalog also stored schema metadata. This documentation helped both developers and business users.\n\n### Q60. What challenges did you face in data modeling and how did you solve them?\n**A:** One challenge was aligning different source systems with inconsistent schemas. I solved it by applying conformance rules in Silver and building standardized dimensions. Another challenge was query slowness in Power BI; I solved it by pre-aggregating summary tables and optimizing partitioning.\n\n## Section D: Optimization, Performance & Troubleshooting (20 Questions)\n\n### Q61. How did you optimize Spark jobs for large data volumes?\n**A:** First, I ensured partitioning",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_5",
    "text": "schemas across sources in Silver. Then I conformed them into unified dimensions. Example: pharmacy and sales sources had different store codes. I standardized codes and built a single store dimension in Gold.\n\n### Q59. How do you document data models?\n**A:** I maintained data dictionaries in Confluence, showing column definitions, lineage, and business rules. Unity Catalog also stored schema metadata. This documentation helped both developers and business users.\n\n### Q60. What challenges did you face in data modeling and how did you solve them?\n**A:** One challenge was aligning different source systems with inconsistent schemas. I solved it by applying conformance rules in Silver and building standardized dimensions. Another challenge was query slowness in Power BI; I solved it by pre-aggregating summary tables and optimizing partitioning.\n\n## Section D: Optimization, Performance & Troubleshooting (20 Questions)\n\n### Q61. How did you optimize Spark jobs for large data volumes?\n**A:** First, I ensured partitioning strategy matched query patterns, usually date-based. Then, I tuned shuffle partitions (`spark.sql.shuffle.partitions`) based on cluster size, avoiding both too few (skew) and too many (overhead). For joins, I used broadcast for small tables, salting for skew, and AQE (Adaptive Query Execution) to rebalance tasks. I also compacted small files using `OPTIMIZE`. One sales pipeline dropped from 2+ hours to 40 mins with these steps.\n\n### Q62. How do you identify bottlenecks in a Spark job?\n**A:** I use Spark UI to analyze DAG stages, tasks, and shuffle read/write sizes. If some tasks run much longer, it usually signals skew. If GC overhead is high, executors need memory tuning. If there's high shuffle volume, I check if joins/aggregations are causing unnecessary repartitions.\n\n### Q63. How do you resolve skew in Spark joins?\n**A:** If skew is caused by a few heavy keys, I apply salting — appending a random suffix to distribute skewed keys across partitions. For small lookup joins, I use `broadcast()`. AQE also automatically splits skewed partitions at runtime in Databricks.\n\n### Q64. How do you optimize Delta Lake performance?\n**A:** I regularly run `OPTIMIZE` with ZORDER on frequently filtered columns. I compact small files into larger Parquet files, improving metadata handling. I also vacuum old versions to reduce storage overhead. For query pruning, I carefully partition on high-cardinality columns like date, not on low-cardinality ones.\n\n### Q65. How do you handle small file problems?\n**A:** I control ingestion batch size in ADF to avoid generating thousands of tiny files. After ingestion, I use `OPTIMIZE` in Delta to compact them. For streaming, I use auto-compaction. This reduces metadata load and speeds up queries.\n\n### Q66. How do you handle long-running jobs?\n**A:** First, I profile the job in Spark UI to identify slow stages. Then, I repartition or broadcast joins as needed. If the job processes full data daily, I redesign it to incremental load. For one job that ran for 5+ hours, converting to incremental reduced it to under 1 hour.\n\n### Q67. How do you tune Spark cluster configurations?\n**A:** I tune driver/executor memory based on data size. I increase executor cores for parallelism but avoid too many to prevent GC pressure. I use autoscaling for heavy workloads, but for cost control, I size clusters to match partitioning. I also enable cache for reused datasets.\n\n### Q68. How do you optimize joins in PySpark?\n**A:** I decide based on size: Small table + large table → broadcast join; Large tables with skew → repartition + salting; Balanced large tables → hash partition on join keys. I always avoid Cartesian joins and prune unnecessary columns before joining.\n\n### Q69. How do you handle out-of-memory issues in Spark jobs?\n**A:** I check if wide transformations (e.g., groupBy) are blowing up. Then, I increase executor memory or repartition data to spread load. I persist intermediate results on disk instead of memory if needed. In Walgreens, this fixed a memory issue with 1B+ row aggregation.\n\n### Q70. How do you optimize aggregations in PySpark?\n**A:** I partition data on aggregation keys, cache intermediate results if reused, and pre-filter unnecessary rows early. For distinct counts, I used approx algorithms like HyperLogLog when exact wasn't needed, saving resources.\n\n### Q71. How do you debug frequent job failures?\n**A:** I check Databricks logs for stack traces, isolate failing transformation, and test with sample data. If schema mismatch, I enforce schema in Bronze. If bad records, I redirect to quarantine. If infrastructure, I scale cluster or",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_6",
    "text": "joining.\n\n### Q69. How do you handle out-of-memory issues in Spark jobs?\n**A:** I check if wide transformations (e.g., groupBy) are blowing up. Then, I increase executor memory or repartition data to spread load. I persist intermediate results on disk instead of memory if needed. In Walgreens, this fixed a memory issue with 1B+ row aggregation.\n\n### Q70. How do you optimize aggregations in PySpark?\n**A:** I partition data on aggregation keys, cache intermediate results if reused, and pre-filter unnecessary rows early. For distinct counts, I used approx algorithms like HyperLogLog when exact wasn't needed, saving resources.\n\n### Q71. How do you debug frequent job failures?\n**A:** I check Databricks logs for stack traces, isolate failing transformation, and test with sample data. If schema mismatch, I enforce schema in Bronze. If bad records, I redirect to quarantine. If infrastructure, I scale cluster or tune configs.\n\n### Q72. How do you tune pipeline latency?\n**A:** I parallelize independent tasks in ADF, use event triggers for real-time ingestion, and optimize Spark transformations for early filtering. For dashboards, I pre-aggregate Gold tables so BI loads in seconds.\n\n### Q73. How do you troubleshoot data quality issues raised by business?\n**A:** First, I trace lineage in Unity Catalog or Purview to identify which layer/data caused it. Then, I check validations in Silver logs. If caused by schema drift, I fix mapping and reprocess. Communication is key — I keep business updated on issue status and fix ETA.\n\n### Q74. How do you deal with high shuffle volume in Spark jobs?\n**A:** I reduce unnecessary shuffles by avoiding multiple repartitions, pruning columns early, and reusing partitioning. For unavoidable large shuffles, I increase shuffle partitions and enable AQE.\n\n### Q75. How do you handle slow dashboards due to data issues?\n**A:** I optimize Gold tables by compacting files, pre-aggregating metrics, and using summary tables. I also partition models so Power BI queries can prune efficiently. One KPI dashboard load time went from 90s to 15s after introducing summary tables.\n\n### Q76. How do you debug performance issues across layers (Bronze → Silver → Gold)?\n**A:** I compare row counts and timings logged at each layer. If Bronze is fine but Silver is slow, I check schema enforcement and dedup logic. If Gold is slow, I check joins and aggregations. Logging at each step helps isolate bottlenecks.\n\n### Q77. How do you optimize pipelines for cost?\n**A:** I use job clusters with auto-termination instead of always-on clusters. I right-size clusters based on workload. I compact files to reduce storage and metadata cost. For non-critical jobs, I use spot instances.\n\n### Q78. How do you approach troubleshooting late data arrival?\n**A:** I check if ingestion trigger failed in ADF. If source delayed, I escalate to source team. If files arrived but schema mismatched, I fix schema mapping and re-run partial load. I log SLA misses to ensure business visibility.\n\n### Q79. How do you ensure optimized queries in BI tools?\n**A:** I pre-aggregate Gold data, reduce table joins by denormalizing small dimensions, and ensure partition pruning. I also monitor Power BI query logs to tune backend models accordingly.\n\n### Q80. What's the toughest optimization challenge you solved?\n**A:** A sales fact pipeline processing 10TB+ daily was breaching SLA. Spark jobs had heavy shuffles and skewed joins. I applied salting, ZORDER, and partition tuning, and compacted files weekly. This reduced runtime from 2 hours to 40 minutes, restored SLA compliance, and saved 25% compute cost.\n\n## Section E: Governance, CI/CD, Validation, Offshore & Stakeholder Collaboration (20 Questions)\n\n### Q81. How did you implement data governance in Walgreens?\n**A:** We used Unity Catalog as the central governance layer. It controlled table- and column-level access, enforced policies like masking PII, and provided lineage. For example, DOB and SSN columns were masked for analysts while full access was limited to compliance teams. Purview was also integrated to show lineage across ADF, Databricks, and Power BI.\n\n### Q82. How do you protect PII data in Databricks?\n**A:** I masked PII columns in Unity Catalog, enforced row-level security policies, and ensured encryption at rest in ADLS. Keys and secrets were stored in Key Vault, never in code. For reporting, Power BI consumed masked views, ensuring compliance while still supporting analysis.\n\n### Q83. How do you ensure role-based access?\n**A:** Unity Catalog roles were mapped to business roles.",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_7",
    "text": "/CD, Validation, Offshore & Stakeholder Collaboration (20 Questions)\n\n### Q81. How did you implement data governance in Walgreens?\n**A:** We used Unity Catalog as the central governance layer. It controlled table- and column-level access, enforced policies like masking PII, and provided lineage. For example, DOB and SSN columns were masked for analysts while full access was limited to compliance teams. Purview was also integrated to show lineage across ADF, Databricks, and Power BI.\n\n### Q82. How do you protect PII data in Databricks?\n**A:** I masked PII columns in Unity Catalog, enforced row-level security policies, and ensured encryption at rest in ADLS. Keys and secrets were stored in Key Vault, never in code. For reporting, Power BI consumed masked views, ensuring compliance while still supporting analysis.\n\n### Q83. How do you ensure role-based access?\n**A:** Unity Catalog roles were mapped to business roles. Developers had write access in dev, read-only in test/prod. Analysts had read-only access to Gold tables only. Access was granted at schema/table/column levels, following the principle of least privilege.\n\n### Q84. How do you manage data lineage?\n**A:** Unity Catalog and Purview automatically captured lineage from ingestion → transformation → Gold → dashboards. This made it easy to trace an issue in a dashboard back to the source system. Business teams used lineage views during audits to verify compliance.\n\n### Q85. How do you integrate Databricks with Azure DevOps CI/CD?\n**A:** We stored all notebooks and ADF pipelines in Git repos. Azure DevOps pipelines deployed them to different environments. Databricks CLI automated notebook deployment, and ADF JSONs were deployed with ARM templates. Environment variables replaced connection strings dynamically.\n\n### Q86. How do you test pipelines before deployment?\n**A:** I implemented unit tests on sample datasets, row count reconciliations, and schema validations in lower environments. Each PR triggered DevOps tests to ensure correctness before merging to main. Only validated pipelines were deployed to higher environments.\n\n### Q87. How do you handle rollback in CI/CD if deployment fails?\n**A:** Each deployment version was tagged in Git. If a pipeline failed in prod, I rolled back to the last stable version quickly using Git tags and redeployment. Delta Lake time travel also supported rolling back data changes.\n\n### Q88. How do you validate data quality automatically?\n**A:** I built a PySpark validation framework that ran checks like duplicates, nulls, referential integrity, and business rules. Results were logged into validation Delta tables. Any violations triggered ADF failure path alerts and were visible in Power BI dashboards.\n\n### Q89. How do you report data quality to business?\n**A:** Business users accessed a Power BI dashboard that showed row counts, duplicates, null % by table, and rule violations. This gave real-time transparency into data health. Business could drill down to see which rules failed.\n\n### Q90. How do you monitor SLA compliance?\n**A:** I logged pipeline run durations and compared them to SLA thresholds. If a pipeline breached SLA, ADF triggered alerts. Weekly SLA adherence reports were shared with stakeholders to ensure trust in data delivery.\n\n### Q91. How do you collaborate with offshore teams?\n**A:** I led daily standups with offshore, reviewing backlog and helping unblock them. I also created reusable PySpark frameworks (ingestion, validation) so offshore could onboard new sources with minimal coding. Code reviews and knowledge-sharing sessions ensured they ramped up quickly.\n\n### Q92. How do you mentor offshore developers?\n**A:** I reviewed their PySpark scripts, explained optimization techniques like partitioning and salting, and walked them through Spark UI. I also created runbooks for common errors so they could resolve issues without escalation.\n\n### Q93. How do you handle production incidents with offshore?\n**A:** Offshore raised tickets during their shift. I joined morning calls, reviewed logs, and guided them in root cause analysis. If it was schema drift, I advised schema mapping. If infrastructure, I helped with cluster tuning. Communication with business ensured transparency on resolution ETA.\n\n### Q94. How do you communicate with stakeholders?\n**A:** I tailored communication: technical details for developers, impact and ETA for business stakeholders. For example, when a pipeline failed due to schema drift, I told business: \"Data will be delayed by 1 hour while we patch schema. No data loss.\" This maintained confidence.\n\n### Q95. How do you balance technical delivery with business priorities?\n**A:** I aligned backlog with business SLA commitments. Critical sales reports were prioritized for early-morning refresh. Less critical pipelines (like historical reloads) were scheduled during off-peak. This kept business impact minimal.\n\n### Q96. How do you approach handling unexpected requests?",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_8",
    "text": "incidents with offshore?\n**A:** Offshore raised tickets during their shift. I joined morning calls, reviewed logs, and guided them in root cause analysis. If it was schema drift, I advised schema mapping. If infrastructure, I helped with cluster tuning. Communication with business ensured transparency on resolution ETA.\n\n### Q94. How do you communicate with stakeholders?\n**A:** I tailored communication: technical details for developers, impact and ETA for business stakeholders. For example, when a pipeline failed due to schema drift, I told business: \"Data will be delayed by 1 hour while we patch schema. No data loss.\" This maintained confidence.\n\n### Q95. How do you balance technical delivery with business priorities?\n**A:** I aligned backlog with business SLA commitments. Critical sales reports were prioritized for early-morning refresh. Less critical pipelines (like historical reloads) were scheduled during off-peak. This kept business impact minimal.\n\n### Q96. How do you approach handling unexpected requests?\n**A:** I first clarify urgency with the business, then estimate technical impact. If it's quick (like adding a column), I handle same day. If bigger (like new dataset), I put it in sprint backlog. For example, adding a new KPI to Power BI was prioritized within 24h because executives needed it.\n\n### Q97. How do you ensure compliance in pipelines?\n**A:** I ensured encryption at rest (ADLS), masking in Unity Catalog, and logging of all access requests. Data quality dashboards ensured transparency. For audits, Purview lineage reports showed full data flows, proving compliance.\n\n### Q98. How do you track pipeline metrics over time?\n**A:** I logged row counts, runtime, and errors into Delta monitoring tables. A Power BI dashboard visualized historical pipeline performance, showing trends in failures or runtimes. This helped proactively optimize before SLAs broke.\n\n### Q99. How do you ensure knowledge transfer across teams?\n**A:** I maintained detailed documentation in Confluence, covering pipeline design, schema definitions, and troubleshooting steps. I also held KT sessions with offshore, walking through real examples in Databricks notebooks.\n\n### Q100. What's your biggest governance or collaboration achievement?\n**A:** My biggest achievement was implementing Unity Catalog + validation framework end-to-end. It ensured PII was masked, data lineage was transparent, and business saw data quality dashboards. Combined with mentoring offshore, it built trust in the system. Stakeholders appreciated that pipelines were compliant, optimized, and reliable — all while I led a distributed team.\n\n## Section F: Scenario-Based Questions (20 Questions)\n\n### S1. Scenario: A downstream Power BI dashboard shows wrong sales numbers. How do you handle it?\n**A:** First, I'd trace lineage in Unity Catalog to identify which Gold table feeds that report. Then, I'd check Silver → Gold transformations for logic errors. I'd validate row counts and business rules in validation logs. If the issue came from source schema drift (like a new column added in pharmacy data), I'd patch mapping in Silver, reprocess affected partitions, and communicate with stakeholders immediately, explaining ETA for fix. This keeps business confidence while resolving the root cause.\n\n### S2. Scenario: Your pipeline starts failing at 2am due to corrupt input files. What's your approach?\n**A:** I'd configure ingestion with `PERMISSIVE` mode so corrupt rows are flagged into `_corrupt_record`. These rows would be redirected into a quarantine Delta table. The main pipeline would continue for good rows, avoiding SLA breach. Later, I'd review the bad records, escalate to source teams, and patch schema validation if needed. This way, business dashboards still refresh on time.\n\n### S3. Scenario: You need to migrate Synapse notebooks into Databricks. How would you do it?\n**A:** Synapse transformations are SQL-based. I'd first review existing T-SQL scripts, then rewrite them into PySpark DataFrame transformations in Databricks. I'd leverage Delta Lake features like ACID and time travel for consistency. To validate migration, I'd reconcile row counts, run sample KPI checks, and parallel-run old Synapse vs new Databricks for a cycle before cutover.\n\n### S4. Scenario: A job that usually runs in 30 minutes suddenly takes 2 hours. How do you troubleshoot?\n**A:** I'd check Spark UI to see if shuffle partitions increased, or if skew developed. I'd check recent data volume spikes. If caused by skew, I'd apply salting or broadcast joins. If due to small file explosion, I'd run compaction (`OPTIMIZE`). If it's cluster issue, I'd tune executors or restart with right sizing. Documentation of findings ensures root cause is understood.\n\n### S5. Scenario: Two sets of users need different partitioning (state-based vs product-based). How do you solve",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_9",
    "text": ", then rewrite them into PySpark DataFrame transformations in Databricks. I'd leverage Delta Lake features like ACID and time travel for consistency. To validate migration, I'd reconcile row counts, run sample KPI checks, and parallel-run old Synapse vs new Databricks for a cycle before cutover.\n\n### S4. Scenario: A job that usually runs in 30 minutes suddenly takes 2 hours. How do you troubleshoot?\n**A:** I'd check Spark UI to see if shuffle partitions increased, or if skew developed. I'd check recent data volume spikes. If caused by skew, I'd apply salting or broadcast joins. If due to small file explosion, I'd run compaction (`OPTIMIZE`). If it's cluster issue, I'd tune executors or restart with right sizing. Documentation of findings ensures root cause is understood.\n\n### S5. Scenario: Two sets of users need different partitioning (state-based vs product-based). How do you solve it?\n**A:** Instead of duplicating datasets, I'd partition primarily on date (common for both) and apply ZORDER indexing on state and product columns. This ensures pruning for both user groups. If certain queries are very heavy, I'd create summary tables (by state or product) in Gold for faster BI performance.\n\n### S6. Scenario: A new data source is onboarded. How do you make your pipeline flexible?\n**A:** I'd store ingestion configs in metadata tables (path, schema, delimiter, rules). ADF would read configs and trigger the generic ingestion pipeline. In Silver, reusable PySpark validation functions enforce schema and rules. Offshore team only needs to add metadata entries, no code changes. This accelerates onboarding.\n\n### S7. Scenario: Business complains KPIs don't match finance reports. What's your action plan?\n**A:** First, I'd meet with finance team to understand calculation logic. Then, I'd trace current KPI logic in Gold models. If discrepancy is due to business rule misalignment (e.g., revenue net of returns), I'd adjust logic and document it. If due to data lag, I'd reschedule refresh. Clear documentation + governance ensures future alignment.\n\n### S8. Scenario: Pipeline fails due to schema drift — new column added in source. What's your fix?\n**A:** Bronze ingests flexibly. In Silver, I'd add the new column with default or nulls, update schema enforcement, and test in dev. After validating downstream transformations, I'd push change to prod. I'd also update schema documentation and inform business about the new attribute.\n\n### S9. Scenario: Offshore reports jobs failed overnight, but you're onsite. How do you handle?\n**A:** I'd quickly check logs to identify root cause. If it's schema mismatch, I'd patch mapping. If volume spike, I'd rescale cluster. I'd communicate to business with ETA (\"Data delayed by 1 hour, fix in progress\"). Meanwhile, I'd guide offshore via call so they learn the resolution. This builds trust and team capability.\n\n### S10. Scenario: Data load is complete but dashboards are very slow. What's your fix?\n**A:** I'd review Gold models. If queries scan too many rows, I'd pre-aggregate summary tables. I'd compact files (`OPTIMIZE`) and ensure partitioning matches query filters. For Power BI, I'd use DirectQuery with RLS and reduce joins by denormalizing smaller dimensions.\n\n### S11. Scenario: You need to handle late-arriving sales transactions. How do you design this?\n**A:** I'd use Delta `merge` with watermarking. Late rows within X days are merged automatically. If outside retention, I'd reprocess specific partitions after business approval. For analytics, SCD ensures late facts still map to correct dimension versions.\n\n### S12. Scenario: Business requests PII masking for compliance. What's your approach?\n**A:** I'd configure Unity Catalog to mask sensitive columns like SSN or DOB. Only compliance teams would see full values. Analysts would see masked/nulls. Access logs would track queries. This balanced compliance and usability.\n\n### S13. Scenario: API source throttles requests, but you must ingest daily. How do you manage?\n**A:** In ADF, I'd configure pagination and retries. In Databricks, I'd implement rate-limiting logic with exponential backoff. If needed, I'd parallelize calls with controlled concurrency. Failures are logged and retried separately, so ingestion completes within SLA without breaching API limits.\n\n### S14. Scenario: You find too many small files in ADLS. How do you handle this?\n**A:** I'd batch ingestion in ADF to reduce small files. In Delta, I'd run weekly compaction jobs (`OPTIMIZE`) and ZORDER for query columns",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "comprehensive_data_engineering_qa_10",
    "text": "masking for compliance. What's your approach?\n**A:** I'd configure Unity Catalog to mask sensitive columns like SSN or DOB. Only compliance teams would see full values. Analysts would see masked/nulls. Access logs would track queries. This balanced compliance and usability.\n\n### S13. Scenario: API source throttles requests, but you must ingest daily. How do you manage?\n**A:** In ADF, I'd configure pagination and retries. In Databricks, I'd implement rate-limiting logic with exponential backoff. If needed, I'd parallelize calls with controlled concurrency. Failures are logged and retried separately, so ingestion completes within SLA without breaching API limits.\n\n### S14. Scenario: You find too many small files in ADLS. How do you handle this?\n**A:** I'd batch ingestion in ADF to reduce small files. In Delta, I'd run weekly compaction jobs (`OPTIMIZE`) and ZORDER for query columns. This reduced file count, improved performance, and lowered metadata overhead.\n\n### S15. Scenario: A regulatory audit requires proof of lineage. How do you provide it?\n**A:** I'd use Purview/Unity Catalog lineage reports to show flow from source → Bronze → Silver → Gold → dashboard. This visual lineage, plus validation logs, provided end-to-end transparency. During audit, we demonstrated compliance with role-based access and PII masking.\n\n### S16. Scenario: Pipeline processing jumps from 1TB/day to 5TB/day. How do you scale?\n**A:** I'd scale clusters with more executors temporarily. Then, I'd review partitioning to ensure balanced distribution. I'd switch from full loads to incremental using modified_date. I'd also optimize joins and ZORDER on high-cardinality columns. This allowed scaling without uncontrolled cost.\n\n### S17. Scenario: Business wants new KPI \"avg sales per customer\" in dashboards. How do you deliver?\n**A:** I'd update Gold model by joining sales fact with customer dimension, compute metric in PySpark, and store in summary table. I'd validate numbers with business team, deploy changes via CI/CD, and update Power BI to expose the new KPI.\n\n### S18. Scenario: Data quality check flags 5% null store_id in Silver. What's your action?\n**A:** I'd first confirm if nulls are due to source issues. If yes, escalate to source team. Meanwhile, I'd assign default store \"Unknown\" for analysis continuity. I'd document rule and flag these records in data quality dashboard so business is aware.\n\n### S19. Scenario: Offshore wants to add a new pipeline but lacks guidance. How do you help?\n**A:** I'd ask them to define source and target configs in metadata. Then, I'd guide them on plugging configs into our reusable ingestion framework. I'd review their PySpark script and provide optimization tips. Over time, this made them independent and efficient.\n\n### S20. Scenario: Customer interview asks: 'What's your biggest achievement?'\n**A:** I'd highlight optimizing a 10TB sales pipeline from 2+ hours to 40 mins using partition tuning, salting, ZORDER, and compaction. This improvement ensured SLA compliance and cut costs by 25%. I'd also mention building validation dashboards for data quality, which gave business confidence in our platform.\n\n---\n\n## Key Takeaways\n\n- **Performance**: Proper partitioning, broadcast joins, and adaptive query execution are crucial\n- **Data Quality**: Implement comprehensive error handling and data validation\n- **Monitoring**: Log metrics and create dashboards for pipeline health\n- **Testing**: Unit tests and automated validation ensure pipeline reliability\n- **Code Organization**: Modular design improves maintainability and reusability\n- **Delta Lake**: Leverage time travel and merge capabilities for data management\n- **Governance**: Unity Catalog and Purview provide comprehensive data governance\n- **Collaboration**: Effective offshore team management and stakeholder communication",
    "metadata": {
      "tags": [
        "data-engineering",
        "pyspark",
        "databricks",
        "azure-data-factory",
        "delta-lake",
        "optimization",
        "governance",
        "walgreens",
        "interview-prep",
        "scenarios"
      ],
      "persona": "de",
      "file_path": "data_eng/comprehensive_data_engineering_qa.md",
      "file_name": "comprehensive_data_engineering_qa.md"
    }
  },
  {
    "id": "interview_examples_0",
    "text": "# Interview Response Examples\n\n## Example 1: Partitioning Strategy\n**Q: How do you pick partitioning for a 200M-row Delta table used by state and product queries?**\n\nI start from access patterns and row distribution. For mixed state/product queries, I usually partition by `txn_date` (ingest/write efficiency + pruning) and use **Z-ORDER on state, product_id** for read locality. If state is very uneven, I avoid partitioning by it to prevent small/huge-file imbalance; I rely on **Z-ORDER + AQE** and sometimes **salting** if a few states dominate joins. On the write path I **OPTIMIZE** weekly to compact files and keep pruning effective. I validate with Spark UI (shuffle read, tasks skew) and query explain to confirm pruning stats.\n\n## Example 2: Nested JSON Processing\n**Q: You get nested JSON from an API; how do you land it in Silver?**\n\nI infer or define a **StructType**, then normalize with `from_json`/`explode` into a flat schema we control. I keep raw JSON in Bronze for replay, and in Silver I enforce types, handle null defaults, and dedupe on `(business_key, event_ts)`. For evolution, I allow **additive** fields only and route breaking changes to a quarantine stream. Finally, I write Delta with merge-on keys and track schema drift in a small audit table.\n\n## Example 3: ADF Orchestration\n**Q: Orchestrating ADF → Databricks for incremental upserts—what's the flow?**\n\nADF pipeline passes a **watermark** (last modified) to a parameterized notebook. The notebook reads only changed rows, applies business rules, then runs a **Delta MERGE** keyed by the business ID plus `effective_ts`. I checkpoint the watermark in a control table, and failures roll back by not advancing it. I add a light set of row-count, duplicate, and null checks and publish metrics to a monitor table that feeds a Power BI ops dashboard.\n\n## Example 4: Performance Troubleshooting\n**Q: A long-running job regressed from 35 to 90 minutes—what's your first move?**\n\nI open the **Spark UI** and compare stage DAGs between good vs bad runs to locate the new hot stage. If it's a join, I check skew (shuffle read variance) and either **broadcast** the small side, **repartition** by the join key, or **salt** the heavy key. If it's file I/O, I check small-file counts and run **OPTIMIZE**/`ZORDER`; if input grew, I bump `spark.sql.shuffle.partitions` or enable **AQE**. I also diff config and datasource versions and add a regression alert on duration.\n\n## Example 5: Schema Evolution\n**Q: How do you handle schema changes in production pipelines?**\n\nI use a two-layer approach: Bronze accepts schema drift with permissive mode, Silver enforces strict schema. When new columns arrive, I validate them in dev first, then add with defaults in Silver after business approval. For Delta tables, I use `mergeSchema` in controlled deployments, never blindly. I also log schema changes to an audit table and update documentation. This keeps pipelines flexible but prevents downstream breakages.\n\n## Example 6: Data Quality Framework\n**Q: How do you implement data quality checks across your pipeline?**\n\nI built a reusable PySpark validation framework that runs checks like row counts, null ratios, duplicates, and business rules. Results go into a monitoring Delta table, and violations trigger ADF alerts. I surface this in Power BI dashboards so business can see data health in real-time. For critical tables, I also do referential integrity checks and flag orphaned records in a quarantine table for manual review.\n\n## Example 7: Cost Optimization\n**Q: How do you optimize Databricks costs while maintaining performance?**\n\nI use job clusters with auto-termination instead of always-on, right-size based on workload patterns, and enable auto-scaling. I compact small files weekly with `OPTIMIZE` and use ZORDER for query performance. For non-critical jobs, I use spot instances. I also monitor cluster utilization and tune executor memory to avoid over-provisioning. These changes typically cut costs by 25-30% while maintaining SLA performance.\n\n## Example 8: Multi-Source Integration\n**Q: How do you integrate data from different source systems with inconsistent schemas?**\n\nI standardize schemas in the Silver layer using conformance rules. I map different source column names to standard names, handle data type differences with explicit casting, and apply business rules for missing or invalid values. I build unified dimensions in Gold that combine data from multiple sources. For example, I created a single store dimension that maps different store codes from pharmacy",
    "metadata": {
      "tags": [
        "interview",
        "examples",
        "scenarios",
        "responses"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_examples.md",
      "file_name": "interview_examples.md"
    }
  },
  {
    "id": "interview_examples_1",
    "text": "Q: How do you optimize Databricks costs while maintaining performance?**\n\nI use job clusters with auto-termination instead of always-on, right-size based on workload patterns, and enable auto-scaling. I compact small files weekly with `OPTIMIZE` and use ZORDER for query performance. For non-critical jobs, I use spot instances. I also monitor cluster utilization and tune executor memory to avoid over-provisioning. These changes typically cut costs by 25-30% while maintaining SLA performance.\n\n## Example 8: Multi-Source Integration\n**Q: How do you integrate data from different source systems with inconsistent schemas?**\n\nI standardize schemas in the Silver layer using conformance rules. I map different source column names to standard names, handle data type differences with explicit casting, and apply business rules for missing or invalid values. I build unified dimensions in Gold that combine data from multiple sources. For example, I created a single store dimension that maps different store codes from pharmacy and sales systems to a standard format.",
    "metadata": {
      "tags": [
        "interview",
        "examples",
        "scenarios",
        "responses"
      ],
      "persona": "de",
      "file_path": "data_eng/interview_examples.md",
      "file_name": "interview_examples.md"
    }
  },
  {
    "text": "---\ntags: [business-intelligence, power-bi, tableau, data-visualization, dashboards, analytics]\npersona: bi\n---\n\n# Business Intelligence Development & Tejuu's Experience\n\n## Power BI Development\n\n### Dashboard Design and Development\n**Tejuu's Power BI Expertise:**\nI've built over 50 Power BI dashboards across different business functions - sales, finance, operations, and HR. What I've learned is that a good dashboard tells a story and helps users make decisions quickly.\n\n**My Dashboard Design Principles:**\n```\n1. Know Your Audience\n   - Executives need high-level KPIs\n   - Managers need drill-down capabilities\n   - Analysts need detailed data access\n\n2. Follow the 5-Second Rule\n   - Key insights should be visible in 5 seconds\n   - Use clear titles and labels\n   - Highlight important metrics\n\n3. Use Visual Hierarchy\n   - Most important metrics at the top\n   - Use size and color to guide attention\n   - Group related metrics together\n\n4. Keep It Simple\n   - Maximum 5-7 visuals per page\n  ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 0
    },
    "id": "tejuu_42"
  },
  {
    "text": "s\n\n3. Use Visual Hierarchy\n   - Most important metrics at the top\n   - Use size and color to guide attention\n   - Group related metrics together\n\n4. Keep It Simple\n   - Maximum 5-7 visuals per page\n   - Avoid chart junk and unnecessary decorations\n   - Use consistent colors and formatting\n\n5. Enable Interactivity\n   - Add filters and slicers\n   - Enable drill-through pages\n   - Use bookmarks for different views\n```\n\n**Sales Dashboard Example:**\n```\nPage 1: Executive Overview\n- Total Revenue (Card visual with YoY comparison)\n- Revenue Trend (Line chart with forecast)\n- Revenue by Region (Map visual)\n- Top 10 Products (Bar chart)\n- Key Metrics Table (Revenue, Orders, Avg Order Value)\n\nPage 2: Sales Performance\n- Sales by Sales Rep (Matrix with conditional formatting)\n- Sales vs Target (Gauge charts)\n- Win Rate by Product Category (Funnel chart)\n- Pipeline Analysis (Waterfall chart)\n\nPage 3: Customer Analysis\n- Customer Segmentation (Scatter plot)\n- Customer Lifetime Value (Tree map)\n- Ch",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 1
    },
    "id": "tejuu_43"
  },
  {
    "text": " charts)\n- Win Rate by Product Category (Funnel chart)\n- Pipeline Analysis (Waterfall chart)\n\nPage 3: Customer Analysis\n- Customer Segmentation (Scatter plot)\n- Customer Lifetime Value (Tree map)\n- Churn Analysis (Line and column chart)\n- Customer Acquisition Cost (KPI visual)\n```\n\n### DAX Formulas and Calculations\n**Tejuu's DAX Knowledge:**\nDAX is essential for creating powerful calculations in Power BI. Here are some of my most-used formulas:\n\n**Time Intelligence:**\n```dax\n-- Year-to-Date Sales\nYTD Sales = \nTOTALYTD(\n    SUM(Sales[Amount]),\n    'Date'[Date]\n)\n\n-- Previous Year Sales\nPY Sales = \nCALCULATE(\n    SUM(Sales[Amount]),\n    SAMEPERIODLASTYEAR('Date'[Date])\n)\n\n-- Year-over-Year Growth\nYoY Growth % = \nDIVIDE(\n    [YTD Sales] - [PY Sales],\n    [PY Sales],\n    0\n)\n\n-- Moving Average (3 months)\n3M Moving Avg = \nAVERAGEX(\n    DATESINPERIOD(\n        'Date'[Date],\n        LASTDATE('Date'[Date]),\n        -3,\n        MONTH\n    ),\n    [Total Sales]\n)\n```\n\n**Customer Metrics:**\n```dax\n-",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 2
    },
    "id": "tejuu_44"
  },
  {
    "text": " (3 months)\n3M Moving Avg = \nAVERAGEX(\n    DATESINPERIOD(\n        'Date'[Date],\n        LASTDATE('Date'[Date]),\n        -3,\n        MONTH\n    ),\n    [Total Sales]\n)\n```\n\n**Customer Metrics:**\n```dax\n-- Customer Count\nTotal Customers = \nDISTINCTCOUNT(Sales[CustomerID])\n\n-- New Customers\nNew Customers = \nCALCULATE(\n    DISTINCTCOUNT(Sales[CustomerID]),\n    FILTER(\n        ALL(Sales),\n        Sales[OrderDate] = \n        CALCULATE(\n            MIN(Sales[OrderDate]),\n            ALLEXCEPT(Sales, Sales[CustomerID])\n        )\n    )\n)\n\n-- Customer Retention Rate\nRetention Rate = \nVAR CurrentCustomers = [Total Customers]\nVAR PreviousCustomers = \n    CALCULATE(\n        [Total Customers],\n        DATEADD('Date'[Date], -1, MONTH)\n    )\nVAR RetainedCustomers = \n    CALCULATE(\n        DISTINCTCOUNT(Sales[CustomerID]),\n        FILTER(\n            ALL(Sales),\n            [CustomerID] IN VALUES(Sales[CustomerID]) &&\n            [OrderDate] >= EOMONTH(TODAY(), -2) &&\n            [OrderDate] < EOMONTH(TO",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 3
    },
    "id": "tejuu_45"
  },
  {
    "text": "s[CustomerID]),\n        FILTER(\n            ALL(Sales),\n            [CustomerID] IN VALUES(Sales[CustomerID]) &&\n            [OrderDate] >= EOMONTH(TODAY(), -2) &&\n            [OrderDate] < EOMONTH(TODAY(), -1)\n        )\n    )\nRETURN\nDIVIDE(RetainedCustomers, PreviousCustomers, 0)\n```\n\n**Advanced Calculations:**\n```dax\n-- Running Total\nRunning Total = \nCALCULATE(\n    SUM(Sales[Amount]),\n    FILTER(\n        ALL('Date'[Date]),\n        'Date'[Date] <= MAX('Date'[Date])\n    )\n)\n\n-- Pareto Analysis (80/20 Rule)\nCumulative % = \nVAR CurrentRevenue = SUM(Sales[Amount])\nVAR TotalRevenue = \n    CALCULATE(\n        SUM(Sales[Amount]),\n        ALL(Products)\n    )\nVAR RunningTotal = \n    CALCULATE(\n        SUM(Sales[Amount]),\n        FILTER(\n            ALL(Products),\n            RANKX(\n                ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n                DESC\n            ) <= RANKX(\n                ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 4
    },
    "id": "tejuu_46"
  },
  {
    "text": "ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n                DESC\n            ) <= RANKX(\n                ALL(Products),\n                SUM(Sales[Amount]),\n                ,\n                DESC\n            )\n        )\n    )\nRETURN\nDIVIDE(RunningTotal, TotalRevenue, 0)\n\n-- Dynamic Ranking\nProduct Rank = \nRANKX(\n    ALL(Products[ProductName]),\n    [Total Sales],\n    ,\n    DESC,\n    DENSE\n)\n```\n\n### Data Modeling in Power BI\n**Tejuu's Data Modeling Approach:**\nGood data modeling is the foundation of a fast and reliable Power BI report. Here's my approach:\n\n**Star Schema Design:**\n```\nFact Tables:\n- Sales (OrderID, CustomerID, ProductID, DateKey, Amount, Quantity)\n- Inventory (ProductID, DateKey, StockLevel, WarehouseID)\n- Budget (DateKey, DepartmentID, BudgetAmount)\n\nDimension Tables:\n- Customers (CustomerID, Name, Segment, Region, City)\n- Products (ProductID, Name, Category, SubCategory, Price)\n- Date (DateKey, Date, Year, Quarter, Month, Week, Day)\n- Employees ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 5
    },
    "id": "tejuu_47"
  },
  {
    "text": "\nDimension Tables:\n- Customers (CustomerID, Name, Segment, Region, City)\n- Products (ProductID, Name, Category, SubCategory, Price)\n- Date (DateKey, Date, Year, Quarter, Month, Week, Day)\n- Employees (EmployeeID, Name, Department, Manager, HireDate)\n```\n\n**Relationship Best Practices:**\n```\n1. Use Star Schema\n   - Fact tables in the center\n   - Dimension tables around it\n   - One-to-many relationships\n\n2. Create a Date Table\n   - Use CALENDAR or CALENDARAUTO\n   - Mark as date table\n   - Include all needed date attributes\n\n3. Avoid Bi-Directional Relationships\n   - Can cause ambiguity\n   - Impacts performance\n   - Use DAX instead when possible\n\n4. Hide Unnecessary Columns\n   - Foreign keys\n   - Technical columns\n   - Improves user experience\n\n5. Use Calculated Columns vs Measures Appropriately\n   - Calculated columns: Static, stored in model\n   - Measures: Dynamic, calculated at query time\n   - Prefer measures for aggregations\n```\n\n## Tableau Development\n\n### Building Interactive Dashbo",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 6
    },
    "id": "tejuu_48"
  },
  {
    "text": "ely\n   - Calculated columns: Static, stored in model\n   - Measures: Dynamic, calculated at query time\n   - Prefer measures for aggregations\n```\n\n## Tableau Development\n\n### Building Interactive Dashboards\n**Tejuu's Tableau Experience:**\nI've worked with Tableau for 3+ years, building executive dashboards and self-service analytics solutions. Tableau's strength is its intuitive drag-and-drop interface and powerful visualization capabilities.\n\n**My Tableau Dashboard Structure:**\n```\n1. Overview Dashboard\n   - KPI Summary\n   - Trend Analysis\n   - Geographic Distribution\n   - Quick Filters\n\n2. Detailed Analysis\n   - Drill-down capabilities\n   - Parameter controls\n   - Calculated fields\n   - Reference lines and bands\n\n3. What-If Analysis\n   - Parameters for scenarios\n   - Calculated fields for projections\n   - Dynamic titles and labels\n```\n\n**Calculated Fields in Tableau:**\n```\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / LOOKUP(SUM([Sales]), -12)\n\n// Customer Lifet",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 7
    },
    "id": "tejuu_49"
  },
  {
    "text": "projections\n   - Dynamic titles and labels\n```\n\n**Calculated Fields in Tableau:**\n```\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / LOOKUP(SUM([Sales]), -12)\n\n// Customer Lifetime Value\n{FIXED [Customer ID]: SUM([Sales])}\n\n// Cohort Analysis\nIF DATEDIFF('month', {FIXED [Customer ID]: MIN([Order Date])}, [Order Date]) = 0\nTHEN \"Month 0\"\nELSEIF DATEDIFF('month', {FIXED [Customer ID]: MIN([Order Date])}, [Order Date]) = 1\nTHEN \"Month 1\"\nELSE \"Month 2+\"\nEND\n\n// Dynamic Ranking\nRANK(SUM([Sales]), 'desc')\n\n// Moving Average\nWINDOW_AVG(SUM([Sales]), -2, 0)\n```\n\n## Excel for Business Intelligence\n\n### Advanced Excel Techniques\n**Tejuu's Excel Expertise:**\nEven with Power BI and Tableau, Excel remains a critical tool. I use it for ad-hoc analysis, data preparation, and quick reports.\n\n**Power Query (ETL in Excel):**\n```\nCommon Transformations:\n1. Remove Duplicates\n2. Fill Down/Up\n3. Pivot/Unpivot Columns\n4. Merge Queries (Joins)\n5. Append Queries (Union)\n6. Split Columns",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 8
    },
    "id": "tejuu_50"
  },
  {
    "text": " reports.\n\n**Power Query (ETL in Excel):**\n```\nCommon Transformations:\n1. Remove Duplicates\n2. Fill Down/Up\n3. Pivot/Unpivot Columns\n4. Merge Queries (Joins)\n5. Append Queries (Union)\n6. Split Columns\n7. Change Data Types\n8. Add Custom Columns\n\nM Language Examples:\n// Add custom column\n= Table.AddColumn(#\"Previous Step\", \"Full Name\", \n    each [First Name] & \" \" & [Last Name])\n\n// Filter rows\n= Table.SelectRows(#\"Previous Step\", \n    each [Sales] > 1000)\n\n// Group by\n= Table.Group(#\"Previous Step\", {\"Region\"}, \n    {{\"Total Sales\", each List.Sum([Sales]), type number}})\n```\n\n**Power Pivot and Data Modeling:**\n```\nDAX in Excel:\n// Total Sales\nTotal Sales:=SUM(Sales[Amount])\n\n// Sales vs Budget Variance\nVariance:=[Total Sales]-[Total Budget]\n\n// Variance %\nVariance %:=DIVIDE([Variance],[Total Budget],0)\n\n// Top N Products\nTop 10 Products:=\nCALCULATE(\n    [Total Sales],\n    TOPN(10, ALL(Products[Product Name]), [Total Sales], DESC)\n)\n```\n\n**Advanced Formulas:**\n```excel\n// Dynamic Named R",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 9
    },
    "id": "tejuu_51"
  },
  {
    "text": "l Budget],0)\n\n// Top N Products\nTop 10 Products:=\nCALCULATE(\n    [Total Sales],\n    TOPN(10, ALL(Products[Product Name]), [Total Sales], DESC)\n)\n```\n\n**Advanced Formulas:**\n```excel\n// Dynamic Named Ranges\n=OFFSET(Sheet1!$A$1,0,0,COUNTA(Sheet1!$A:$A),1)\n\n// Array Formulas (Ctrl+Shift+Enter)\n=SUM(IF(A2:A100=\"Yes\",B2:B100,0))\n\n// XLOOKUP (Modern Excel)\n=XLOOKUP(A2,Table1[ID],Table1[Name],\"Not Found\",0,1)\n\n// SUMIFS with Multiple Criteria\n=SUMIFS(Sales[Amount],Sales[Region],\"West\",Sales[Date],\">=\"&DATE(2023,1,1))\n\n// Dynamic Dashboard with CHOOSE and MATCH\n=CHOOSE(MATCH(B1,{\"Sales\",\"Profit\",\"Quantity\"},0),\n    SUM(Sales[Amount]),\n    SUM(Sales[Profit]),\n    SUM(Sales[Quantity]))\n```\n\n## Data Visualization Best Practices\n\n### Choosing the Right Chart\n**Tejuu's Chart Selection Guide:**\n```\nComparison:\n- Bar Chart: Compare values across categories\n- Column Chart: Show changes over time\n- Bullet Chart: Compare actual vs target\n\nComposition:\n- Pie Chart: Show parts of a whole (max 5 slices)\n- ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 10
    },
    "id": "tejuu_52"
  },
  {
    "text": "arison:\n- Bar Chart: Compare values across categories\n- Column Chart: Show changes over time\n- Bullet Chart: Compare actual vs target\n\nComposition:\n- Pie Chart: Show parts of a whole (max 5 slices)\n- Stacked Bar: Show composition across categories\n- Treemap: Show hierarchical composition\n\nDistribution:\n- Histogram: Show frequency distribution\n- Box Plot: Show statistical distribution\n- Scatter Plot: Show correlation between variables\n\nTrend:\n- Line Chart: Show trends over time\n- Area Chart: Show cumulative trends\n- Waterfall: Show sequential changes\n\nRelationship:\n- Scatter Plot: Show correlation\n- Bubble Chart: Show 3 dimensions\n- Heat Map: Show patterns in matrix data\n```\n\n### Color Theory and Accessibility\n**My Color Guidelines:**\n```\n1. Use Color Purposefully\n   - Red: Negative, danger, stop\n   - Green: Positive, success, go\n   - Blue: Neutral, information\n   - Orange/Yellow: Warning, attention\n\n2. Ensure Accessibility\n   - Check color blindness compatibility\n   - Use patterns in a",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 11
    },
    "id": "tejuu_53"
  },
  {
    "text": ", stop\n   - Green: Positive, success, go\n   - Blue: Neutral, information\n   - Orange/Yellow: Warning, attention\n\n2. Ensure Accessibility\n   - Check color blindness compatibility\n   - Use patterns in addition to colors\n   - Maintain sufficient contrast\n   - Test with grayscale\n\n3. Limit Color Palette\n   - Maximum 5-6 colors per dashboard\n   - Use shades for variations\n   - Keep brand colors consistent\n\n4. Highlight What Matters\n   - Use bright colors for important data\n   - Gray out less important information\n   - Use white space effectively\n```\n\n## ETL and Data Preparation\n\n### Data Quality Checks\n**Tejuu's Data Validation Process:**\n```\n1. Completeness Checks\n   - Missing values\n   - Null percentages\n   - Required fields populated\n\n2. Accuracy Checks\n   - Data type validation\n   - Range checks (min/max)\n   - Format validation (email, phone)\n\n3. Consistency Checks\n   - Cross-field validation\n   - Referential integrity\n   - Duplicate detection\n\n4. Timeliness Checks\n   - Data freshness\n ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 12
    },
    "id": "tejuu_54"
  },
  {
    "text": "hecks (min/max)\n   - Format validation (email, phone)\n\n3. Consistency Checks\n   - Cross-field validation\n   - Referential integrity\n   - Duplicate detection\n\n4. Timeliness Checks\n   - Data freshness\n   - Update frequency\n   - Historical completeness\n\nSQL for Data Quality:\n-- Check for nulls\nSELECT \n    COUNT(*) as total_records,\n    COUNT(customer_id) as non_null_customers,\n    COUNT(*) - COUNT(customer_id) as null_count,\n    ROUND((COUNT(*) - COUNT(customer_id)) * 100.0 / COUNT(*), 2) as null_percentage\nFROM sales;\n\n-- Check for duplicates\nSELECT \n    order_id,\n    COUNT(*) as duplicate_count\nFROM orders\nGROUP BY order_id\nHAVING COUNT(*) > 1;\n\n-- Check data ranges\nSELECT \n    MIN(order_date) as earliest_date,\n    MAX(order_date) as latest_date,\n    MIN(amount) as min_amount,\n    MAX(amount) as max_amount,\n    AVG(amount) as avg_amount\nFROM orders;\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- Power BI dashboard development and DAX\n- Tableau visualization and calculated fie",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 13
    },
    "id": "tejuu_55"
  },
  {
    "text": "t) as max_amount,\n    AVG(amount) as avg_amount\nFROM orders;\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- Power BI dashboard development and DAX\n- Tableau visualization and calculated fields\n- Advanced Excel (Power Query, Power Pivot)\n- SQL for data analysis\n- Data modeling and ETL\n- Data visualization best practices\n\n### Tools & Technologies:\n- **BI Tools**: Power BI, Tableau, Qlik Sense\n- **Databases**: SQL Server, Oracle, MySQL, PostgreSQL\n- **Excel**: Power Query, Power Pivot, VBA\n- **ETL**: SSIS, Alteryx, Informatica\n- **Cloud**: Azure, AWS, Google Cloud\n\n### Achievements:\n- Built 50+ Power BI dashboards used by 200+ users\n- Reduced report generation time from 2 days to 2 hours\n- Improved data accuracy by 95% through automated quality checks\n- Saved $100K annually by identifying cost optimization opportunities\n- Trained 30+ business users on self-service BI tools\n\n### Project Examples:\n- Sales Performance Dashboard (real-time tracking, 15+ KPIs)\n- Customer Analytics P",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 14
    },
    "id": "tejuu_56"
  },
  {
    "text": "tifying cost optimization opportunities\n- Trained 30+ business users on self-service BI tools\n\n### Project Examples:\n- Sales Performance Dashboard (real-time tracking, 15+ KPIs)\n- Customer Analytics Platform (segmentation, churn prediction)\n- Financial Reporting Suite (P&L, balance sheet, cash flow)\n- Operational Metrics Dashboard (efficiency, productivity, quality)\n- Executive KPI Dashboard (company-wide metrics, drill-down)\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/bi_development.md",
      "file_name": "bi_development.md",
      "chunk_index": 15
    },
    "id": "tejuu_57"
  },
  {
    "text": "---\ntags: [tableau, data-visualization, calculated-fields, parameters, dashboards, lod]\npersona: tejuu\n---\n\n# Tableau Expertise - Tejuu's Experience\n\n## Tableau Dashboard Development\n\n### Building Executive Dashboards\n**Tejuu's Approach:**\nSo I've built numerous executive dashboards in Tableau, and what I've learned is that executives want to see the story quickly. They don't have time to dig through data - they need insights at a glance.\n\n**My Executive Dashboard Structure:**\n```\nPage 1: Overview\n- KPI Summary Cards (Revenue, Profit, Growth %)\n- Trend Line (12-month rolling)\n- Geographic Heat Map\n- Top 10 Products/Customers\n- Quick Filters (Date, Region, Category)\n\nPage 2: Deep Dive\n- Detailed Tables with Drill-Down\n- Comparison Charts (Actual vs Target vs Prior Year)\n- Distribution Analysis\n- Cohort Analysis\n\nPage 3: What-If Analysis\n- Parameter Controls for Scenarios\n- Calculated Projections\n- Sensitivity Analysis\n```\n\n### Interactive Features I Implement\n**Navigation and Interactiv",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 16
    },
    "id": "tejuu_58"
  },
  {
    "text": "sis\n- Cohort Analysis\n\nPage 3: What-If Analysis\n- Parameter Controls for Scenarios\n- Calculated Projections\n- Sensitivity Analysis\n```\n\n### Interactive Features I Implement\n**Navigation and Interactivity:**\n\n```\n1. Dashboard Actions:\n   - Filter Actions: Click on region to filter all visuals\n   - Highlight Actions: Hover to highlight related data\n   - URL Actions: Link to external systems\n   - Parameter Actions: Dynamic parameter updates\n\n2. Navigation:\n   - Button navigation between pages\n   - Breadcrumb navigation\n   - Back button functionality\n   - Reset filters button\n\n3. Tooltips:\n   - Custom tooltip worksheets\n   - Show additional context\n   - Mini visualizations in tooltips\n   - Formatted text with key metrics\n```\n\n## Advanced Calculated Fields\n\n### LOD (Level of Detail) Expressions\n**Tejuu's Most-Used LOD Patterns:**\n\n```tableau\n// FIXED - Customer Lifetime Value\n{FIXED [Customer ID]: SUM([Sales])}\n\n// INCLUDE - Sales with Category Context\n{INCLUDE [Category]: AVG([Sales])}\n\n//",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 17
    },
    "id": "tejuu_59"
  },
  {
    "text": "\n**Tejuu's Most-Used LOD Patterns:**\n\n```tableau\n// FIXED - Customer Lifetime Value\n{FIXED [Customer ID]: SUM([Sales])}\n\n// INCLUDE - Sales with Category Context\n{INCLUDE [Category]: AVG([Sales])}\n\n// EXCLUDE - Overall Average Excluding Region\n{EXCLUDE [Region]: AVG([Sales])}\n\n// Customer First Purchase Date\n{FIXED [Customer ID]: MIN([Order Date])}\n\n// Customer Cohort Analysis\n// Cohort Month\n{FIXED [Customer ID]: \n    MIN(DATETRUNC('month', [Order Date]))}\n\n// Months Since First Purchase\nDATEDIFF('month', \n    {FIXED [Customer ID]: MIN([Order Date])},\n    [Order Date]\n)\n\n// Percent of Total by Category\nSUM([Sales]) / \n{FIXED [Category]: SUM([Sales])}\n\n// Running Total by Customer\nRUNNING_SUM(SUM([Sales]))\n\n// Rank within Category\nRANK(SUM([Sales]), 'desc')\n\n// Dense Rank for Top N\nRANK_DENSE(SUM([Sales]), 'desc')\n```\n\n### Table Calculations\n**My Frequently Used Table Calcs:**\n\n```tableau\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / \nLOOKUP(SUM([Sales]), -12)\n\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 18
    },
    "id": "tejuu_60"
  },
  {
    "text": "NSE(SUM([Sales]), 'desc')\n```\n\n### Table Calculations\n**My Frequently Used Table Calcs:**\n\n```tableau\n// Year-over-Year Growth\n(SUM([Sales]) - LOOKUP(SUM([Sales]), -12)) / \nLOOKUP(SUM([Sales]), -12)\n\n// Moving Average (3 months)\nWINDOW_AVG(SUM([Sales]), -2, 0)\n\n// Percent of Total\nSUM([Sales]) / TOTAL(SUM([Sales]))\n\n// Running Total\nRUNNING_SUM(SUM([Sales]))\n\n// Percent Difference from Average\n(SUM([Sales]) - WINDOW_AVG(SUM([Sales]))) / \nWINDOW_AVG(SUM([Sales]))\n\n// Index for Row Numbering\nINDEX()\n\n// First/Last Value in Partition\nFIRST() = 0  // First row\nLAST() = 0   // Last row\n\n// Lookup Previous Value\nLOOKUP(SUM([Sales]), -1)\n\n// Window Sum for Cumulative %\nRUNNING_SUM(SUM([Sales])) / \nTOTAL(SUM([Sales]))\n```\n\n### Complex Business Logic\n**Healthcare Analytics at Stryker:**\n\n```tableau\n// Patient Risk Stratification\nIF [Total Claims] > 50000 AND [Chronic Conditions] >= 3 THEN \"High Risk\"\nELSEIF [Total Claims] > 25000 OR [Chronic Conditions] >= 2 THEN \"Medium Risk\"\nELSE \"Low Risk\"\nE",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 19
    },
    "id": "tejuu_61"
  },
  {
    "text": "\n// Patient Risk Stratification\nIF [Total Claims] > 50000 AND [Chronic Conditions] >= 3 THEN \"High Risk\"\nELSEIF [Total Claims] > 25000 OR [Chronic Conditions] >= 2 THEN \"Medium Risk\"\nELSE \"Low Risk\"\nEND\n\n// Readmission Flag\nIF DATEDIFF('day', [Previous Discharge Date], [Admission Date]) <= 30\nTHEN \"30-Day Readmission\"\nELSE \"New Admission\"\nEND\n\n// Length of Stay Category\nCASE [Length of Stay]\n    WHEN 0 THEN \"Same Day\"\n    WHEN 1 THEN \"Overnight\"\n    WHEN 2 THEN \"2 Days\"\n    WHEN 3 THEN \"3 Days\"\n    ELSE \"Extended Stay (4+ days)\"\nEND\n\n// Cost per Day\n[Total Cost] / [Length of Stay]\n\n// Case Mix Adjusted Cost\n[Total Cost] / [Case Mix Index]\n\n// Utilization Rate\n[Actual Bed Days] / [Available Bed Days]\n```\n\n## Parameters and What-If Analysis\n\n### Dynamic Parameters\n**Tejuu's Parameter Strategies:**\n\n```tableau\n// Top N Parameter\nParameter: Top N (Integer, Range 5-20, Default 10)\n\nCalculated Field: Top N Filter\nRANK(SUM([Sales]), 'desc') <= [Top N Parameter]\n\n// Date Range Parameter\nParame",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 20
    },
    "id": "tejuu_62"
  },
  {
    "text": "ies:**\n\n```tableau\n// Top N Parameter\nParameter: Top N (Integer, Range 5-20, Default 10)\n\nCalculated Field: Top N Filter\nRANK(SUM([Sales]), 'desc') <= [Top N Parameter]\n\n// Date Range Parameter\nParameter: Date Range (String, List)\n- Last 7 Days\n- Last 30 Days\n- Last 90 Days\n- Last 12 Months\n- Year to Date\n- Custom\n\nCalculated Field: Date Filter\nCASE [Date Range Parameter]\n    WHEN \"Last 7 Days\" THEN [Order Date] >= TODAY() - 7\n    WHEN \"Last 30 Days\" THEN [Order Date] >= TODAY() - 30\n    WHEN \"Last 90 Days\" THEN [Order Date] >= TODAY() - 90\n    WHEN \"Last 12 Months\" THEN [Order Date] >= DATEADD('month', -12, TODAY())\n    WHEN \"Year to Date\" THEN YEAR([Order Date]) = YEAR(TODAY())\nEND\n\n// Metric Selector Parameter\nParameter: Select Metric (String, List)\n- Sales\n- Profit\n- Quantity\n- Avg Order Value\n\nCalculated Field: Selected Metric\nCASE [Select Metric Parameter]\n    WHEN \"Sales\" THEN SUM([Sales])\n    WHEN \"Profit\" THEN SUM([Profit])\n    WHEN \"Quantity\" THEN SUM([Quantity])\n    WHEN \"Av",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 21
    },
    "id": "tejuu_63"
  },
  {
    "text": "er Value\n\nCalculated Field: Selected Metric\nCASE [Select Metric Parameter]\n    WHEN \"Sales\" THEN SUM([Sales])\n    WHEN \"Profit\" THEN SUM([Profit])\n    WHEN \"Quantity\" THEN SUM([Quantity])\n    WHEN \"Avg Order Value\" THEN AVG([Order Value])\nEND\n\n// Growth Rate Parameter for Projections\nParameter: Growth Rate (Float, Range 0-0.5, Default 0.1)\n\nCalculated Field: Projected Sales\nSUM([Sales]) * (1 + [Growth Rate Parameter])\n```\n\n### Scenario Analysis\n**What-If Modeling:**\n\n```tableau\n// Price Elasticity Scenario\nParameter: Price Change % (Float, -50% to +50%)\n\nCalculated Field: New Price\n[Current Price] * (1 + [Price Change %])\n\nCalculated Field: Projected Demand\n// Assuming elasticity of -1.5\n[Current Demand] * POWER(\n    (1 + [Price Change %]),\n    -1.5\n)\n\nCalculated Field: Projected Revenue\n[New Price] * [Projected Demand]\n\n// Staffing Scenario\nParameter: Additional Staff (Integer, 0-50)\n\nCalculated Field: Total Staff\n[Current Staff] + [Additional Staff]\n\nCalculated Field: Projected Capac",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 22
    },
    "id": "tejuu_64"
  },
  {
    "text": "New Price] * [Projected Demand]\n\n// Staffing Scenario\nParameter: Additional Staff (Integer, 0-50)\n\nCalculated Field: Total Staff\n[Current Staff] + [Additional Staff]\n\nCalculated Field: Projected Capacity\n[Total Staff] * [Productivity per Staff]\n\nCalculated Field: Projected Cost\n([Current Staff] * [Current Wage]) + \n([Additional Staff] * [New Hire Wage])\n\n// Budget Allocation Scenario\nParameter: Marketing % (Float, 0-1)\n\nCalculated Field: Marketing Budget\n[Total Budget] * [Marketing % Parameter]\n\nCalculated Field: Operations Budget\n[Total Budget] * (1 - [Marketing % Parameter])\n```\n\n## Data Blending and Relationships\n\n### Cross-Database Joins\n**My Approach at CVS Health:**\n\n```\nPrimary Data Source: SQL Server (Sales Data)\nSecondary Data Source: Excel (Target Data)\n\nBlending Strategy:\n1. Define relationships on common fields\n2. Use primary data source for main viz\n3. Bring in secondary data as needed\n4. Aggregate appropriately to avoid duplication\n\nExample:\n- Primary: fact_sales (Order D",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 23
    },
    "id": "tejuu_65"
  },
  {
    "text": "ine relationships on common fields\n2. Use primary data source for main viz\n3. Bring in secondary data as needed\n4. Aggregate appropriately to avoid duplication\n\nExample:\n- Primary: fact_sales (Order Date, Product ID, Sales)\n- Secondary: targets (Month, Product ID, Target)\n- Relationship: Product ID\n- Calculation: Actual vs Target %\n```\n\n### Data Source Filters\n**Performance Optimization:**\n\n```tableau\n// Extract Filters\n1. Date Range: Last 2 years only\n2. Status: Exclude \"Cancelled\" orders\n3. Amount: > $0\n\n// Context Filters\n- Region (if analyzing specific region)\n- Date Range (before other filters)\n\n// Data Source Filters\n- Exclude test data\n- Filter out inactive records\n- Remove PII if not needed\n```\n\n## Dashboard Performance Optimization\n\n### Tejuu's Performance Checklist\n**What I Do to Speed Up Dashboards:**\n\n```\n1. Data Source Optimization:\n   - Use extracts instead of live connections\n   - Filter data at source\n   - Aggregate data when possible\n   - Remove unused fields\n   - Use ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 24
    },
    "id": "tejuu_66"
  },
  {
    "text": " Speed Up Dashboards:**\n\n```\n1. Data Source Optimization:\n   - Use extracts instead of live connections\n   - Filter data at source\n   - Aggregate data when possible\n   - Remove unused fields\n   - Use appropriate data types\n\n2. Calculation Optimization:\n   - Avoid complex calculations in viz\n   - Use table calcs instead of row-level calcs\n   - Materialize calculations in extract\n   - Use FIXED LODs sparingly\n   - Avoid nested LODs\n\n3. Visual Optimization:\n   - Limit number of marks (< 10K per sheet)\n   - Use aggregated data\n   - Avoid high-cardinality dimensions\n   - Use filters to reduce data\n   - Limit dashboard complexity (< 10 sheets)\n\n4. Dashboard Design:\n   - Use containers efficiently\n   - Minimize dashboard actions\n   - Use sheet-specific filters\n   - Hide unused sheets\n   - Optimize images and logos\n\n5. Extract Optimization:\n   - Schedule refreshes during off-hours\n   - Use incremental refresh\n   - Partition large extracts\n   - Remove historical data if not needed\n```\n\n### Extr",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 25
    },
    "id": "tejuu_67"
  },
  {
    "text": "e images and logos\n\n5. Extract Optimization:\n   - Schedule refreshes during off-hours\n   - Use incremental refresh\n   - Partition large extracts\n   - Remove historical data if not needed\n```\n\n### Extract Refresh Strategies\n**My Implementation:**\n\n```\nFull Refresh:\n- Weekly on Sunday night\n- For dimension tables\n- For small fact tables\n\nIncremental Refresh:\n- Daily for large fact tables\n- Filter: [Date] > MAX([Date]) - 7\n- Keeps last 7 days refreshable\n\nHybrid Approach:\n- Incremental daily (Mon-Sat)\n- Full refresh weekly (Sunday)\n- Ensures data consistency\n```\n\n## Advanced Visualizations\n\n### Custom Chart Types\n**Tejuu's Favorite Advanced Viz:**\n\n```\n1. Waterfall Chart\n   - Show sequential changes\n   - Revenue bridge analysis\n   - Budget variance analysis\n\n2. Bullet Chart\n   - KPI performance vs target\n   - Show good/satisfactory/poor ranges\n   - Compact executive view\n\n3. Funnel Chart\n   - Conversion analysis\n   - Sales pipeline stages\n   - Patient journey analysis\n\n4. Sankey Diagram\n ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 26
    },
    "id": "tejuu_68"
  },
  {
    "text": "ce vs target\n   - Show good/satisfactory/poor ranges\n   - Compact executive view\n\n3. Funnel Chart\n   - Conversion analysis\n   - Sales pipeline stages\n   - Patient journey analysis\n\n4. Sankey Diagram\n   - Flow analysis\n   - Customer journey\n   - Resource allocation\n\n5. Box and Whisker Plot\n   - Distribution analysis\n   - Outlier detection\n   - Statistical analysis\n\n6. Pareto Chart\n   - 80/20 analysis\n   - Prioritization\n   - ABC analysis\n```\n\n### Custom Shapes and Icons\n**Visual Enhancement:**\n\n```\nCustom Shapes for:\n- Geographic maps (custom territories)\n- Process flow diagrams\n- Organization charts\n- Network diagrams\n\nIcon Usage:\n- KPI indicators (↑ ↓ ■)\n- Status indicators (✓ ✗ ⚠)\n- Category icons\n- Action buttons\n```\n\n## Tableau Server Administration\n\n### Publishing and Permissions\n**My Governance Model:**\n\n```\nFolder Structure:\n/Executive Dashboards\n  - Finance\n  - Operations\n  - Sales\n/Departmental Reports\n  - HR\n  - Marketing\n  - IT\n/Sandbox\n  - Development\n  - Testing\n\nPermissio",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 27
    },
    "id": "tejuu_69"
  },
  {
    "text": " Governance Model:**\n\n```\nFolder Structure:\n/Executive Dashboards\n  - Finance\n  - Operations\n  - Sales\n/Departmental Reports\n  - HR\n  - Marketing\n  - IT\n/Sandbox\n  - Development\n  - Testing\n\nPermission Levels:\n- Viewer: Can view only\n- Interactor: Can view and interact\n- Editor: Can edit and publish\n- Publisher: Full control\n\nRow-Level Security:\n- User filters in data source\n- Dynamic user-based filtering\n- Integration with AD groups\n```\n\n### Monitoring and Maintenance\n**What I Track:**\n\n```\nPerformance Metrics:\n- Dashboard load times\n- Extract refresh duration\n- Failed refreshes\n- User adoption rates\n- Most viewed dashboards\n\nMaintenance Tasks:\n- Weekly: Review failed refreshes\n- Monthly: Clean up unused content\n- Quarterly: Performance audit\n- Annually: User access review\n\nAlerts:\n- Extract refresh failures\n- Dashboard load time > 10s\n- Disk space warnings\n- License usage threshold\n```\n\n## Tableau Prep for ETL\n\n### Data Preparation Workflows\n**Tejuu's Prep Flows:**\n\n```\nCommon Transf",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 28
    },
    "id": "tejuu_70"
  },
  {
    "text": "act refresh failures\n- Dashboard load time > 10s\n- Disk space warnings\n- License usage threshold\n```\n\n## Tableau Prep for ETL\n\n### Data Preparation Workflows\n**Tejuu's Prep Flows:**\n\n```\nCommon Transformations:\n1. Clean Data\n   - Remove nulls\n   - Fix data types\n   - Standardize formats\n   - Remove duplicates\n\n2. Join Data\n   - Left/Right/Inner joins\n   - Union multiple sources\n   - Pivot/Unpivot\n\n3. Aggregate Data\n   - Group by dimensions\n   - Calculate summaries\n   - Create rollups\n\n4. Calculate Fields\n   - Derived columns\n   - Conditional logic\n   - Date calculations\n\n5. Output\n   - Tableau extract (.hyper)\n   - Database table\n   - CSV file\n```\n\n### Prep Best Practices\n**My Workflow Design:**\n\n```\n1. Modular Design\n   - Separate flows for different sources\n   - Reusable cleaning steps\n   - Clear naming conventions\n\n2. Documentation\n   - Add descriptions to steps\n   - Document business logic\n   - Note data quality issues\n\n3. Error Handling\n   - Check for nulls\n   - Validate data type",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 29
    },
    "id": "tejuu_71"
  },
  {
    "text": "  - Clear naming conventions\n\n2. Documentation\n   - Add descriptions to steps\n   - Document business logic\n   - Note data quality issues\n\n3. Error Handling\n   - Check for nulls\n   - Validate data types\n   - Handle edge cases\n   - Log errors\n\n4. Performance\n   - Filter early\n   - Aggregate when possible\n   - Limit row sampling in dev\n   - Optimize joins\n```\n\n## Interview Talking Points\n\n### Technical Achievements\n- Built 30+ Tableau dashboards for healthcare and retail\n- Implemented complex LOD calculations for patient analytics\n- Optimized dashboard performance from 45s to 5s load time\n- Created reusable templates reducing development time by 50%\n- Trained 20+ users on Tableau best practices\n\n### Problem-Solving Examples\n**Performance Issue:**\n\"At Stryker, we had this patient dashboard that was super slow - taking 45 seconds to load. I analyzed it and found we were using live connection to a large database with no filters. I created an extract with appropriate filters, moved some calcu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 30
    },
    "id": "tejuu_72"
  },
  {
    "text": "t was super slow - taking 45 seconds to load. I analyzed it and found we were using live connection to a large database with no filters. I created an extract with appropriate filters, moved some calculations to the data source, and reduced the number of marks. Got it down to 5 seconds.\"\n\n**Complex Calculation:**\n\"At CVS Health, we needed to calculate patient cohort retention over time. This required LOD expressions to identify first purchase date, then table calculations to track behavior over subsequent months. I broke it down into smaller calculated fields, tested each piece, and documented the logic for the team.\"\n\n### Tools & Technologies\n- **Tableau**: Desktop, Server, Prep, Online\n- **Calculations**: LOD, Table Calcs, Parameters\n- **Data Sources**: SQL Server, Oracle, Excel, CSV\n- **Integration**: Tableau APIs, webhooks, embedded analytics\n- **Administration**: User management, permissions, monitoring\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 31
    },
    "id": "tejuu_73"
  },
  {
    "text": "Integration**: Tableau APIs, webhooks, embedded analytics\n- **Administration**: User management, permissions, monitoring\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/tableau_expertise.md",
      "file_name": "tableau_expertise.md",
      "chunk_index": 32
    },
    "id": "tejuu_74"
  },
  {
    "text": "---\ntags: [power-bi, dax, data-modeling, performance, rls, best-practices]\npersona: tejuu\n---\n\n# Advanced Power BI & DAX - Tejuu's Expertise\n\n## Power BI Data Modeling\n\n### Star Schema Design for Healthcare Analytics\n**Tejuu's Implementation at Stryker:**\nSo at Stryker, I designed this comprehensive star schema for our Medicaid and healthcare analytics. The key was understanding the business requirements first - what questions do clinicians, finance, and operations teams need to answer?\n\n**My Star Schema Structure:**\n```\nFact Tables:\n- fact_claims (claim_id, patient_key, provider_key, date_key, claim_amount, paid_amount, denied_amount)\n- fact_patient_visits (visit_id, patient_key, facility_key, date_key, visit_type, duration_minutes, cost)\n- fact_inventory (product_key, warehouse_key, date_key, quantity_on_hand, reorder_level, unit_cost)\n\nDimension Tables:\n- dim_patient (patient_key, patient_id, age_group, gender, state, insurance_type, risk_score)\n- dim_provider (provider_key, provide",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 33
    },
    "id": "tejuu_75"
  },
  {
    "text": "_key, quantity_on_hand, reorder_level, unit_cost)\n\nDimension Tables:\n- dim_patient (patient_key, patient_id, age_group, gender, state, insurance_type, risk_score)\n- dim_provider (provider_key, provider_id, provider_name, specialty, network_status, region)\n- dim_facility (facility_key, facility_id, facility_name, facility_type, city, state, bed_count)\n- dim_product (product_key, product_id, product_name, category, subcategory, manufacturer)\n- dim_date (date_key, date, year, quarter, month, week, day_of_week, is_holiday, fiscal_period)\n- dim_insurance (insurance_key, payer_name, plan_type, coverage_level)\n```\n\n**Relationship Best Practices I Follow:**\n```\n1. Always use surrogate keys (integers) for relationships\n2. One-to-many relationships from dimension to fact\n3. Avoid bi-directional relationships (use DAX instead)\n4. Hide foreign keys from report view\n5. Mark date table properly\n6. Use role-playing dimensions sparingly\n```\n\n### Row-Level Security (RLS) Implementation\n**Tejuu's RLS St",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 34
    },
    "id": "tejuu_76"
  },
  {
    "text": "tionships (use DAX instead)\n4. Hide foreign keys from report view\n5. Mark date table properly\n6. Use role-playing dimensions sparingly\n```\n\n### Row-Level Security (RLS) Implementation\n**Tejuu's RLS Strategy:**\nSo implementing RLS was critical at Central Bank and Stryker because different users needed access to different data. Here's how I approached it:\n\n**Role-Based Access Example:**\n```dax\n-- Regional Manager Role\n[Region] = USERPRINCIPALNAME()\n\n-- State-Level Access\n[State] IN {\n    LOOKUPVALUE(\n        user_access[State],\n        user_access[Email],\n        USERPRINCIPALNAME()\n    )\n}\n\n-- Dynamic Security with Bridge Table\nVAR UserEmail = USERPRINCIPALNAME()\nVAR UserRegions = \n    CALCULATETABLE(\n        VALUES(security_bridge[Region]),\n        security_bridge[UserEmail] = UserEmail\n    )\nRETURN\n    [Region] IN UserRegions\n\n-- Manager Hierarchy (see your team and below)\nVAR CurrentUser = USERPRINCIPALNAME()\nVAR UserLevel = \n    LOOKUPVALUE(\n        dim_employee[Level],\n        dim_",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 35
    },
    "id": "tejuu_77"
  },
  {
    "text": "  )\nRETURN\n    [Region] IN UserRegions\n\n-- Manager Hierarchy (see your team and below)\nVAR CurrentUser = USERPRINCIPALNAME()\nVAR UserLevel = \n    LOOKUPVALUE(\n        dim_employee[Level],\n        dim_employee[Email],\n        CurrentUser\n    )\nRETURN\n    dim_employee[Manager_Email] = CurrentUser ||\n    dim_employee[Email] = CurrentUser\n```\n\n**Testing RLS:**\n```\n1. Create test users in Azure AD\n2. Use \"View as\" feature in Power BI Desktop\n3. Test each role with sample queries\n4. Document expected vs actual results\n5. Validate with actual users before production\n```\n\n## Advanced DAX Patterns\n\n### Time Intelligence Mastery\n**Tejuu's Most-Used Time Intelligence Patterns:**\n\n```dax\n-- Year-to-Date with Fiscal Calendar\nYTD Sales = \nCALCULATE(\n    SUM(fact_sales[Amount]),\n    DATESYTD(\n        dim_date[Date],\n        \"06/30\"  -- Fiscal year ends June 30\n    )\n)\n\n-- Prior Year Same Period\nPY Sales = \nCALCULATE(\n    [Total Sales],\n    SAMEPERIODLASTYEAR(dim_date[Date])\n)\n\n-- Year-over-Year Growt",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 36
    },
    "id": "tejuu_78"
  },
  {
    "text": "_date[Date],\n        \"06/30\"  -- Fiscal year ends June 30\n    )\n)\n\n-- Prior Year Same Period\nPY Sales = \nCALCULATE(\n    [Total Sales],\n    SAMEPERIODLASTYEAR(dim_date[Date])\n)\n\n-- Year-over-Year Growth %\nYoY Growth % = \nVAR CurrentPeriod = [Total Sales]\nVAR PriorPeriod = [PY Sales]\nRETURN\n    DIVIDE(\n        CurrentPeriod - PriorPeriod,\n        PriorPeriod,\n        0\n    )\n\n-- Month-to-Date vs Prior Month-to-Date\nMTD vs PMTD = \nVAR MTD = \n    CALCULATE(\n        [Total Sales],\n        DATESMTD(dim_date[Date])\n    )\nVAR PMTD = \n    CALCULATE(\n        [Total Sales],\n        DATESMTD(\n            DATEADD(dim_date[Date], -1, MONTH)\n        )\n    )\nRETURN\n    MTD - PMTD\n\n-- Rolling 12 Months\nRolling 12M Sales = \nCALCULATE(\n    [Total Sales],\n    DATESINPERIOD(\n        dim_date[Date],\n        LASTDATE(dim_date[Date]),\n        -12,\n        MONTH\n    )\n)\n\n-- Same Period Last Year with Date Table\nSPLY Sales = \nCALCULATE(\n    [Total Sales],\n    DATEADD(dim_date[Date], -1, YEAR)\n)\n```\n\n### Complex",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 37
    },
    "id": "tejuu_79"
  },
  {
    "text": "TDATE(dim_date[Date]),\n        -12,\n        MONTH\n    )\n)\n\n-- Same Period Last Year with Date Table\nSPLY Sales = \nCALCULATE(\n    [Total Sales],\n    DATEADD(dim_date[Date], -1, YEAR)\n)\n```\n\n### Complex Calculated Columns vs Measures\n**When I Use Each:**\n\n**Calculated Columns (Stored in Model):**\n```dax\n-- Age Group (Calculated Column)\nAge Group = \nSWITCH(\n    TRUE(),\n    dim_patient[Age] < 18, \"Pediatric\",\n    dim_patient[Age] < 65, \"Adult\",\n    \"Senior\"\n)\n\n-- Days Since Last Visit (Calculated Column)\nDays Since Last Visit = \nDATEDIFF(\n    dim_patient[Last_Visit_Date],\n    TODAY(),\n    DAY\n)\n\n-- Full Name (Calculated Column)\nFull Name = \ndim_patient[First_Name] & \" \" & dim_patient[Last_Name]\n```\n\n**Measures (Calculated at Query Time):**\n```dax\n-- Total Claims (Measure)\nTotal Claims = \nSUM(fact_claims[Claim_Amount])\n\n-- Average Claim Amount (Measure)\nAvg Claim Amount = \nAVERAGE(fact_claims[Claim_Amount])\n\n-- Distinct Patient Count (Measure)\nPatient Count = \nDISTINCTCOUNT(fact_claims[Pati",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 38
    },
    "id": "tejuu_80"
  },
  {
    "text": "act_claims[Claim_Amount])\n\n-- Average Claim Amount (Measure)\nAvg Claim Amount = \nAVERAGE(fact_claims[Claim_Amount])\n\n-- Distinct Patient Count (Measure)\nPatient Count = \nDISTINCTCOUNT(fact_claims[Patient_Key])\n\n-- Claim Approval Rate (Measure)\nApproval Rate = \nDIVIDE(\n    CALCULATE([Total Claims], fact_claims[Status] = \"Approved\"),\n    [Total Claims],\n    0\n)\n```\n\n### Advanced DAX for Healthcare Analytics\n**Tejuu's Medicaid Analytics at Stryker:**\n\n```dax\n-- Patient Risk Score\nPatient Risk Score = \nVAR PatientVisits = \n    CALCULATE(\n        COUNTROWS(fact_patient_visits),\n        ALLEXCEPT(fact_patient_visits, fact_patient_visits[Patient_Key])\n    )\nVAR ChronicConditions = \n    CALCULATE(\n        DISTINCTCOUNT(fact_diagnosis[Condition_Code]),\n        fact_diagnosis[Is_Chronic] = TRUE()\n    )\nVAR HighCostFlag = \n    IF(\n        CALCULATE(\n            SUM(fact_claims[Paid_Amount]),\n            ALLEXCEPT(fact_claims, fact_claims[Patient_Key])\n        ) > 50000,\n        1,\n        0\n    )",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 39
    },
    "id": "tejuu_81"
  },
  {
    "text": "  )\nVAR HighCostFlag = \n    IF(\n        CALCULATE(\n            SUM(fact_claims[Paid_Amount]),\n            ALLEXCEPT(fact_claims, fact_claims[Patient_Key])\n        ) > 50000,\n        1,\n        0\n    )\nRETURN\n    (PatientVisits * 0.3) + \n    (ChronicConditions * 0.5) + \n    (HighCostFlag * 0.2)\n\n-- Readmission Rate (30-day)\n30-Day Readmission Rate = \nVAR ReadmissionWindow = 30\nVAR Readmissions = \n    CALCULATE(\n        COUNTROWS(fact_patient_visits),\n        FILTER(\n            fact_patient_visits,\n            VAR CurrentVisitDate = fact_patient_visits[Visit_Date]\n            VAR PatientKey = fact_patient_visits[Patient_Key]\n            VAR PriorVisit = \n                CALCULATE(\n                    MAX(fact_patient_visits[Visit_Date]),\n                    ALLEXCEPT(fact_patient_visits, fact_patient_visits[Patient_Key]),\n                    fact_patient_visits[Visit_Date] < CurrentVisitDate\n                )\n            RETURN\n                DATEDIFF(PriorVisit, CurrentVisitDate, DAY)",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 40
    },
    "id": "tejuu_82"
  },
  {
    "text": "ct_patient_visits[Patient_Key]),\n                    fact_patient_visits[Visit_Date] < CurrentVisitDate\n                )\n            RETURN\n                DATEDIFF(PriorVisit, CurrentVisitDate, DAY) <= ReadmissionWindow\n        )\n    )\nVAR TotalVisits = COUNTROWS(fact_patient_visits)\nRETURN\n    DIVIDE(Readmissions, TotalVisits, 0)\n\n-- Cost per Member per Month (PMPM)\nPMPM = \nVAR TotalCost = SUM(fact_claims[Paid_Amount])\nVAR MemberMonths = \n    SUMX(\n        VALUES(dim_patient[Patient_Key]),\n        CALCULATE(\n            DISTINCTCOUNT(dim_date[Month_Year])\n        )\n    )\nRETURN\n    DIVIDE(TotalCost, MemberMonths, 0)\n\n-- Length of Stay Analysis\nAvg Length of Stay = \nAVERAGEX(\n    fact_patient_visits,\n    DATEDIFF(\n        fact_patient_visits[Admission_Date],\n        fact_patient_visits[Discharge_Date],\n        DAY\n    )\n)\n\n-- Case Mix Index\nCase Mix Index = \nAVERAGEX(\n    VALUES(fact_patient_visits[Visit_ID]),\n    RELATED(dim_drg[Relative_Weight])\n)\n```\n\n### Performance Optimization ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 41
    },
    "id": "tejuu_83"
  },
  {
    "text": "ischarge_Date],\n        DAY\n    )\n)\n\n-- Case Mix Index\nCase Mix Index = \nAVERAGEX(\n    VALUES(fact_patient_visits[Visit_ID]),\n    RELATED(dim_drg[Relative_Weight])\n)\n```\n\n### Performance Optimization Techniques\n**My DAX Optimization Rules:**\n\n```dax\n-- BEFORE: Slow (multiple context transitions)\nSlow Measure = \nSUMX(\n    fact_sales,\n    fact_sales[Quantity] * \n    RELATED(dim_product[Unit_Price])\n)\n\n-- AFTER: Fast (calculated column or import both fields)\nFast Measure = \nSUM(fact_sales[Line_Total])\n\n-- BEFORE: Slow (row context in measure)\nSlow Customer Count = \nCOUNTROWS(\n    FILTER(\n        dim_customer,\n        CALCULATE(SUM(fact_sales[Amount])) > 1000\n    )\n)\n\n-- AFTER: Fast (use CALCULATETABLE)\nFast Customer Count = \nCOUNTROWS(\n    CALCULATETABLE(\n        VALUES(dim_customer[Customer_Key]),\n        fact_sales[Amount] > 1000\n    )\n)\n\n-- Use Variables to Avoid Recalculation\nOptimized Measure = \nVAR TotalSales = SUM(fact_sales[Amount])\nVAR TotalCost = SUM(fact_sales[Cost])\nVAR Margin",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 42
    },
    "id": "tejuu_84"
  },
  {
    "text": "Key]),\n        fact_sales[Amount] > 1000\n    )\n)\n\n-- Use Variables to Avoid Recalculation\nOptimized Measure = \nVAR TotalSales = SUM(fact_sales[Amount])\nVAR TotalCost = SUM(fact_sales[Cost])\nVAR Margin = TotalSales - TotalCost\nVAR MarginPct = DIVIDE(Margin, TotalSales, 0)\nRETURN\n    IF(MarginPct > 0.3, \"High\", \"Low\")\n```\n\n## Power BI Performance Tuning\n\n### Query Performance Optimization\n**Tejuu's Optimization Checklist:**\n\n```\n1. Data Model Optimization:\n   - Remove unused columns and tables\n   - Use integer keys instead of text\n   - Disable auto date/time hierarchy\n   - Use appropriate data types\n   - Avoid calculated columns when possible\n\n2. DAX Optimization:\n   - Use variables to store intermediate results\n   - Avoid row context in measures\n   - Use TREATAS instead of FILTER when possible\n   - Minimize use of CALCULATE\n   - Use SELECTEDVALUE instead of VALUES + HASONEVALUE\n\n3. Visual Optimization:\n   - Limit visuals per page (max 10-15)\n   - Use bookmarks for different views\n   - A",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 43
    },
    "id": "tejuu_85"
  },
  {
    "text": "ble\n   - Minimize use of CALCULATE\n   - Use SELECTEDVALUE instead of VALUES + HASONEVALUE\n\n3. Visual Optimization:\n   - Limit visuals per page (max 10-15)\n   - Use bookmarks for different views\n   - Avoid high-cardinality fields in visuals\n   - Use aggregated data when possible\n   - Implement drill-through instead of showing all detail\n\n4. Data Refresh Optimization:\n   - Use incremental refresh for large tables\n   - Partition large tables by date\n   - Schedule refreshes during off-peak hours\n   - Use dataflows for common transformations\n```\n\n### Incremental Refresh Configuration\n**My Implementation at Stryker:**\n\n```\n1. Create RangeStart and RangeEnd parameters (Date/Time type)\n2. Filter source data:\n   = Table.SelectRows(Source, \n       each [Date] >= RangeStart and [Date] < RangeEnd)\n3. Configure incremental refresh policy:\n   - Archive data: 5 years\n   - Incremental refresh: 10 days\n   - Detect data changes: Yes\n   - Only refresh complete days: Yes\n```\n\n## Advanced Visualizations\n\n#",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 44
    },
    "id": "tejuu_86"
  },
  {
    "text": "Configure incremental refresh policy:\n   - Archive data: 5 years\n   - Incremental refresh: 10 days\n   - Detect data changes: Yes\n   - Only refresh complete days: Yes\n```\n\n## Advanced Visualizations\n\n### Custom Visuals I Use\n**Tejuu's Favorite Custom Visuals:**\n\n```\n1. Zebra BI Tables - For variance analysis\n2. Drill Down Donut PRO - For hierarchical data\n3. Chiclet Slicer - For better filter experience\n4. Power KPI - For executive dashboards\n5. Sankey Diagram - For flow analysis\n6. Box and Whisker - For distribution analysis\n```\n\n### Conditional Formatting Patterns\n**My Advanced Formatting Techniques:**\n\n```dax\n-- Color Scale Based on Performance\nColor Measure = \nVAR Performance = [Actual] / [Target]\nRETURN\n    SWITCH(\n        TRUE(),\n        Performance >= 1.1, \"#00B050\",  -- Green\n        Performance >= 0.9, \"#FFC000\",  -- Yellow\n        \"#FF0000\"                        -- Red\n    )\n\n-- Icon Set Based on Trend\nTrend Icon = \nVAR CurrentMonth = [Sales This Month]\nVAR PriorMonth = [Sale",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 45
    },
    "id": "tejuu_87"
  },
  {
    "text": "  Performance >= 0.9, \"#FFC000\",  -- Yellow\n        \"#FF0000\"                        -- Red\n    )\n\n-- Icon Set Based on Trend\nTrend Icon = \nVAR CurrentMonth = [Sales This Month]\nVAR PriorMonth = [Sales Last Month]\nVAR Change = DIVIDE(CurrentMonth - PriorMonth, PriorMonth, 0)\nRETURN\n    SWITCH(\n        TRUE(),\n        Change > 0.05, \"▲\",\n        Change < -0.05, \"▼\",\n        \"■\"\n    )\n\n-- Data Bars with Conditional Logic\nData Bar Value = \nVAR MaxValue = \n    CALCULATE(\n        MAX(fact_sales[Amount]),\n        ALLSELECTED(dim_product[Product_Name])\n    )\nVAR CurrentValue = SUM(fact_sales[Amount])\nRETURN\n    DIVIDE(CurrentValue, MaxValue, 0)\n```\n\n## Power BI Deployment & ALM\n\n### Application Lifecycle Management\n**Tejuu's ALM Strategy:**\n\n```\nDevelopment → Test → Production Pipeline\n\n1. Development Workspace:\n   - Individual developer workspaces\n   - Connect to Dev database\n   - Rapid iteration and testing\n   - Git integration for version control\n\n2. Test Workspace:\n   - Shared team worksp",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 46
    },
    "id": "tejuu_88"
  },
  {
    "text": "pment Workspace:\n   - Individual developer workspaces\n   - Connect to Dev database\n   - Rapid iteration and testing\n   - Git integration for version control\n\n2. Test Workspace:\n   - Shared team workspace\n   - Connect to Test database\n   - UAT with business users\n   - Performance testing\n\n3. Production Workspace:\n   - Premium capacity\n   - Connect to Prod database\n   - Scheduled refreshes\n   - Monitoring and alerts\n\nDeployment Process:\n1. Export .pbix from Dev\n2. Update data source to Test\n3. Publish to Test workspace\n4. UAT sign-off\n5. Export from Test\n6. Update data source to Prod\n7. Publish to Prod workspace\n8. Configure refresh schedule\n9. Set up monitoring\n```\n\n### Version Control with Git\n**My Git Workflow:**\n\n```bash\n# Save Power BI file as PBIX\n# Use external tools to extract PBIT\n\n# Git workflow\ngit checkout -b feature/new-dashboard\n# Make changes in Power BI\ngit add .\ngit commit -m \"Add sales performance dashboard\"\ngit push origin feature/new-dashboard\n# Create pull request\n# ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 47
    },
    "id": "tejuu_89"
  },
  {
    "text": "# Git workflow\ngit checkout -b feature/new-dashboard\n# Make changes in Power BI\ngit add .\ngit commit -m \"Add sales performance dashboard\"\ngit push origin feature/new-dashboard\n# Create pull request\n# Code review\n# Merge to main\n```\n\n### Deployment Pipelines\n**Configuration:**\n\n```\nPipeline Stages:\n1. Development\n   - Auto-deploy on commit to dev branch\n   - Run data quality tests\n   - Generate documentation\n\n2. Test\n   - Deploy on PR approval\n   - Run regression tests\n   - UAT validation\n\n3. Production\n   - Manual approval required\n   - Deploy during maintenance window\n   - Backup before deployment\n   - Rollback plan ready\n```\n\n## Power BI Best Practices\n\n### Naming Conventions\n**My Standard Naming:**\n\n```\nTables:\n- fact_sales, fact_claims, fact_inventory\n- dim_customer, dim_product, dim_date\n\nMeasures:\n- Total Sales, Avg Order Value, YTD Revenue\n- Use spaces, title case\n- Group related measures in folders\n\nColumns:\n- Use snake_case or PascalCase consistently\n- Prefix calculated column",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 48
    },
    "id": "tejuu_90"
  },
  {
    "text": "ate\n\nMeasures:\n- Total Sales, Avg Order Value, YTD Revenue\n- Use spaces, title case\n- Group related measures in folders\n\nColumns:\n- Use snake_case or PascalCase consistently\n- Prefix calculated columns with \"calc_\"\n- Hide foreign keys\n\nRelationships:\n- Always name relationships descriptively\n- Document complex relationships\n```\n\n### Documentation Standards\n**What I Document:**\n\n```markdown\n# Dashboard Documentation\n\n## Purpose\nExecutive sales performance dashboard for regional managers\n\n## Data Sources\n- Azure SQL: sales_db.fact_sales\n- Refresh: Daily at 6 AM EST\n- Latency: T-1 day\n\n## Key Metrics\n- Total Sales: SUM of fact_sales[Amount]\n- YoY Growth: (Current - Prior Year) / Prior Year\n- Target Attainment: Actual / Target\n\n## Filters\n- Date Range: Last 12 months default\n- Region: Multi-select\n- Product Category: Single select\n\n## Row-Level Security\n- Regional managers see their region only\n- VPs see all regions\n- Executives see all data\n\n## Known Issues\n- Data quality issue with Regio",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 49
    },
    "id": "tejuu_91"
  },
  {
    "text": "lect\n- Product Category: Single select\n\n## Row-Level Security\n- Regional managers see their region only\n- VPs see all regions\n- Executives see all data\n\n## Known Issues\n- Data quality issue with Region \"Unknown\" - investigating\n- Slow performance on Product Detail page - optimization in progress\n\n## Change Log\n- 2024-01-15: Added YoY comparison\n- 2024-01-10: Initial release\n```\n\n## Interview Talking Points\n\n### Technical Achievements\n- Built 50+ Power BI dashboards serving 200+ users\n- Implemented RLS for 10+ different security roles\n- Optimized DAX reducing query time from 30s to 3s\n- Designed star schemas for healthcare and sales analytics\n- Achieved 99% dashboard uptime in production\n\n### Problem-Solving Examples\n**Performance Issue:**\n\"At Stryker, we had this dashboard that was taking 30 seconds to load. I analyzed the DAX and found we were using calculated columns in measures, causing multiple context transitions. I refactored the measures to use variables and proper filter contex",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 50
    },
    "id": "tejuu_92"
  },
  {
    "text": "30 seconds to load. I analyzed the DAX and found we were using calculated columns in measures, causing multiple context transitions. I refactored the measures to use variables and proper filter context, and got the load time down to 3 seconds.\"\n\n**Complex Business Logic:**\n\"At Central Bank, finance needed a complex calculation for regulatory reporting. It involved multiple date ranges, conditional logic, and aggregations across different grain levels. I broke it down into smaller measures, used variables for readability, and documented each step. The final measure was accurate and performant.\"\n\n### Tools & Technologies\n- **Power BI**: Desktop, Service, Premium, Embedded, Report Server\n- **DAX**: Advanced patterns, optimization, debugging\n- **Power Query**: M language, custom functions, data transformation\n- **Azure**: Synapse, Data Factory, SQL Database\n- **Version Control**: Git, Azure DevOps\n- **ALM**: Deployment pipelines, testing, documentation\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 51
    },
    "id": "tejuu_93"
  },
  {
    "text": "a transformation\n- **Azure**: Synapse, Data Factory, SQL Database\n- **Version Control**: Git, Azure DevOps\n- **ALM**: Deployment pipelines, testing, documentation\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_intelligence/power_bi_advanced.md",
      "file_name": "power_bi_advanced.md",
      "chunk_index": 52
    },
    "id": "tejuu_94"
  },
  {
    "text": "---\ntags: [business-analyst, requirements-gathering, stakeholder-management, documentation, process-improvement]\npersona: ba\n---\n\n# Business Analyst Core Skills & Tejuu's Experience\n\n## Requirements Gathering and Analysis\n\n### Stakeholder Interviews and Workshops\n**Tejuu's Approach:**\nSo in my role as a Business Analyst, I've conducted hundreds of stakeholder interviews and workshops. What I found works best is starting with open-ended questions to understand the business problem before diving into solutions.\n\nFor example, at my previous company, we had a project where the sales team wanted a new reporting dashboard. Instead of just asking what reports they needed, I spent time understanding their daily workflows, pain points, and what decisions they were trying to make. This helped me uncover that they actually needed real-time alerts, not just another dashboard.\n\n**My Interview Technique:**\n```\nPreparation Phase:\n- Research the stakeholder's role and department\n- Review existing docu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 53
    },
    "id": "tejuu_95"
  },
  {
    "text": "over that they actually needed real-time alerts, not just another dashboard.\n\n**My Interview Technique:**\n```\nPreparation Phase:\n- Research the stakeholder's role and department\n- Review existing documentation and processes\n- Prepare open-ended questions\n- Set clear meeting objectives\n\nDuring the Interview:\n- Start with business context: \"Walk me through your typical day\"\n- Ask \"why\" questions: \"Why is this important to you?\"\n- Use active listening and take detailed notes\n- Clarify assumptions: \"So what I'm hearing is...\"\n- Ask about pain points: \"What frustrates you most about the current process?\"\n\nFollow-up:\n- Send meeting notes within 24 hours\n- Confirm understanding of key points\n- Identify any gaps or follow-up questions\n```\n\n### Requirements Documentation\n**Tejuu's Documentation Standards:**\nI've learned that good documentation is the foundation of successful projects. Here's how I structure my requirements documents:\n\n**Business Requirements Document (BRD) Template:**\n```\n1. Ex",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 54
    },
    "id": "tejuu_96"
  },
  {
    "text": "ndards:**\nI've learned that good documentation is the foundation of successful projects. Here's how I structure my requirements documents:\n\n**Business Requirements Document (BRD) Template:**\n```\n1. Executive Summary\n   - Project overview (2-3 paragraphs)\n   - Business objectives\n   - Expected benefits and ROI\n\n2. Current State Analysis\n   - As-Is process flows\n   - Pain points and challenges\n   - Current system limitations\n\n3. Future State Vision\n   - To-Be process flows\n   - Expected improvements\n   - Success metrics\n\n4. Functional Requirements\n   - User stories with acceptance criteria\n   - Business rules\n   - Data requirements\n\n5. Non-Functional Requirements\n   - Performance requirements\n   - Security and compliance\n   - Usability standards\n\n6. Assumptions and Constraints\n7. Dependencies and Risks\n8. Approval and Sign-off\n```\n\n**User Story Format I Use:**\n```\nAs a [role], I want to [action] so that [benefit].\n\nAcceptance Criteria:\n- Given [context]\n- When [action]\n- Then [expected r",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 55
    },
    "id": "tejuu_97"
  },
  {
    "text": "s and Risks\n8. Approval and Sign-off\n```\n\n**User Story Format I Use:**\n```\nAs a [role], I want to [action] so that [benefit].\n\nAcceptance Criteria:\n- Given [context]\n- When [action]\n- Then [expected result]\n\nExample:\nAs a Sales Manager, I want to view real-time sales performance by region \nso that I can identify underperforming areas and take immediate action.\n\nAcceptance Criteria:\n- Given I am logged into the dashboard\n- When I select a specific date range\n- Then I see sales data grouped by region with YoY comparison\n- And I can drill down to individual sales rep performance\n```\n\n## Process Mapping and Improvement\n\n### Creating Process Flow Diagrams\n**Tejuu's Process Mapping Experience:**\nOne of my key strengths is visualizing complex business processes. I use tools like Visio, Lucidchart, and Miro to create process maps that everyone can understand.\n\n**My Process Mapping Approach:**\n```\nStep 1: Identify Process Boundaries\n- Where does the process start?\n- Where does it end?\n- What tr",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 56
    },
    "id": "tejuu_98"
  },
  {
    "text": ", and Miro to create process maps that everyone can understand.\n\n**My Process Mapping Approach:**\n```\nStep 1: Identify Process Boundaries\n- Where does the process start?\n- Where does it end?\n- What triggers the process?\n\nStep 2: Map Current State (As-Is)\n- Interview process owners\n- Shadow users performing the process\n- Document every step, decision point, and handoff\n- Identify pain points and bottlenecks\n\nStep 3: Analyze and Identify Improvements\n- Look for redundant steps\n- Identify manual tasks that could be automated\n- Find bottlenecks causing delays\n- Spot quality issues or error-prone steps\n\nStep 4: Design Future State (To-Be)\n- Eliminate unnecessary steps\n- Automate manual processes\n- Streamline handoffs\n- Add quality checks where needed\n\nStep 5: Calculate Impact\n- Time savings\n- Cost reduction\n- Error reduction\n- Customer satisfaction improvement\n```\n\n**Real Example - Order Processing Improvement:**\n```\nAs-Is Process (12 steps, 45 minutes average):\n1. Customer calls to place o",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 57
    },
    "id": "tejuu_99"
  },
  {
    "text": "ost reduction\n- Error reduction\n- Customer satisfaction improvement\n```\n\n**Real Example - Order Processing Improvement:**\n```\nAs-Is Process (12 steps, 45 minutes average):\n1. Customer calls to place order\n2. Sales rep manually enters order in Excel\n3. Sales rep emails order to inventory team\n4. Inventory team checks stock manually\n5. Inventory team emails back availability\n6. Sales rep calls customer to confirm\n7. Sales rep enters order in ERP system\n8. Finance team manually creates invoice\n9. Finance emails invoice to customer\n10. Warehouse receives printed order form\n11. Warehouse picks and packs order\n12. Shipping updates tracking manually\n\nTo-Be Process (6 steps, 15 minutes average):\n1. Customer places order through web portal\n2. System automatically checks inventory\n3. System confirms order and sends confirmation email\n4. ERP system auto-generates invoice\n5. Warehouse receives digital order notification\n6. System auto-updates tracking information\n\nImpact:\n- 67% time reduction (45 ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 58
    },
    "id": "tejuu_100"
  },
  {
    "text": " order and sends confirmation email\n4. ERP system auto-generates invoice\n5. Warehouse receives digital order notification\n6. System auto-updates tracking information\n\nImpact:\n- 67% time reduction (45 min → 15 min)\n- 90% fewer errors (manual entry eliminated)\n- $50K annual cost savings\n- Improved customer satisfaction (instant confirmation)\n```\n\n## Data Analysis and Reporting\n\n### SQL for Business Analysis\n**Tejuu's SQL Skills:**\nAs a Business Analyst, I use SQL daily to extract insights from databases. Here are some common queries I run:\n\n**Sales Performance Analysis:**\n```sql\n-- Monthly sales trend with year-over-year comparison\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    SUM(order_amount) as current_year_sales,\n    LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date)) as previous_year_sales,\n    ROUND(((SUM(order_amount) - LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) \n           / LAG(SUM(order_amount), 12) OVER (ORDER",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 59
    },
    "id": "tejuu_101"
  },
  {
    "text": "', order_date)) as previous_year_sales,\n    ROUND(((SUM(order_amount) - LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) \n           / LAG(SUM(order_amount), 12) OVER (ORDER BY DATE_TRUNC('month', order_date))) * 100, 2) as yoy_growth_pct\nFROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '2 years'\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month;\n```\n\n**Customer Segmentation:**\n```sql\n-- RFM analysis for customer segmentation\nWITH customer_metrics AS (\n    SELECT \n        customer_id,\n        MAX(order_date) as last_order_date,\n        COUNT(DISTINCT order_id) as order_count,\n        SUM(order_amount) as total_spent\n    FROM orders\n    WHERE order_date >= CURRENT_DATE - INTERVAL '1 year'\n    GROUP BY customer_id\n),\nrfm_scores AS (\n    SELECT \n        customer_id,\n        DATEDIFF(CURRENT_DATE, last_order_date) as recency_days,\n        order_count as frequency,\n        total_spent as monetary,\n        NTILE(5) OVER (ORDER BY DATEDIFF(CURRENT_DATE,",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 60
    },
    "id": "tejuu_102"
  },
  {
    "text": " customer_id,\n        DATEDIFF(CURRENT_DATE, last_order_date) as recency_days,\n        order_count as frequency,\n        total_spent as monetary,\n        NTILE(5) OVER (ORDER BY DATEDIFF(CURRENT_DATE, last_order_date) DESC) as recency_score,\n        NTILE(5) OVER (ORDER BY order_count) as frequency_score,\n        NTILE(5) OVER (ORDER BY total_spent) as monetary_score\n    FROM customer_metrics\n)\nSELECT \n    customer_id,\n    recency_days,\n    frequency,\n    monetary,\n    CASE \n        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'\n        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n        WHEN recency_score >= 4 AND frequency_score <= 2 THEN 'Promising'\n        WHEN recency_score <= 2 AND frequency_score >= 3 THEN 'At Risk'\n        WHEN recency_score <= 2 AND frequency_score <= 2 THEN 'Lost'\n        ELSE 'Others'\n    END as customer_segment\nFROM rfm_scores;\n```\n\n**Product Performance:**\n```sql\n-- Top performing products with contribution an",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 61
    },
    "id": "tejuu_103"
  },
  {
    "text": "_score <= 2 AND frequency_score <= 2 THEN 'Lost'\n        ELSE 'Others'\n    END as customer_segment\nFROM rfm_scores;\n```\n\n**Product Performance:**\n```sql\n-- Top performing products with contribution analysis\nSELECT \n    product_name,\n    SUM(quantity_sold) as total_quantity,\n    SUM(revenue) as total_revenue,\n    ROUND(SUM(revenue) * 100.0 / SUM(SUM(revenue)) OVER (), 2) as revenue_contribution_pct,\n    ROUND(SUM(SUM(revenue)) OVER (ORDER BY SUM(revenue) DESC) * 100.0 / SUM(SUM(revenue)) OVER (), 2) as cumulative_pct\nFROM sales\nWHERE sale_date >= CURRENT_DATE - INTERVAL '90 days'\nGROUP BY product_name\nORDER BY total_revenue DESC\nLIMIT 20;\n```\n\n## Stakeholder Management\n\n### Communication Strategies\n**Tejuu's Stakeholder Management:**\nManaging different stakeholders with different priorities is one of the biggest challenges. Here's what I've learned:\n\n**Stakeholder Analysis Matrix:**\n```\nHigh Power, High Interest (Manage Closely):\n- C-level executives\n- Project sponsors\n- Key decision ma",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 62
    },
    "id": "tejuu_104"
  },
  {
    "text": " is one of the biggest challenges. Here's what I've learned:\n\n**Stakeholder Analysis Matrix:**\n```\nHigh Power, High Interest (Manage Closely):\n- C-level executives\n- Project sponsors\n- Key decision makers\nStrategy: Regular updates, involve in key decisions, address concerns immediately\n\nHigh Power, Low Interest (Keep Satisfied):\n- Department heads not directly involved\n- Compliance/legal teams\nStrategy: Periodic updates, keep informed of major changes\n\nLow Power, High Interest (Keep Informed):\n- End users\n- Team members\nStrategy: Regular communication, gather feedback, involve in testing\n\nLow Power, Low Interest (Monitor):\n- Peripheral stakeholders\nStrategy: Minimal communication, inform of major milestones\n```\n\n**My Communication Approach:**\n```\nFor Executives:\n- Focus on business impact and ROI\n- Use high-level dashboards and metrics\n- Keep updates brief (1-page summaries)\n- Highlight risks and mitigation plans\n\nFor Technical Teams:\n- Provide detailed requirements and specifications\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 63
    },
    "id": "tejuu_105"
  },
  {
    "text": "and ROI\n- Use high-level dashboards and metrics\n- Keep updates brief (1-page summaries)\n- Highlight risks and mitigation plans\n\nFor Technical Teams:\n- Provide detailed requirements and specifications\n- Use technical language appropriately\n- Be available for clarification\n- Respect their expertise and input\n\nFor End Users:\n- Use simple, non-technical language\n- Show how changes benefit them\n- Involve them in testing and feedback\n- Address their concerns empathetically\n\nFor Project Managers:\n- Provide clear timelines and dependencies\n- Flag blockers early\n- Keep requirements documentation updated\n- Attend all status meetings prepared\n```\n\n## Gap Analysis and Feasibility Studies\n\n### Conducting Gap Analysis\n**Tejuu's Gap Analysis Framework:**\n```\n1. Define Current State\n   - Document existing capabilities\n   - Identify current performance metrics\n   - List available resources\n\n2. Define Desired Future State\n   - Document required capabilities\n   - Set target performance metrics\n   - Ident",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 64
    },
    "id": "tejuu_106"
  },
  {
    "text": " capabilities\n   - Identify current performance metrics\n   - List available resources\n\n2. Define Desired Future State\n   - Document required capabilities\n   - Set target performance metrics\n   - Identify needed resources\n\n3. Identify Gaps\n   - Capability gaps\n   - Performance gaps\n   - Resource gaps\n   - Knowledge/skill gaps\n\n4. Prioritize Gaps\n   - Impact on business objectives\n   - Urgency and dependencies\n   - Cost and effort to close\n\n5. Develop Action Plan\n   - Quick wins (high impact, low effort)\n   - Strategic initiatives (high impact, high effort)\n   - Fill-ins (low impact, low effort)\n   - Reconsider (low impact, high effort)\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- Requirements gathering and documentation\n- Process mapping and improvement\n- SQL and data analysis\n- Stakeholder management\n- Gap analysis and feasibility studies\n\n### Tools & Technologies:\n- **Documentation**: Confluence, SharePoint, MS Word\n- **Process Mapping**: Visio, Lucidchart, Miro, Draw.io\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 65
    },
    "id": "tejuu_107"
  },
  {
    "text": "- Stakeholder management\n- Gap analysis and feasibility studies\n\n### Tools & Technologies:\n- **Documentation**: Confluence, SharePoint, MS Word\n- **Process Mapping**: Visio, Lucidchart, Miro, Draw.io\n- **Project Management**: Jira, Azure DevOps, Asana\n- **Data Analysis**: SQL, Excel, Power BI, Tableau\n- **Collaboration**: MS Teams, Slack, Zoom\n\n### Achievements:\n- Reduced process time by 67% through process optimization\n- Saved $50K annually by identifying automation opportunities\n- Improved customer satisfaction by 25% through better requirements\n- Successfully delivered 15+ projects on time and within budget\n- Managed stakeholder groups of 20+ people across multiple departments\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/ba_core_skills.md",
      "file_name": "ba_core_skills.md",
      "chunk_index": 66
    },
    "id": "tejuu_108"
  },
  {
    "text": "---\ntags: [analytics-engineer, dbt, data-modeling, sql, data-warehouse, metrics]\npersona: ae\n---\n\n# Analytics Engineer & Tejuu's Experience\n\n## Modern Data Stack and Analytics Engineering\n\n### What is an Analytics Engineer?\n**Tejuu's Role:**\nSo as an Analytics Engineer, I'm kind of the bridge between data engineering and business intelligence. I take raw data from data warehouses and transform it into clean, well-modeled datasets that business users can easily understand and use for analysis.\n\nThe key difference from a traditional BI developer is that I write code (mostly SQL and Python) to build data models, use version control, write tests, and follow software engineering best practices. It's like being a data engineer but focused on the analytics layer rather than data pipelines.\n\n**My Responsibilities:**\n```\n1. Data Modeling\n   - Design dimensional models (star/snowflake schemas)\n   - Create reusable data marts\n   - Define metrics and KPIs\n   - Document data lineage\n\n2. Transformat",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 67
    },
    "id": "tejuu_109"
  },
  {
    "text": "Responsibilities:**\n```\n1. Data Modeling\n   - Design dimensional models (star/snowflake schemas)\n   - Create reusable data marts\n   - Define metrics and KPIs\n   - Document data lineage\n\n2. Transformation Development\n   - Write SQL transformations\n   - Build dbt models\n   - Create staging, intermediate, and mart layers\n   - Implement business logic\n\n3. Data Quality & Testing\n   - Write data quality tests\n   - Implement validation rules\n   - Monitor data freshness\n   - Alert on anomalies\n\n4. Documentation & Governance\n   - Document data models\n   - Define metrics definitions\n   - Create data dictionaries\n   - Maintain data catalog\n\n5. Collaboration\n   - Work with data engineers on data pipelines\n   - Partner with analysts on requirements\n   - Support BI developers with data models\n   - Train business users on data usage\n```\n\n## dbt (Data Build Tool)\n\n### Building Transformation Pipelines with dbt\n**Tejuu's dbt Experience:**\nI use dbt daily to transform raw data into analytics-ready datas",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 68
    },
    "id": "tejuu_110"
  },
  {
    "text": " business users on data usage\n```\n\n## dbt (Data Build Tool)\n\n### Building Transformation Pipelines with dbt\n**Tejuu's dbt Experience:**\nI use dbt daily to transform raw data into analytics-ready datasets. It's become the standard tool for analytics engineering, and I love how it brings software engineering practices to data transformation.\n\n**My dbt Project Structure:**\n```\nmy_dbt_project/\n├── models/\n│   ├── staging/          # Raw data cleaning\n│   │   ├── stg_customers.sql\n│   │   ├── stg_orders.sql\n│   │   └── stg_products.sql\n│   ├── intermediate/     # Business logic\n│   │   ├── int_customer_orders.sql\n│   │   └── int_order_items.sql\n│   └── marts/           # Final analytics tables\n│       ├── finance/\n│       │   ├── fct_revenue.sql\n│       │   └── dim_customers.sql\n│       └── marketing/\n│           ├── fct_campaigns.sql\n│           └── dim_customer_segments.sql\n├── tests/\n│   └── assert_positive_revenue.sql\n├── macros/\n│   └── generate_schema_name.sql\n└── dbt_project.yml\n```\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 69
    },
    "id": "tejuu_111"
  },
  {
    "text": "keting/\n│           ├── fct_campaigns.sql\n│           └── dim_customer_segments.sql\n├── tests/\n│   └── assert_positive_revenue.sql\n├── macros/\n│   └── generate_schema_name.sql\n└── dbt_project.yml\n```\n\n**Staging Layer Example:**\n```sql\n-- models/staging/stg_customers.sql\n{{\n    config(\n        materialized='view',\n        schema='staging'\n    )\n}}\n\nWITH source AS (\n    SELECT * FROM {{ source('raw', 'customers') }}\n),\n\nrenamed AS (\n    SELECT\n        customer_id,\n        TRIM(LOWER(email)) AS email,\n        TRIM(first_name) AS first_name,\n        TRIM(last_name) AS last_name,\n        CONCAT(first_name, ' ', last_name) AS full_name,\n        phone,\n        address,\n        city,\n        state,\n        zip_code,\n        country,\n        created_at,\n        updated_at\n    FROM source\n    WHERE customer_id IS NOT NULL\n)\n\nSELECT * FROM renamed\n```\n\n**Intermediate Layer Example:**\n```sql\n-- models/intermediate/int_customer_orders.sql\n{{\n    config(\n        materialized='ephemeral'\n    )\n}}\n\nWI",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 70
    },
    "id": "tejuu_112"
  },
  {
    "text": "customer_id IS NOT NULL\n)\n\nSELECT * FROM renamed\n```\n\n**Intermediate Layer Example:**\n```sql\n-- models/intermediate/int_customer_orders.sql\n{{\n    config(\n        materialized='ephemeral'\n    )\n}}\n\nWITH customers AS (\n    SELECT * FROM {{ ref('stg_customers') }}\n),\n\norders AS (\n    SELECT * FROM {{ ref('stg_orders') }}\n),\n\ncustomer_orders AS (\n    SELECT\n        c.customer_id,\n        c.full_name,\n        c.email,\n        c.city,\n        c.state,\n        o.order_id,\n        o.order_date,\n        o.order_amount,\n        o.order_status\n    FROM customers c\n    LEFT JOIN orders o ON c.customer_id = o.customer_id\n)\n\nSELECT * FROM customer_orders\n```\n\n**Marts Layer Example:**\n```sql\n-- models/marts/finance/fct_revenue.sql\n{{\n    config(\n        materialized='incremental',\n        unique_key='order_id',\n        schema='finance'\n    )\n}}\n\nWITH customer_orders AS (\n    SELECT * FROM {{ ref('int_customer_orders') }}\n),\n\nrevenue_metrics AS (\n    SELECT\n        order_id,\n        customer_id,\n    ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 71
    },
    "id": "tejuu_113"
  },
  {
    "text": "der_id',\n        schema='finance'\n    )\n}}\n\nWITH customer_orders AS (\n    SELECT * FROM {{ ref('int_customer_orders') }}\n),\n\nrevenue_metrics AS (\n    SELECT\n        order_id,\n        customer_id,\n        order_date,\n        order_amount,\n        \n        -- Time dimensions\n        DATE_TRUNC('month', order_date) AS order_month,\n        DATE_TRUNC('quarter', order_date) AS order_quarter,\n        DATE_TRUNC('year', order_date) AS order_year,\n        \n        -- Customer metrics\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date\n        ) AS customer_order_number,\n        \n        -- Revenue metrics\n        SUM(order_amount) OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) AS customer_lifetime_value,\n        \n        CURRENT_TIMESTAMP AS dbt_updated_at\n        \n    FROM customer_orders\n    WHERE order_status = 'completed'\n    \n    {% if is_incre",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 72
    },
    "id": "tejuu_114"
  },
  {
    "text": "ND CURRENT ROW\n        ) AS customer_lifetime_value,\n        \n        CURRENT_TIMESTAMP AS dbt_updated_at\n        \n    FROM customer_orders\n    WHERE order_status = 'completed'\n    \n    {% if is_incremental() %}\n        AND order_date > (SELECT MAX(order_date) FROM {{ this }})\n    {% endif %}\n)\n\nSELECT * FROM revenue_metrics\n```\n\n### dbt Tests and Data Quality\n**My Testing Strategy:**\n```yaml\n# models/staging/schema.yml\nversion: 2\n\nmodels:\n  - name: stg_customers\n    description: Cleaned and standardized customer data\n    columns:\n      - name: customer_id\n        description: Primary key for customers\n        tests:\n          - unique\n          - not_null\n      \n      - name: email\n        description: Customer email address\n        tests:\n          - not_null\n          - unique\n          - dbt_utils.email\n      \n      - name: created_at\n        description: Timestamp when customer was created\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 73
    },
    "id": "tejuu_115"
  },
  {
    "text": " - dbt_utils.email\n      \n      - name: created_at\n        description: Timestamp when customer was created\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \">= '2020-01-01'\"\n\n  - name: stg_orders\n    description: Cleaned and standardized order data\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n      \n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('stg_customers')\n              field: customer_id\n      \n      - name: order_amount\n        tests:\n          - not_null\n          - dbt_utils.expression_is_true:\n              expression: \"> 0\"\n```\n\n**Custom Tests:**\n```sql\n-- tests/assert_revenue_positive.sql\nSELECT\n    order_id,\n    order_amount\nFROM {{ ref('fct_revenue') }}\nWHERE order_amount <= 0\n```\n\n### dbt Macros for Reusability\n**My Commonly Used Macros:**\n```sql\n-- macros/cents_to_dollars.sql\n{% macro cents_to_dollars(colu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 74
    },
    "id": "tejuu_116"
  },
  {
    "text": "order_amount\nFROM {{ ref('fct_revenue') }}\nWHERE order_amount <= 0\n```\n\n### dbt Macros for Reusability\n**My Commonly Used Macros:**\n```sql\n-- macros/cents_to_dollars.sql\n{% macro cents_to_dollars(column_name, precision=2) %}\n    ROUND({{ column_name }} / 100.0, {{ precision }})\n{% endmacro %}\n\n-- Usage in model:\nSELECT\n    order_id,\n    {{ cents_to_dollars('amount_cents') }} AS amount_dollars\nFROM orders\n\n-- macros/generate_surrogate_key.sql\n{% macro generate_surrogate_key(field_list) %}\n    MD5(CONCAT(\n        {% for field in field_list %}\n            COALESCE(CAST({{ field }} AS VARCHAR), '')\n            {% if not loop.last %} || '|' || {% endif %}\n        {% endfor %}\n    ))\n{% endmacro %}\n\n-- Usage:\nSELECT\n    {{ generate_surrogate_key(['customer_id', 'order_date']) }} AS order_key,\n    *\nFROM orders\n\n-- macros/pivot_metric.sql\n{% macro pivot_metric(column, values, metric, agg_function='SUM') %}\n    {% for value in values %}\n        {{ agg_function }}(\n            CASE WHEN {{ colu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 75
    },
    "id": "tejuu_117"
  },
  {
    "text": "  *\nFROM orders\n\n-- macros/pivot_metric.sql\n{% macro pivot_metric(column, values, metric, agg_function='SUM') %}\n    {% for value in values %}\n        {{ agg_function }}(\n            CASE WHEN {{ column }} = '{{ value }}' \n            THEN {{ metric }} \n            ELSE 0 END\n        ) AS {{ value | replace(' ', '_') | lower }}\n        {% if not loop.last %}, {% endif %}\n    {% endfor %}\n{% endmacro %}\n```\n\n## Metrics Layer and Semantic Modeling\n\n### Defining Business Metrics\n**Tejuu's Metrics Framework:**\n```yaml\n# metrics/revenue_metrics.yml\nversion: 2\n\nmetrics:\n  - name: total_revenue\n    label: Total Revenue\n    model: ref('fct_revenue')\n    description: Sum of all completed order amounts\n    \n    calculation_method: sum\n    expression: order_amount\n    \n    timestamp: order_date\n    time_grains: [day, week, month, quarter, year]\n    \n    dimensions:\n      - customer_segment\n      - product_category\n      - region\n    \n    filters:\n      - field: order_status\n        operator: '='\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 76
    },
    "id": "tejuu_118"
  },
  {
    "text": "ime_grains: [day, week, month, quarter, year]\n    \n    dimensions:\n      - customer_segment\n      - product_category\n      - region\n    \n    filters:\n      - field: order_status\n        operator: '='\n        value: \"'completed'\"\n\n  - name: average_order_value\n    label: Average Order Value\n    model: ref('fct_revenue')\n    description: Average amount per order\n    \n    calculation_method: average\n    expression: order_amount\n    \n    timestamp: order_date\n    time_grains: [day, week, month, quarter, year]\n\n  - name: customer_lifetime_value\n    label: Customer Lifetime Value\n    model: ref('dim_customers')\n    description: Total revenue per customer\n    \n    calculation_method: sum\n    expression: total_spent\n    \n    dimensions:\n      - customer_segment\n      - acquisition_channel\n```\n\n## SQL Optimization for Analytics\n\n### Query Performance Optimization\n**Tejuu's SQL Optimization Techniques:**\n```sql\n-- BEFORE: Slow query with subqueries\nSELECT \n    c.customer_id,\n    c.customer_name,",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 77
    },
    "id": "tejuu_119"
  },
  {
    "text": "SQL Optimization for Analytics\n\n### Query Performance Optimization\n**Tejuu's SQL Optimization Techniques:**\n```sql\n-- BEFORE: Slow query with subqueries\nSELECT \n    c.customer_id,\n    c.customer_name,\n    (SELECT COUNT(*) FROM orders o WHERE o.customer_id = c.customer_id) as order_count,\n    (SELECT SUM(amount) FROM orders o WHERE o.customer_id = c.customer_id) as total_spent\nFROM customers c;\n\n-- AFTER: Optimized with JOIN\nSELECT \n    c.customer_id,\n    c.customer_name,\n    COUNT(o.order_id) as order_count,\n    SUM(o.amount) as total_spent\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.customer_name;\n\n-- BEFORE: Multiple passes over data\nSELECT customer_id, SUM(amount) FROM orders GROUP BY customer_id;\nSELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id;\nSELECT customer_id, AVG(amount) FROM orders GROUP BY customer_id;\n\n-- AFTER: Single pass with multiple aggregations\nSELECT \n    customer_id,\n    SUM(amount) as total_amount,\n  ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 78
    },
    "id": "tejuu_120"
  },
  {
    "text": "s GROUP BY customer_id;\nSELECT customer_id, AVG(amount) FROM orders GROUP BY customer_id;\n\n-- AFTER: Single pass with multiple aggregations\nSELECT \n    customer_id,\n    SUM(amount) as total_amount,\n    COUNT(*) as order_count,\n    AVG(amount) as avg_amount\nFROM orders\nGROUP BY customer_id;\n\n-- Using CTEs for readability and performance\nWITH monthly_sales AS (\n    SELECT \n        DATE_TRUNC('month', order_date) as month,\n        SUM(amount) as total_sales\n    FROM orders\n    WHERE order_date >= '2023-01-01'\n    GROUP BY DATE_TRUNC('month', order_date)\n),\nsales_with_growth AS (\n    SELECT \n        month,\n        total_sales,\n        LAG(total_sales) OVER (ORDER BY month) as prev_month_sales,\n        (total_sales - LAG(total_sales) OVER (ORDER BY month)) / \n        LAG(total_sales) OVER (ORDER BY month) * 100 as growth_rate\n    FROM monthly_sales\n)\nSELECT * FROM sales_with_growth\nWHERE growth_rate IS NOT NULL\nORDER BY month;\n```\n\n## Data Warehouse Modeling\n\n### Dimensional Modeling\n**Teju",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 79
    },
    "id": "tejuu_121"
  },
  {
    "text": "R BY month) * 100 as growth_rate\n    FROM monthly_sales\n)\nSELECT * FROM sales_with_growth\nWHERE growth_rate IS NOT NULL\nORDER BY month;\n```\n\n## Data Warehouse Modeling\n\n### Dimensional Modeling\n**Tejuu's Dimensional Model Design:**\n```sql\n-- Dimension Table: Customers\nCREATE TABLE dim_customers (\n    customer_key INT PRIMARY KEY,  -- Surrogate key\n    customer_id VARCHAR(50),        -- Natural key\n    customer_name VARCHAR(200),\n    email VARCHAR(200),\n    segment VARCHAR(50),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    country VARCHAR(50),\n    first_order_date DATE,\n    customer_since_days INT,\n    is_active BOOLEAN,\n    effective_date DATE,            -- SCD Type 2\n    expiration_date DATE,           -- SCD Type 2\n    is_current BOOLEAN,             -- SCD Type 2\n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n);\n\n-- Fact Table: Sales\nCREATE TABLE fct_sales (\n    sale_key BIGINT PRIMARY KEY,\n    order_id VARCHAR(50),\n    customer_key INT REFERENCES dim_customers(customer_key",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 80
    },
    "id": "tejuu_122"
  },
  {
    "text": "ESTAMP,\n    updated_at TIMESTAMP\n);\n\n-- Fact Table: Sales\nCREATE TABLE fct_sales (\n    sale_key BIGINT PRIMARY KEY,\n    order_id VARCHAR(50),\n    customer_key INT REFERENCES dim_customers(customer_key),\n    product_key INT REFERENCES dim_products(product_key),\n    date_key INT REFERENCES dim_date(date_key),\n    employee_key INT REFERENCES dim_employees(employee_key),\n    \n    -- Measures\n    quantity INT,\n    unit_price DECIMAL(10,2),\n    discount_amount DECIMAL(10,2),\n    tax_amount DECIMAL(10,2),\n    total_amount DECIMAL(10,2),\n    cost_amount DECIMAL(10,2),\n    profit_amount DECIMAL(10,2),\n    \n    -- Degenerate dimensions\n    order_number VARCHAR(50),\n    invoice_number VARCHAR(50),\n    \n    created_at TIMESTAMP\n);\n\n-- Date Dimension (very important for analytics)\nCREATE TABLE dim_date (\n    date_key INT PRIMARY KEY,\n    date DATE,\n    day_of_week VARCHAR(10),\n    day_of_month INT,\n    day_of_year INT,\n    week_of_year INT,\n    month_number INT,\n    month_name VARCHAR(10),\n    quar",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 81
    },
    "id": "tejuu_123"
  },
  {
    "text": " (\n    date_key INT PRIMARY KEY,\n    date DATE,\n    day_of_week VARCHAR(10),\n    day_of_month INT,\n    day_of_year INT,\n    week_of_year INT,\n    month_number INT,\n    month_name VARCHAR(10),\n    quarter INT,\n    year INT,\n    is_weekend BOOLEAN,\n    is_holiday BOOLEAN,\n    holiday_name VARCHAR(100),\n    fiscal_year INT,\n    fiscal_quarter INT,\n    fiscal_month INT\n);\n```\n\n## Data Governance and Documentation\n\n### Data Catalog and Lineage\n**My Documentation Approach:**\n```markdown\n# Data Model: Customer Analytics\n\n## Purpose\nProvides a comprehensive view of customer behavior, segmentation, and lifetime value for marketing and sales teams.\n\n## Source Systems\n- Salesforce CRM (customer data)\n- Shopify (order data)\n- Google Analytics (web behavior)\n\n## Refresh Schedule\n- Staging: Every 1 hour\n- Intermediate: Every 2 hours\n- Marts: Daily at 6 AM EST\n\n## Data Lineage\n```\nraw.salesforce.accounts \n  → staging.stg_customers \n    → intermediate.int_customer_orders \n      → marts.dim_customers\n`",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 82
    },
    "id": "tejuu_124"
  },
  {
    "text": "\n- Intermediate: Every 2 hours\n- Marts: Daily at 6 AM EST\n\n## Data Lineage\n```\nraw.salesforce.accounts \n  → staging.stg_customers \n    → intermediate.int_customer_orders \n      → marts.dim_customers\n```\n\n## Key Metrics\n| Metric | Definition | Formula |\n|--------|-----------|---------|\n| Customer Lifetime Value | Total revenue from customer | SUM(order_amount) |\n| Average Order Value | Average amount per order | SUM(order_amount) / COUNT(orders) |\n| Purchase Frequency | Orders per customer per year | COUNT(orders) / customer_tenure_years |\n\n## Data Quality Rules\n- customer_id must be unique and not null\n- email must be valid format\n- total_spent must be >= 0\n- first_order_date must be <= last_order_date\n\n## Access Control\n- Finance team: Full access\n- Marketing team: Read access (excluding PII)\n- Sales team: Read access (own region only)\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- dbt development and testing\n- SQL optimization and performance tuning\n- Dimensional modeling ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 83
    },
    "id": "tejuu_125"
  },
  {
    "text": "PII)\n- Sales team: Read access (own region only)\n```\n\n## Interview Talking Points\n\n### Technical Skills:\n- dbt development and testing\n- SQL optimization and performance tuning\n- Dimensional modeling (star/snowflake schemas)\n- Data quality and validation\n- Metrics definition and semantic modeling\n- Version control (Git) for data transformations\n\n### Tools & Technologies:\n- **Transformation**: dbt, SQL, Python\n- **Data Warehouses**: Snowflake, BigQuery, Redshift\n- **Version Control**: Git, GitHub, GitLab\n- **Orchestration**: Airflow, Dagster, Prefect\n- **BI Tools**: Power BI, Tableau, Looker\n- **Data Quality**: Great Expectations, dbt tests\n\n### Achievements:\n- Built 100+ dbt models serving 50+ analysts\n- Reduced data transformation time from 6 hours to 1 hour\n- Implemented data quality tests catching 95% of issues\n- Created reusable metrics layer used across 20+ dashboards\n- Documented 200+ data models improving data discovery\n- Trained 15+ analysts on dbt and SQL best practices\n\n### P",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 84
    },
    "id": "tejuu_126"
  },
  {
    "text": "ts catching 95% of issues\n- Created reusable metrics layer used across 20+ dashboards\n- Documented 200+ data models improving data discovery\n- Trained 15+ analysts on dbt and SQL best practices\n\n### Project Examples:\n- Customer 360 data mart (unified customer view)\n- Financial reporting data warehouse (P&L, balance sheet)\n- Marketing analytics platform (campaign performance, attribution)\n- Product analytics (usage, engagement, retention)\n- Operational metrics (efficiency, productivity, quality)\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/analytics_engineer.md",
      "file_name": "analytics_engineer.md",
      "chunk_index": 85
    },
    "id": "tejuu_127"
  },
  {
    "text": "---\ntags: [sql, advanced-sql, window-functions, cte, optimization, healthcare, medicaid]\npersona: tejuu\n---\n\n# Advanced SQL for Business Analysis - Tejuu's Expertise\n\n## Healthcare & Medicaid Analytics SQL\n\n### Claims Analysis at Stryker\n**Tejuu's Real-World SQL for Medicaid:**\n\n```sql\n-- Medicaid Claims Analysis with Payment Patterns\nWITH claim_summary AS (\n    SELECT \n        claim_id,\n        patient_id,\n        provider_id,\n        claim_date,\n        service_date,\n        claim_amount,\n        paid_amount,\n        denied_amount,\n        claim_status,\n        denial_reason,\n        payer_name,\n        DATEDIFF(day, service_date, claim_date) as days_to_submit,\n        DATEDIFF(day, claim_date, payment_date) as days_to_payment\n    FROM fact_claims\n    WHERE payer_type = 'Medicaid'\n        AND claim_date >= '2023-01-01'\n),\npayment_metrics AS (\n    SELECT \n        provider_id,\n        COUNT(*) as total_claims,\n        SUM(claim_amount) as total_billed,\n        SUM(paid_amount) as total",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 86
    },
    "id": "tejuu_128"
  },
  {
    "text": "  AND claim_date >= '2023-01-01'\n),\npayment_metrics AS (\n    SELECT \n        provider_id,\n        COUNT(*) as total_claims,\n        SUM(claim_amount) as total_billed,\n        SUM(paid_amount) as total_paid,\n        SUM(denied_amount) as total_denied,\n        AVG(paid_amount) as avg_payment,\n        AVG(days_to_payment) as avg_days_to_payment,\n        SUM(CASE WHEN claim_status = 'Denied' THEN 1 ELSE 0 END) as denied_count,\n        SUM(CASE WHEN claim_status = 'Paid' THEN 1 ELSE 0 END) as paid_count\n    FROM claim_summary\n    GROUP BY provider_id\n)\nSELECT \n    p.provider_id,\n    p.provider_name,\n    p.provider_specialty,\n    pm.total_claims,\n    pm.total_billed,\n    pm.total_paid,\n    ROUND((pm.total_paid / NULLIF(pm.total_billed, 0)) * 100, 2) as payment_rate_pct,\n    pm.denied_count,\n    ROUND((pm.denied_count * 100.0 / pm.total_claims), 2) as denial_rate_pct,\n    pm.avg_payment,\n    pm.avg_days_to_payment,\n    CASE \n        WHEN pm.avg_days_to_payment <= 30 THEN 'Fast'\n        WHEN p",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 87
    },
    "id": "tejuu_129"
  },
  {
    "text": "ROUND((pm.denied_count * 100.0 / pm.total_claims), 2) as denial_rate_pct,\n    pm.avg_payment,\n    pm.avg_days_to_payment,\n    CASE \n        WHEN pm.avg_days_to_payment <= 30 THEN 'Fast'\n        WHEN pm.avg_days_to_payment <= 60 THEN 'Average'\n        ELSE 'Slow'\n    END as payment_speed_category\nFROM payment_metrics pm\nJOIN dim_provider p ON pm.provider_id = p.provider_id\nWHERE pm.total_claims >= 10  -- Minimum volume threshold\nORDER BY pm.total_billed DESC;\n```\n\n### Patient Readmission Analysis\n**30-Day Readmission Tracking:**\n\n```sql\n-- Identify 30-Day Hospital Readmissions\nWITH patient_visits AS (\n    SELECT \n        patient_id,\n        visit_id,\n        admission_date,\n        discharge_date,\n        diagnosis_code,\n        drg_code,\n        facility_id,\n        LEAD(admission_date) OVER (\n            PARTITION BY patient_id \n            ORDER BY admission_date\n        ) as next_admission_date,\n        LEAD(visit_id) OVER (\n            PARTITION BY patient_id \n            ORDER BY ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 88
    },
    "id": "tejuu_130"
  },
  {
    "text": "ER (\n            PARTITION BY patient_id \n            ORDER BY admission_date\n        ) as next_admission_date,\n        LEAD(visit_id) OVER (\n            PARTITION BY patient_id \n            ORDER BY admission_date\n        ) as next_visit_id\n    FROM fact_patient_visits\n    WHERE discharge_date IS NOT NULL\n        AND admission_date >= '2023-01-01'\n),\nreadmissions AS (\n    SELECT \n        patient_id,\n        visit_id as initial_visit_id,\n        next_visit_id as readmission_visit_id,\n        admission_date as initial_admission,\n        discharge_date as initial_discharge,\n        next_admission_date as readmission_date,\n        DATEDIFF(day, discharge_date, next_admission_date) as days_between_visits,\n        diagnosis_code as initial_diagnosis,\n        drg_code as initial_drg\n    FROM patient_visits\n    WHERE next_admission_date IS NOT NULL\n        AND DATEDIFF(day, discharge_date, next_admission_date) <= 30\n)\nSELECT \n    r.patient_id,\n    p.patient_name,\n    p.age,\n    p.gender,\n    ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 89
    },
    "id": "tejuu_131"
  },
  {
    "text": "ient_visits\n    WHERE next_admission_date IS NOT NULL\n        AND DATEDIFF(day, discharge_date, next_admission_date) <= 30\n)\nSELECT \n    r.patient_id,\n    p.patient_name,\n    p.age,\n    p.gender,\n    r.initial_visit_id,\n    r.readmission_visit_id,\n    r.initial_admission,\n    r.initial_discharge,\n    r.readmission_date,\n    r.days_between_visits,\n    r.initial_diagnosis,\n    d.diagnosis_description,\n    r.initial_drg,\n    drg.drg_description,\n    f.facility_name,\n    CASE \n        WHEN r.days_between_visits <= 7 THEN 'Very High Risk'\n        WHEN r.days_between_visits <= 14 THEN 'High Risk'\n        WHEN r.days_between_visits <= 21 THEN 'Medium Risk'\n        ELSE 'Low Risk'\n    END as readmission_risk_category\nFROM readmissions r\nJOIN dim_patient p ON r.patient_id = p.patient_id\nJOIN dim_diagnosis d ON r.initial_diagnosis = d.diagnosis_code\nJOIN dim_drg drg ON r.initial_drg = drg.drg_code\nJOIN fact_patient_visits v ON r.initial_visit_id = v.visit_id\nJOIN dim_facility f ON v.facility_id ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 90
    },
    "id": "tejuu_132"
  },
  {
    "text": "agnosis d ON r.initial_diagnosis = d.diagnosis_code\nJOIN dim_drg drg ON r.initial_drg = drg.drg_code\nJOIN fact_patient_visits v ON r.initial_visit_id = v.visit_id\nJOIN dim_facility f ON v.facility_id = f.facility_id\nORDER BY r.days_between_visits, r.readmission_date DESC;\n\n-- Readmission Rate by Diagnosis\nSELECT \n    d.diagnosis_code,\n    d.diagnosis_description,\n    COUNT(DISTINCT v.visit_id) as total_discharges,\n    COUNT(DISTINCT r.readmission_visit_id) as readmissions,\n    ROUND((COUNT(DISTINCT r.readmission_visit_id) * 100.0 / \n           COUNT(DISTINCT v.visit_id)), 2) as readmission_rate_pct,\n    AVG(r.days_between_visits) as avg_days_to_readmission\nFROM fact_patient_visits v\nJOIN dim_diagnosis d ON v.primary_diagnosis = d.diagnosis_code\nLEFT JOIN readmissions r ON v.visit_id = r.initial_visit_id\nWHERE v.discharge_date >= '2023-01-01'\nGROUP BY d.diagnosis_code, d.diagnosis_description\nHAVING COUNT(DISTINCT v.visit_id) >= 20  -- Minimum volume\nORDER BY readmission_rate_pct DESC;\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 91
    },
    "id": "tejuu_133"
  },
  {
    "text": "itial_visit_id\nWHERE v.discharge_date >= '2023-01-01'\nGROUP BY d.diagnosis_code, d.diagnosis_description\nHAVING COUNT(DISTINCT v.visit_id) >= 20  -- Minimum volume\nORDER BY readmission_rate_pct DESC;\n```\n\n## Advanced Window Functions for Business Analytics\n\n### Sales Performance Analysis\n**Tejuu's Sales Analytics at CVS Health:**\n\n```sql\n-- Comprehensive Sales Performance with Rankings and Trends\nWITH daily_sales AS (\n    SELECT \n        sale_date,\n        product_id,\n        store_id,\n        region,\n        SUM(quantity) as units_sold,\n        SUM(sales_amount) as daily_sales,\n        SUM(cost_amount) as daily_cost\n    FROM fact_sales\n    WHERE sale_date >= '2023-01-01'\n    GROUP BY sale_date, product_id, store_id, region\n),\nsales_with_metrics AS (\n    SELECT \n        sale_date,\n        product_id,\n        store_id,\n        region,\n        daily_sales,\n        daily_cost,\n        daily_sales - daily_cost as daily_profit,\n        \n        -- Running totals\n        SUM(daily_sales) OVE",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 92
    },
    "id": "tejuu_134"
  },
  {
    "text": "product_id,\n        store_id,\n        region,\n        daily_sales,\n        daily_cost,\n        daily_sales - daily_cost as daily_profit,\n        \n        -- Running totals\n        SUM(daily_sales) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) as running_total_sales,\n        \n        -- Moving averages\n        AVG(daily_sales) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n        ) as moving_avg_7day,\n        \n        AVG(daily_sales) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n        ) as moving_avg_30day,\n        \n        -- Year-over-year comparison\n        LAG(daily_sales, 365) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n        ) as sales_last_year,",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 93
    },
    "id": "tejuu_135"
  },
  {
    "text": "vg_30day,\n        \n        -- Year-over-year comparison\n        LAG(daily_sales, 365) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n        ) as sales_last_year,\n        \n        -- Month-over-month comparison\n        LAG(daily_sales, 30) OVER (\n            PARTITION BY product_id, store_id \n            ORDER BY sale_date\n        ) as sales_last_month,\n        \n        -- Rankings\n        RANK() OVER (\n            PARTITION BY sale_date, region \n            ORDER BY daily_sales DESC\n        ) as daily_rank_in_region,\n        \n        DENSE_RANK() OVER (\n            PARTITION BY DATE_TRUNC('month', sale_date), region \n            ORDER BY SUM(daily_sales) OVER (\n                PARTITION BY product_id, store_id, DATE_TRUNC('month', sale_date)\n            ) DESC\n        ) as monthly_rank_in_region,\n        \n        -- Percentiles\n        PERCENT_RANK() OVER (\n            PARTITION BY region \n            ORDER BY daily_sales\n        ) as sales_percen",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 94
    },
    "id": "tejuu_136"
  },
  {
    "text": "   ) DESC\n        ) as monthly_rank_in_region,\n        \n        -- Percentiles\n        PERCENT_RANK() OVER (\n            PARTITION BY region \n            ORDER BY daily_sales\n        ) as sales_percentile\n        \n    FROM daily_sales\n)\nSELECT \n    sale_date,\n    p.product_name,\n    s.store_name,\n    sm.region,\n    sm.daily_sales,\n    sm.daily_profit,\n    sm.running_total_sales,\n    sm.moving_avg_7day,\n    sm.moving_avg_30day,\n    sm.sales_last_year,\n    ROUND(((sm.daily_sales - sm.sales_last_year) / \n           NULLIF(sm.sales_last_year, 0)) * 100, 2) as yoy_growth_pct,\n    sm.daily_rank_in_region,\n    sm.monthly_rank_in_region,\n    ROUND(sm.sales_percentile * 100, 2) as sales_percentile_rank,\n    CASE \n        WHEN sm.daily_sales > sm.moving_avg_30day * 1.2 THEN 'High Performance'\n        WHEN sm.daily_sales < sm.moving_avg_30day * 0.8 THEN 'Low Performance'\n        ELSE 'Normal'\n    END as performance_category\nFROM sales_with_metrics sm\nJOIN dim_product p ON sm.product_id = p.produc",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 95
    },
    "id": "tejuu_137"
  },
  {
    "text": "  WHEN sm.daily_sales < sm.moving_avg_30day * 0.8 THEN 'Low Performance'\n        ELSE 'Normal'\n    END as performance_category\nFROM sales_with_metrics sm\nJOIN dim_product p ON sm.product_id = p.product_id\nJOIN dim_store s ON sm.store_id = s.store_id\nWHERE sale_date >= '2024-01-01'\nORDER BY sale_date DESC, sm.daily_sales DESC;\n```\n\n### Customer Cohort Analysis\n**Retention and Lifetime Value:**\n\n```sql\n-- Customer Cohort Analysis with Retention Rates\nWITH first_purchase AS (\n    SELECT \n        customer_id,\n        MIN(order_date) as cohort_month,\n        MIN(order_id) as first_order_id\n    FROM fact_orders\n    GROUP BY customer_id\n),\ncustomer_orders AS (\n    SELECT \n        o.customer_id,\n        o.order_date,\n        o.order_amount,\n        fp.cohort_month,\n        DATE_TRUNC('month', o.order_date) as order_month,\n        DATEDIFF(month, fp.cohort_month, DATE_TRUNC('month', o.order_date)) as months_since_first\n    FROM fact_orders o\n    JOIN first_purchase fp ON o.customer_id = fp.cust",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 96
    },
    "id": "tejuu_138"
  },
  {
    "text": "der_date) as order_month,\n        DATEDIFF(month, fp.cohort_month, DATE_TRUNC('month', o.order_date)) as months_since_first\n    FROM fact_orders o\n    JOIN first_purchase fp ON o.customer_id = fp.customer_id\n    WHERE o.order_date >= '2023-01-01'\n),\ncohort_data AS (\n    SELECT \n        cohort_month,\n        months_since_first,\n        COUNT(DISTINCT customer_id) as customers,\n        SUM(order_amount) as total_revenue,\n        AVG(order_amount) as avg_order_value\n    FROM customer_orders\n    GROUP BY cohort_month, months_since_first\n),\ncohort_size AS (\n    SELECT \n        cohort_month,\n        COUNT(DISTINCT customer_id) as cohort_size\n    FROM first_purchase\n    GROUP BY cohort_month\n)\nSELECT \n    cd.cohort_month,\n    cs.cohort_size,\n    cd.months_since_first,\n    cd.customers,\n    ROUND((cd.customers * 100.0 / cs.cohort_size), 2) as retention_rate_pct,\n    cd.total_revenue,\n    cd.avg_order_value,\n    SUM(cd.total_revenue) OVER (\n        PARTITION BY cd.cohort_month \n        ORDER BY",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 97
    },
    "id": "tejuu_139"
  },
  {
    "text": "(cd.customers * 100.0 / cs.cohort_size), 2) as retention_rate_pct,\n    cd.total_revenue,\n    cd.avg_order_value,\n    SUM(cd.total_revenue) OVER (\n        PARTITION BY cd.cohort_month \n        ORDER BY cd.months_since_first\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) as cumulative_revenue,\n    ROUND(SUM(cd.total_revenue) OVER (\n        PARTITION BY cd.cohort_month \n        ORDER BY cd.months_since_first\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) / cs.cohort_size, 2) as lifetime_value\nFROM cohort_data cd\nJOIN cohort_size cs ON cd.cohort_month = cs.cohort_month\nORDER BY cd.cohort_month, cd.months_since_first;\n```\n\n## Complex CTEs and Recursive Queries\n\n### Organizational Hierarchy Analysis\n**Tejuu's HR Analytics:**\n\n```sql\n-- Recursive CTE for Employee Hierarchy\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: Top-level managers (no manager)\n    SELECT \n        employee_id,\n        employee_name,\n        manager_id,\n        job_title,\n       ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 98
    },
    "id": "tejuu_140"
  },
  {
    "text": "ierarchy\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: Top-level managers (no manager)\n    SELECT \n        employee_id,\n        employee_name,\n        manager_id,\n        job_title,\n        department,\n        salary,\n        1 as level,\n        CAST(employee_name AS VARCHAR(1000)) as hierarchy_path,\n        CAST(employee_id AS VARCHAR(1000)) as id_path\n    FROM dim_employee\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: Employees with managers\n    SELECT \n        e.employee_id,\n        e.employee_name,\n        e.manager_id,\n        e.job_title,\n        e.department,\n        e.salary,\n        eh.level + 1,\n        CAST(eh.hierarchy_path || ' > ' || e.employee_name AS VARCHAR(1000)),\n        CAST(eh.id_path || '>' || e.employee_id AS VARCHAR(1000))\n    FROM dim_employee e\n    INNER JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n    WHERE eh.level < 10  -- Prevent infinite recursion\n),\nteam_metrics AS (\n    SELECT \n        eh.employee_",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 99
    },
    "id": "tejuu_141"
  },
  {
    "text": "  FROM dim_employee e\n    INNER JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n    WHERE eh.level < 10  -- Prevent infinite recursion\n),\nteam_metrics AS (\n    SELECT \n        eh.employee_id,\n        eh.employee_name,\n        eh.level,\n        COUNT(DISTINCT e2.employee_id) as direct_reports,\n        COUNT(DISTINCT e3.employee_id) as total_team_size,\n        AVG(e3.salary) as avg_team_salary,\n        SUM(e3.salary) as total_team_cost\n    FROM employee_hierarchy eh\n    LEFT JOIN dim_employee e2 ON eh.employee_id = e2.manager_id\n    LEFT JOIN employee_hierarchy e3 ON e3.id_path LIKE eh.id_path || '%'\n    GROUP BY eh.employee_id, eh.employee_name, eh.level\n)\nSELECT \n    eh.employee_id,\n    eh.employee_name,\n    eh.job_title,\n    eh.department,\n    eh.salary,\n    eh.level,\n    eh.hierarchy_path,\n    tm.direct_reports,\n    tm.total_team_size,\n    tm.avg_team_salary,\n    tm.total_team_cost,\n    ROUND((eh.salary / NULLIF(tm.avg_team_salary, 0)), 2) as salary_vs_team_avg\nFROM emplo",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 100
    },
    "id": "tejuu_142"
  },
  {
    "text": "ierarchy_path,\n    tm.direct_reports,\n    tm.total_team_size,\n    tm.avg_team_salary,\n    tm.total_team_cost,\n    ROUND((eh.salary / NULLIF(tm.avg_team_salary, 0)), 2) as salary_vs_team_avg\nFROM employee_hierarchy eh\nJOIN team_metrics tm ON eh.employee_id = tm.employee_id\nORDER BY eh.level, eh.employee_name;\n```\n\n### Product Category Hierarchy\n**Inventory Analysis with Rollups:**\n\n```sql\n-- Product Category Hierarchy with Sales Rollup\nWITH RECURSIVE category_hierarchy AS (\n    SELECT \n        category_id,\n        category_name,\n        parent_category_id,\n        1 as level,\n        CAST(category_name AS VARCHAR(500)) as category_path\n    FROM dim_category\n    WHERE parent_category_id IS NULL\n    \n    UNION ALL\n    \n    SELECT \n        c.category_id,\n        c.category_name,\n        c.parent_category_id,\n        ch.level + 1,\n        CAST(ch.category_path || ' > ' || c.category_name AS VARCHAR(500))\n    FROM dim_category c\n    INNER JOIN category_hierarchy ch ON c.parent_category_id = ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 101
    },
    "id": "tejuu_143"
  },
  {
    "text": "nt_category_id,\n        ch.level + 1,\n        CAST(ch.category_path || ' > ' || c.category_name AS VARCHAR(500))\n    FROM dim_category c\n    INNER JOIN category_hierarchy ch ON c.parent_category_id = ch.category_id\n),\nsales_by_category AS (\n    SELECT \n        p.category_id,\n        SUM(s.sales_amount) as total_sales,\n        SUM(s.quantity) as total_quantity,\n        COUNT(DISTINCT s.order_id) as order_count,\n        COUNT(DISTINCT s.customer_id) as customer_count\n    FROM fact_sales s\n    JOIN dim_product p ON s.product_id = p.product_id\n    WHERE s.sale_date >= '2024-01-01'\n    GROUP BY p.category_id\n)\nSELECT \n    ch.category_id,\n    ch.category_name,\n    ch.parent_category_id,\n    ch.level,\n    ch.category_path,\n    COALESCE(sc.total_sales, 0) as direct_sales,\n    COALESCE(sc.total_quantity, 0) as direct_quantity,\n    SUM(COALESCE(sc2.total_sales, 0)) as total_sales_with_children,\n    SUM(COALESCE(sc2.total_quantity, 0)) as total_quantity_with_children\nFROM category_hierarchy ch\nLE",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 102
    },
    "id": "tejuu_144"
  },
  {
    "text": "ntity, 0) as direct_quantity,\n    SUM(COALESCE(sc2.total_sales, 0)) as total_sales_with_children,\n    SUM(COALESCE(sc2.total_quantity, 0)) as total_quantity_with_children\nFROM category_hierarchy ch\nLEFT JOIN sales_by_category sc ON ch.category_id = sc.category_id\nLEFT JOIN category_hierarchy ch2 ON ch2.category_path LIKE ch.category_path || '%'\nLEFT JOIN sales_by_category sc2 ON ch2.category_id = sc2.category_id\nGROUP BY \n    ch.category_id,\n    ch.category_name,\n    ch.parent_category_id,\n    ch.level,\n    ch.category_path,\n    sc.total_sales,\n    sc.total_quantity\nORDER BY ch.category_path;\n```\n\n## Data Quality and Validation SQL\n\n### Comprehensive Data Quality Checks\n**Tejuu's Data Validation Framework:**\n\n```sql\n-- Data Quality Report\nWITH null_checks AS (\n    SELECT \n        'fact_sales' as table_name,\n        'customer_id' as column_name,\n        COUNT(*) as total_records,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_count,\n        ROUND(SUM(CASE WHEN cust",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 103
    },
    "id": "tejuu_145"
  },
  {
    "text": "es' as table_name,\n        'customer_id' as column_name,\n        COUNT(*) as total_records,\n        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_count,\n        ROUND(SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as null_percentage\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n    \n    UNION ALL\n    \n    SELECT \n        'fact_sales',\n        'sale_date',\n        COUNT(*),\n        SUM(CASE WHEN sale_date IS NULL THEN 1 ELSE 0 END),\n        ROUND(SUM(CASE WHEN sale_date IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n    \n    UNION ALL\n    \n    SELECT \n        'fact_sales',\n        'sales_amount',\n        COUNT(*),\n        SUM(CASE WHEN sales_amount IS NULL THEN 1 ELSE 0 END),\n        ROUND(SUM(CASE WHEN sales_amount IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 104
    },
    "id": "tejuu_146"
  },
  {
    "text": "les_amount IS NULL THEN 1 ELSE 0 END),\n        ROUND(SUM(CASE WHEN sales_amount IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n),\nrange_checks AS (\n    SELECT \n        'fact_sales' as table_name,\n        'sales_amount' as column_name,\n        MIN(sales_amount) as min_value,\n        MAX(sales_amount) as max_value,\n        AVG(sales_amount) as avg_value,\n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sales_amount) as median_value,\n        SUM(CASE WHEN sales_amount < 0 THEN 1 ELSE 0 END) as negative_count,\n        SUM(CASE WHEN sales_amount = 0 THEN 1 ELSE 0 END) as zero_count\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n),\nduplicate_checks AS (\n    SELECT \n        'fact_sales' as table_name,\n        'order_id' as key_column,\n        COUNT(*) as total_records,\n        COUNT(DISTINCT order_id) as unique_records,\n        COUNT(*) - COUNT(DISTINCT order_id) as duplicate_count",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 105
    },
    "id": "tejuu_147"
  },
  {
    "text": "' as table_name,\n        'order_id' as key_column,\n        COUNT(*) as total_records,\n        COUNT(DISTINCT order_id) as unique_records,\n        COUNT(*) - COUNT(DISTINCT order_id) as duplicate_count\n    FROM fact_sales\n    WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'\n),\nreferential_integrity AS (\n    SELECT \n        'fact_sales -> dim_customer' as relationship,\n        COUNT(DISTINCT s.customer_id) as fact_keys,\n        COUNT(DISTINCT c.customer_id) as dim_keys,\n        COUNT(DISTINCT s.customer_id) - COUNT(DISTINCT c.customer_id) as orphaned_records\n    FROM fact_sales s\n    LEFT JOIN dim_customer c ON s.customer_id = c.customer_id\n    WHERE s.sale_date >= CURRENT_DATE - INTERVAL '30 days'\n)\nSELECT \n    'Null Checks' as check_type,\n    table_name,\n    column_name,\n    null_count as issue_count,\n    null_percentage as issue_percentage,\n    CASE WHEN null_percentage > 5 THEN 'FAIL' ELSE 'PASS' END as status\nFROM null_checks\nWHERE null_count > 0\n\nUNION ALL\n\nSELECT \n    'Range C",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 106
    },
    "id": "tejuu_148"
  },
  {
    "text": " as issue_count,\n    null_percentage as issue_percentage,\n    CASE WHEN null_percentage > 5 THEN 'FAIL' ELSE 'PASS' END as status\nFROM null_checks\nWHERE null_count > 0\n\nUNION ALL\n\nSELECT \n    'Range Checks',\n    table_name,\n    column_name,\n    negative_count + zero_count,\n    ROUND((negative_count + zero_count) * 100.0 / \n          (SELECT COUNT(*) FROM fact_sales WHERE sale_date >= CURRENT_DATE - INTERVAL '30 days'), 2),\n    CASE WHEN negative_count > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM range_checks\n\nUNION ALL\n\nSELECT \n    'Duplicate Checks',\n    table_name,\n    key_column,\n    duplicate_count,\n    ROUND(duplicate_count * 100.0 / total_records, 2),\n    CASE WHEN duplicate_count > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM duplicate_checks\n\nUNION ALL\n\nSELECT \n    'Referential Integrity',\n    relationship,\n    '',\n    orphaned_records,\n    ROUND(orphaned_records * 100.0 / fact_keys, 2),\n    CASE WHEN orphaned_records > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM referential_integrity;\n```\n\n## Interview ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 107
    },
    "id": "tejuu_149"
  },
  {
    "text": "ionship,\n    '',\n    orphaned_records,\n    ROUND(orphaned_records * 100.0 / fact_keys, 2),\n    CASE WHEN orphaned_records > 0 THEN 'FAIL' ELSE 'PASS' END\nFROM referential_integrity;\n```\n\n## Interview Talking Points\n\n### Technical Achievements\n- Wrote 500+ SQL queries for business analytics\n- Optimized slow queries from 5 minutes to 10 seconds\n- Built automated data quality checks catching 95% of issues\n- Created reusable SQL templates for common analyses\n- Trained 15+ analysts on advanced SQL techniques\n\n### Problem-Solving Examples\n**Performance Optimization:**\n\"At Stryker, we had this Medicaid claims query that was taking 5 minutes to run. I analyzed the execution plan and found we were doing a full table scan on a 50 million row table. I added appropriate indexes on claim_date and provider_id, rewrote the query to use CTEs instead of subqueries, and got it down to 10 seconds.\"\n\n**Complex Business Logic:**\n\"At Central Bank, finance needed a complex calculation for regulatory reportin",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 108
    },
    "id": "tejuu_150"
  },
  {
    "text": "der_id, rewrote the query to use CTEs instead of subqueries, and got it down to 10 seconds.\"\n\n**Complex Business Logic:**\n\"At Central Bank, finance needed a complex calculation for regulatory reporting that involved multiple date ranges, conditional aggregations, and hierarchical rollups. I broke it down into CTEs, tested each piece separately, and documented the logic. The final query was 200 lines but very maintainable and accurate.\"\n\n### Tools & Technologies\n- **Databases**: SQL Server, Oracle, PostgreSQL, MySQL\n- **SQL Skills**: Window functions, CTEs, recursive queries, performance tuning\n- **Tools**: SQL Server Management Studio, DBeaver, DataGrip\n- **Integration**: Power BI, Tableau, Python, Excel\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/sql_advanced.md",
      "file_name": "sql_advanced.md",
      "chunk_index": 109
    },
    "id": "tejuu_151"
  },
  {
    "text": "---\ntags: [tejuu, experience, resume, business-analyst, bi-developer, analytics-engineer]\npersona: tejuu\n---\n\n# Tejuu's Professional Experience & Background\n\n## Professional Summary\n\nBusiness Intelligence & Analytics professional with hands-on expertise in SQL, Python, and modern BI stacks. Experienced in leading data migration initiatives, building ETL pipelines on Azure, and delivering enterprise-grade Power BI solutions. Adept at turning messy legacy systems into trusted metrics and scalable models that power decision-making for finance, operations, and leadership.\n\n## Professional Experience\n\n### Senior Analytics Engineer — Central Bank of Missouri\n**Duration:** Dec 2024 – Present\n\n**Key Responsibilities and Achievements:**\n\nSo at Central Bank of Missouri, I'm currently leading this major migration project where we're moving all these fragmented Excel files and legacy reports into Azure SQL with Power BI semantic models. What I did was write SQL and Python validation scripts that h",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 110
    },
    "id": "tejuu_152"
  },
  {
    "text": "jor migration project where we're moving all these fragmented Excel files and legacy reports into Azure SQL with Power BI semantic models. What I did was write SQL and Python validation scripts that helped us reconcile over 12 million rows of data. The result was pretty significant - we improved reporting efficiency by 25% and really reduced the manual workload that the team was dealing with.\n\nOne of the things I'm really proud of is building these certified dashboards with role-based security. I implemented RLS (row-level security) and standardized KPIs, which enabled secure self-service for our finance, audit, and risk teams. This actually reduced ad-hoc reporting requests by 30% across multiple departments, which was a huge win for everyone.\n\nI also implemented automated SQL quality checks and monitoring pipelines. What this did was improve the accuracy of 15+ recurring regulatory reports by 20%. This was super important because it strengthened our compliance confidence and increase",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 111
    },
    "id": "tejuu_153"
  },
  {
    "text": "cks and monitoring pipelines. What this did was improve the accuracy of 15+ recurring regulatory reports by 20%. This was super important because it strengthened our compliance confidence and increased trust among senior stakeholders.\n\nAnother major project was partnering with finance and audit groups to map over 200 legacy metrics into standardized semantic models. We ended up deprecating duplicate workbooks and consolidating 30% of manual spreadsheets into governed BI assets that are now used organization-wide.\n\nI also mentor analysts on DAX optimization, KPI documentation, and structured intake processes. This has raised delivery timeliness, improved collaboration, and freed over 30 analyst hours monthly from repetitive reporting and rework.\n\n**Technologies Used:**\n- Azure SQL, Power BI, DAX, Power Query\n- SQL, Python\n- Git for version control\n- Azure Data Factory\n\n### Analytics Engineer — Stryker\n**Duration:** Jan 2022 – Dec 2024\n\n**Key Responsibilities and Achievements:**\n\nAt Stry",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 112
    },
    "id": "tejuu_154"
  },
  {
    "text": ", DAX, Power Query\n- SQL, Python\n- Git for version control\n- Azure Data Factory\n\n### Analytics Engineer — Stryker\n**Duration:** Jan 2022 – Dec 2024\n\n**Key Responsibilities and Achievements:**\n\nAt Stryker, I directed this massive migration of global sales and service data from Oracle and Excel into Azure Synapse star schemas. I built the ETL using Azure Data Factory and Databricks, and validated over 15 million transactions using Python scripts. This enabled unified reporting across all regions, which was a game-changer for the company.\n\nOne of my biggest achievements was engineering these partitioned ELT pipelines with optimized orchestration. I cut refresh runtimes by 45% and lowered compute spend significantly. At the same time, we scaled secure data access to over 1,000 field reps, executives, and business stakeholders worldwide.\n\nI developed comprehensive data lineage documentation and KPI guardrails. What this did was lower discrepancies in quarterly executive scorecards, accelera",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 113
    },
    "id": "tejuu_155"
  },
  {
    "text": "utives, and business stakeholders worldwide.\n\nI developed comprehensive data lineage documentation and KPI guardrails. What this did was lower discrepancies in quarterly executive scorecards, accelerate month-end close by days, and improve trust in CFO reporting packages globally.\n\nI also collaborated with clinical and product teams to translate operational questions into measurable KPIs and drill-through dashboards. This work uncovered process inefficiencies and unlocked over $2 million in actionable revenue opportunities.\n\n**Technologies Used:**\n- Azure Synapse, Azure Data Factory, Databricks\n- SQL, Python, PySpark\n- Power BI, DAX\n- Oracle database\n- Star schema design\n\n**Challenges Overcome:**\n- Data quality issues during migration from legacy systems\n- Performance optimization for large-scale data processing\n- Stakeholder alignment across multiple global regions\n- Balancing technical debt with new feature delivery\n\n### Product Data Analyst — CVS Health\n**Duration:** May 2020 – Jan ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 114
    },
    "id": "tejuu_156"
  },
  {
    "text": "e-scale data processing\n- Stakeholder alignment across multiple global regions\n- Balancing technical debt with new feature delivery\n\n### Product Data Analyst — CVS Health\n**Duration:** May 2020 – Jan 2022\n\n**Key Responsibilities and Achievements:**\n\nAt CVS Health, I migrated pharmacy and inventory reporting from Access and Excel into Azure SQL with Power BI pipelines. I designed Python and SQL reconciliation scripts that validated over 10 million records and reduced manual reconciliation tasks by 40% consistently.\n\nI tuned complex SQL transformations and Power Query refreshes to stabilize recurring dashboards. This saved 25-30 analyst hours monthly and improved the accuracy and reliability of operational reporting across nationwide teams.\n\nI partnered with engineers to define semantic layer fields, KPI acceptance contracts, and validation scripts. This ensured smooth release cycles and reduced risks of downstream disruptions during production deployments.\n\nI also piloted A/B reporting ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 115
    },
    "id": "tejuu_157"
  },
  {
    "text": "r fields, KPI acceptance contracts, and validation scripts. This ensured smooth release cycles and reduced risks of downstream disruptions during production deployments.\n\nI also piloted A/B reporting tied to operational initiatives and staffing levers. I translated findings into measurable throughput improvements that directly influenced roadmap prioritization and leadership decision making.\n\n**Technologies Used:**\n- Azure SQL, Power BI\n- Python, SQL\n- Power Query, DAX\n- Access database migration\n- Excel automation\n\n**Key Learnings:**\n- Importance of data quality checks in migration projects\n- Stakeholder communication is critical for adoption\n- Documentation saves countless hours of rework\n- Automated testing catches issues before production\n\n### Business Analyst — Colruyt IT Group\n**Duration:** May 2018 – Dec 2019\n\n**Key Responsibilities and Achievements:**\n\nAt Colruyt IT Group, I consolidated fragmented Excel and Access reporting processes into standardized Power BI dashboards with ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 116
    },
    "id": "tejuu_158"
  },
  {
    "text": "tion:** May 2018 – Dec 2019\n\n**Key Responsibilities and Achievements:**\n\nAt Colruyt IT Group, I consolidated fragmented Excel and Access reporting processes into standardized Power BI dashboards with reusable DAX templates. This reduced duplication and increased trust in weekly financial and merchandising performance metrics.\n\nI designed SQL dimensional models to support finance and merchandising analyses. This accelerated month-end close cycles, provided interactive drilldowns, and replaced time-consuming manual spreadsheet-based efforts.\n\nI captured requirements across departments, authored detailed user stories, and developed UAT scripts with standardized KPI definitions. This ensured successful deliveries aligned with governance rules and stakeholder expectations.\n\n**Technologies Used:**\n- Power BI, DAX, Power Query\n- SQL Server\n- Excel, Access\n- Requirements gathering tools\n- UAT documentation\n\n**Skills Developed:**\n- Stakeholder management across multiple departments\n- Requiremen",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 117
    },
    "id": "tejuu_159"
  },
  {
    "text": "**\n- Power BI, DAX, Power Query\n- SQL Server\n- Excel, Access\n- Requirements gathering tools\n- UAT documentation\n\n**Skills Developed:**\n- Stakeholder management across multiple departments\n- Requirements gathering and documentation\n- UAT script development\n- KPI definition and governance\n\n## Technical Skills\n\n### Programming & Data\n- **SQL**: Expert level - Complex queries, window functions, CTEs, performance tuning\n- **Python**: Proficient - Pandas, NumPy, data validation scripts, automation\n- **R**: Intermediate - Statistical analysis, data visualization\n- **Git**: Version control for code and documentation\n\n### Analytics & BI Tools\n- **Power BI**: Expert - DAX, Power Query, data modeling, RLS, performance optimization\n- **Tableau**: Proficient - Dashboard design, calculated fields, parameters\n- **Excel**: Advanced - Power Pivot, Power Query, VBA, complex formulas\n\n### Cloud & Data Platforms\n- **Azure**: Synapse, Data Factory, Databricks, Azure SQL\n- **Snowflake**: Data warehousing, q",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 118
    },
    "id": "tejuu_160"
  },
  {
    "text": "eters\n- **Excel**: Advanced - Power Pivot, Power Query, VBA, complex formulas\n\n### Cloud & Data Platforms\n- **Azure**: Synapse, Data Factory, Databricks, Azure SQL\n- **Snowflake**: Data warehousing, query optimization\n- **AWS**: Basic familiarity with S3, Redshift\n- **dbt**: Data transformation, testing, documentation\n\n### Data & Business Skills\n- Data migration and ETL/ELT\n- KPI definition and governance\n- Dimensional modeling (star and snowflake schemas)\n- Forecasting and predictive analytics\n- Data quality and validation\n- Data lineage and documentation\n- Requirements gathering\n- Stakeholder management\n- UAT and testing\n\n## Education\n\n**MS Data Science**\nUniversity of North Texas, Denton, TX\n\n**B.Tech Information Technology**\nJNTU, Hyderabad, India\n\n## Certifications\n\n- **Scrum Alliance** — Certified Scrum Product Owner\n- **Google** — Business Intelligence Professional\n- **Microsoft** — Azure Data Engineer\n\n## Key Achievements & Metrics\n\n### Efficiency & Savings\n- Saved 25-30 hours ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 119
    },
    "id": "tejuu_161"
  },
  {
    "text": "** — Certified Scrum Product Owner\n- **Google** — Business Intelligence Professional\n- **Microsoft** — Azure Data Engineer\n\n## Key Achievements & Metrics\n\n### Efficiency & Savings\n- Saved 25-30 hours monthly through automation\n- Cut manual reconciliation by 40%\n- Reduced refresh runtime by 45%\n- Reduced compute spend by 22%\n\n### Scale & Volume\n- Reconciled 12M+ legacy rows\n- Built pipelines for 15M+ transactions\n- Supported 1K+ field users\n- Processed 10+ TB monthly\n\n### Impact & Adoption\n- Improved reporting efficiency by 25%\n- Reduced ad-hoc requests by 30%\n- Boosted dashboard adoption across regions\n- Consolidated 30% of manual spreadsheets\n\n### Financial Impact\n- Uncovered $2M+ revenue opportunity\n- Cut ad-hoc requests by 32%\n- Improved quarterly close by multiple days\n- Reduced operational costs significantly\n\n### Accuracy & Quality\n- Improved metric accuracy by 20%\n- Reduced discrepancies by 30%\n- Improved trust in KPIs across organization\n\n## Interview Talking Points\n\n### Proble",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 120
    },
    "id": "tejuu_162"
  },
  {
    "text": "ional costs significantly\n\n### Accuracy & Quality\n- Improved metric accuracy by 20%\n- Reduced discrepancies by 30%\n- Improved trust in KPIs across organization\n\n## Interview Talking Points\n\n### Problem-Solving Examples\n\n**Challenge: Legacy System Migration**\nSo at CVS Health, we had this huge challenge where all the pharmacy reporting was in Access databases and Excel spreadsheets. The data was inconsistent, reports were breaking, and analysts were spending hours manually reconciling numbers. What I did was design a migration strategy to move everything to Azure SQL and Power BI. I wrote Python scripts to validate the data during migration, and we reconciled over 10 million records. The result was that we reduced manual work by 40% and improved data accuracy significantly.\n\n**Challenge: Performance Optimization**\nAt Stryker, we had these ELT pipelines that were taking forever to run - like 2-3 hours for a single refresh. This was causing delays in reporting and frustrating users. I ana",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 121
    },
    "id": "tejuu_163"
  },
  {
    "text": "rformance Optimization**\nAt Stryker, we had these ELT pipelines that were taking forever to run - like 2-3 hours for a single refresh. This was causing delays in reporting and frustrating users. I analyzed the execution plans, identified bottlenecks, and implemented partitioning strategies in Databricks. I also optimized the orchestration in Azure Data Factory. The result was that we cut the runtime by 45% - from 2-3 hours down to about an hour. This also reduced our compute costs significantly.\n\n**Challenge: Stakeholder Alignment**\nOne challenge I faced at Central Bank was getting buy-in from different departments for the migration project. Finance wanted things one way, audit had different requirements, and risk had their own needs. What I did was organize workshops with each group, documented their requirements, and found common ground. I created a standardized KPI framework that met everyone's needs. The result was successful adoption across all departments and a 30% reduction in a",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 122
    },
    "id": "tejuu_164"
  },
  {
    "text": "mented their requirements, and found common ground. I created a standardized KPI framework that met everyone's needs. The result was successful adoption across all departments and a 30% reduction in ad-hoc reporting requests.\n\n### Technical Strengths\n\n**Power BI & DAX**\nI'm really strong in Power BI and DAX. I've built over 50 dashboards across different business functions. I know how to optimize DAX formulas for performance, implement RLS for security, and design semantic models that are easy for business users to understand.\n\n**SQL & Data Modeling**\nSQL is my bread and butter. I write complex queries daily, including window functions, CTEs, and performance-tuned joins. I'm experienced in dimensional modeling - both star and snowflake schemas. I know how to design data warehouses that are optimized for analytics.\n\n**Data Migration**\nI've led multiple large-scale data migration projects. I know how to plan migrations, validate data, handle edge cases, and ensure business continuity. I ",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 123
    },
    "id": "tejuu_165"
  },
  {
    "text": " optimized for analytics.\n\n**Data Migration**\nI've led multiple large-scale data migration projects. I know how to plan migrations, validate data, handle edge cases, and ensure business continuity. I always write reconciliation scripts to make sure no data is lost and everything matches.\n\n**Stakeholder Management**\nI'm good at working with non-technical stakeholders. I can translate business requirements into technical specifications and explain technical concepts in business terms. I've worked with executives, finance teams, operations, and IT across multiple companies.\n\n## Work Style & Approach\n\n### My Philosophy\nI believe in building things that last. When I design a dashboard or data model, I think about maintainability, scalability, and documentation. I don't just deliver a solution - I make sure the team can maintain it after I'm done.\n\n### Collaboration\nI work well in cross-functional teams. I've collaborated with data engineers, software developers, business analysts, and execu",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 124
    },
    "id": "tejuu_166"
  },
  {
    "text": " I make sure the team can maintain it after I'm done.\n\n### Collaboration\nI work well in cross-functional teams. I've collaborated with data engineers, software developers, business analysts, and executives. I believe in clear communication, documentation, and knowledge sharing.\n\n### Continuous Learning\nI'm always learning new tools and techniques. I recently learned dbt and have been exploring more advanced Azure services. I believe in staying current with industry trends and best practices.\n\n### Problem-Solving Approach\nWhen I face a problem, I start by understanding the root cause. I gather requirements, analyze the data, and design a solution. I believe in iterative development - start with an MVP, get feedback, and improve. I also believe in testing thoroughly before production deployment.\n",
    "metadata": {
      "persona": "tejuu",
      "file_path": "kb_tejuu/business_analyst/tejuu_experience.md",
      "file_name": "tejuu_experience.md",
      "chunk_index": 125
    },
    "id": "tejuu_167"
  }
]