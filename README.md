# RAG Chatbot - Advanced Interview Preparation Assistant

A sophisticated dual-persona RAG (Retrieval-Augmented Generation) chatbot designed for comprehensive interview preparation, featuring two professional profiles with specialized expertise in AI/ML/Data Engineering and Business Intelligence/Analytics Engineering. Built with modern React frontend and FastAPI backend, optimized for mobile responsiveness and production deployment.

## ğŸ—ï¸ Project Structure

```
rag-chatbot/
â”œâ”€â”€ api/                          # Backend API modules
â”‚   â”œâ”€â”€ chat.py                   # Chat endpoint handlers
â”‚   â”œâ”€â”€ health.py                 # Health check endpoints
â”‚   â”œâ”€â”€ transcribe.py             # Audio transcription
â”‚   â”œâ”€â”€ utils.py                  # Core RAG logic and utilities
â”‚   â”œâ”€â”€ embeddings.npy            # Vector embeddings
â”‚   â”œâ”€â”€ faiss.index              # FAISS vector index
â”‚   â””â”€â”€ meta.json                # Metadata for embeddings
â”œâ”€â”€ core/                         # Core application logic
â”‚   â”œâ”€â”€ answer_v2.py             # Enhanced answer generation
â”‚   â”œâ”€â”€ enhanced_answer.py       # Template-based answering
â”‚   â”œâ”€â”€ memory.py                # Conversation memory
â”‚   â”œâ”€â”€ profile_ctx.py           # Profile context management
â”‚   â”œâ”€â”€ router.py                # Intent routing
â”‚   â””â”€â”€ templates/               # Template system
â”‚       â”œâ”€â”€ fragments/           # Template fragments
â”‚       â”œâ”€â”€ loader.py            # Template loader
â”‚       â”œâ”€â”€ prompter.py          # Template processor
â”‚       â””â”€â”€ registry.py          # Template registry
â”œâ”€â”€ frontend/                     # React frontend
â”‚   â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ public/                  # Static assets
â”‚   â”œâ”€â”€ build/                   # Production build
â”‚   â””â”€â”€ package.json             # Frontend dependencies
â”œâ”€â”€ kb_profile_a/                # Profile A knowledge base
â”‚   â”œâ”€â”€ ai_ml/                   # AI/ML content
â”‚   â””â”€â”€ data_engineering/        # Data Engineering content
â”œâ”€â”€ kb_profile_b/                # Profile B knowledge base
â”‚   â”œâ”€â”€ analytics_mode/          # Analytics Engineering content
â”‚   â””â”€â”€ business_mode/           # Business Intelligence content
â”œâ”€â”€ content/                     # Profile configurations
â”‚   â”œâ”€â”€ metrics/                 # Performance metrics
â”‚   â””â”€â”€ profiles/                # Profile definitions
â”œâ”€â”€ config/                      # Deployment configurations
â”‚   â”œâ”€â”€ vercel.json             # Vercel deployment config
â”‚   â”œâ”€â”€ render.yaml             # Render deployment config
â”‚   â”œâ”€â”€ Procfile                # Heroku deployment config
â”‚   â””â”€â”€ runtime.txt             # Python runtime version
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ generate_embeddings.py   # Generate embeddings
â”‚   â”œâ”€â”€ generate_multi_profile_embeddings.py
â”‚   â””â”€â”€ ingest.py               # Knowledge base ingestion
â”œâ”€â”€ tests/                       # Test files
â”‚   â””â”€â”€ phase2_runner.py        # Phase 2 test runner
â”œâ”€â”€ docs/                        # Documentation
â”‚   â””â”€â”€ *.md                    # All documentation files
â”œâ”€â”€ store/                       # Data storage
â”‚   â”œâ”€â”€ embeddings.npy          # Vector embeddings
â”‚   â”œâ”€â”€ faiss.index             # FAISS vector index
â”‚   â””â”€â”€ meta.json               # Metadata
â”œâ”€â”€ server.py                    # Main FastAPI server (local dev)
â”œâ”€â”€ render_app.py               # Render deployment server
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ build.sh                    # Build script
```

## ğŸš€ Quick Start

### Local Development

1. **Install Dependencies**
   ```bash
   # Backend
   pip install -r requirements.txt
   
   # Frontend
   cd frontend
   npm install
   ```

2. **Set Environment Variables**
   ```bash
   export OPENAI_API_KEY="your-openai-api-key"
   ```

3. **Start the Application**
   ```bash
   # Start backend server
   python server.py
   
   # In another terminal, start frontend
   cd frontend
   npm start
   ```

4. **Access the Application**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000

### Production Deployment

#### Render Deployment
```bash
# Uses render_app.py and config/render.yaml
# Automatically builds frontend and serves both
```

#### Vercel Deployment
```bash
# Frontend only - uses config/vercel.json
# Backend should be deployed separately
```

## ğŸ¯ Features

### ğŸ§  Dual Persona System
- **Profile A**: AI/ML Engineer & Data Engineer with 6+ years experience
  - **AI/ML Mode**: LangChain, LangGraph, RAG systems, MLOps, FastAPI
  - **Data Engineering Mode**: PySpark, Databricks, Azure Data Factory, ETL/ELT
- **Profile B**: Business Intelligence Developer & Analytics Engineer with 6+ years experience
  - **Business Intelligence Mode**: Power BI, Tableau, DAX, stakeholder management
  - **Analytics Engineering Mode**: dbt, data modeling, Azure/AWS cloud platforms

### ğŸ¯ Intelligent Question Routing
- **Auto-detection**: Automatically identifies question type and domain
- **Smart Modes**: 4 specialized modes (Code, SQL, Interview, General)
- **Profile Selection**: Manual or automatic profile/mode selection
- **Generic Profiles**: Two professional profiles (A & B) with distinct expertise areas
- **Context Awareness**: Routes based on technical keywords and question patterns

### ğŸš€ Advanced RAG System
- **Vector Search**: High-performance semantic search with scikit-learn
- **Knowledge Base**: 557+ curated chunks across 4 specialized domains
- **Source Citations**: Transparent source tracking and references
- **Quality Filtering**: Profile-specific content filtering and ranking

### ğŸ’¬ Interview-Optimized Responses
- **Structured Format**: APPROACH â†’ CODE â†’ EXPLANATION â†’ PRO TIP format
- **STAR Method**: Situation, Task, Action, Result for behavioral questions
- **Humanized Output**: Natural, conversational tone with contractions
- **Performance Metrics**: Real project numbers and quantifiable results

### ğŸ¤ Audio & Voice Features
- **Voice Recording**: Real-time audio capture with MediaRecorder API
- **Whisper Integration**: OpenAI Whisper-1 for accurate transcription
- **Auto-Send**: Transcribed text automatically sent to chat
- **Mobile Optimized**: Touch-friendly recording controls

### ğŸ“± Mobile-First Design
- **Fully Responsive**: Optimized for all screen sizes (320px - 1920px+)
- **Touch-Friendly**: 44px minimum touch targets
- **Safe Area Support**: iPhone X+ notch and dynamic island support
- **Progressive Web App**: Installable on mobile devices

### ğŸ¨ Modern UI/UX
- **React 19 + TypeScript**: Latest React with full type safety
- **TailwindCSS**: Utility-first styling with dark/light themes
- **Framer Motion**: Smooth animations and transitions
- **Real-time Chat**: Instant message display with typing indicators

### ğŸ’¾ Advanced Chat Management
- **Persistent History**: Local storage with conversation management
- **Delete Chats**: Individual chat deletion with confirmation
- **Conversation Export**: Copy messages and code snippets
- **Session Management**: Profile-specific conversation contexts

### âš¡ Performance Optimizations
- **Fast Responses**: 5-6 second average response time (gpt-4o-mini)
- **Caching**: LRU cache for embeddings and API responses
- **Code Splitting**: Optimized bundle sizes for faster loading
- **Streaming**: Real-time response generation (prepared)

## ğŸ”§ Configuration

### Environment Variables
- `OPENAI_API_KEY`: Required for OpenAI API access
- `RETRIEVAL_TOPK`: Number of top results to retrieve (default: 6)
- `USE_LLM`: Enable/disable LLM generation (default: false)

### Profile Configuration
- Profiles defined in `content/profiles/`
- Metrics tracked in `content/metrics/`

## ğŸ“š Knowledge Base

### Profile A Knowledge Base (`kb_profile_a/`)
- **AI/ML Content** (109 chunks): LangChain, LangGraph, RAG systems, MLOps, FastAPI, MLflow
- **Data Engineering Content** (219 chunks): PySpark, Databricks, Azure Data Factory, ETL/ELT patterns, Delta Lake, streaming data

### Profile B Knowledge Base (`kb_profile_b/`)
- **Analytics Engineering Content** (122 chunks): dbt, data modeling, star schemas, Azure/AWS platforms
- **Business Intelligence Content** (107 chunks): Power BI, Tableau, DAX, stakeholder management, KPI frameworks

### Content Quality
- **557+ Total Chunks**: Comprehensive coverage across all domains
- **Real Project Examples**: Based on actual work experience across healthcare, retail, and financial sectors
- **Interview-Ready**: Structured for technical and behavioral interview preparation
- **Performance Metrics**: Includes quantifiable results and project outcomes

## ğŸ§ª Testing & Quality Assurance

### Comprehensive Test Suite
```bash
# Run Phase 2 test suite
export API_URL="http://localhost:8000"
python tests/phase2_runner.py

# Run quality assessment
python scripts/quality_assessment.py

# Performance testing
python scripts/performance_test.py
```

### Quality Metrics
- **100% Success Rate**: All 20 test questions pass quality standards
- **93% Excellence Rate**: 74/80 quality criteria met across all modes
- **Zero Generic Intros**: No repetitive "I'm a [role] with X years..." responses
- **Technical Depth**: Specific tools, technologies, and implementations
- **Real Metrics**: Quantifiable project outcomes and performance improvements

### Test Coverage
- **Profile A AI Mode**: 8/8 questions (100% excellent)
- **Profile A DE Mode**: 4/4 questions (100% pass)
- **Profile B BI Mode**: 4/4 questions (100% pass)
- **Profile B AE Mode**: 4/4 questions (100% pass)

## ğŸ“– Documentation

Comprehensive documentation organized in the `docs/` folder:

### ğŸš€ Deployment & Setup
- `DEPLOYMENT_SUCCESS.md` - Complete deployment guide
- `RENDER_DEPLOYMENT.md` - Render platform deployment
- `HYBRID_DEPLOYMENT.md` - Frontend/backend separation
- `PRODUCTION_CHECKLIST.md` - Production readiness checklist

### ğŸ¯ Feature Documentation
- `INTERVIEW_MODE_FEATURES.md` - Interview-optimized answer format
- `MOBILE_RESPONSIVENESS_GUIDE.md` - Mobile optimization guide
- `INTERVIEW_PREPARATION_GUIDE.md` - Comprehensive interview prep
- `LATEST_IMPROVEMENTS.md` - Recent feature updates

### ğŸ“Š Quality & Testing
- `ALL_MODES_QUALITY_REPORT.md` - Complete quality assessment
- `FINAL_QUALITY_REPORT.md` - Final quality metrics
- `COMPLETE_SUCCESS_SUMMARY.md` - Success metrics and achievements

### ğŸ”§ Technical Guides
- `KNOWLEDGE_BASE_SUMMARY.md` - Knowledge base organization
- `PROFILE_FIX.md` - Profile system fixes and improvements
- `CLEANUP_SUMMARY.md` - Code cleanup and optimization

## ğŸ”„ Development Workflow

### ğŸ“ Adding Knowledge Base Content
1. **Add Content**: Add markdown files to appropriate `kb_profile_a/` or `kb_profile_b/` directory
2. **Generate Embeddings**: Run `python scripts/generate_embeddings_complete.py`
3. **Test Quality**: Run `python scripts/quality_assessment.py`
4. **Deploy**: Commit and push changes for automatic deployment

### ğŸ¨ Updating Templates & Prompts
1. **Modify Templates**: Edit templates in `core/templates/fragments/`
2. **Update Prompts**: Modify prompt templates in `api/utils.py`
3. **Test Responses**: Use quality assessment to verify improvements
4. **Deploy Changes**: Push to trigger automatic deployment

### ğŸ§ª Testing & Quality Assurance
1. **Local Testing**: Run `python tests/phase2_runner.py`
2. **Quality Assessment**: Execute `python scripts/quality_assessment.py`
3. **Performance Testing**: Run `python scripts/performance_test.py`
4. **Production Testing**: Test deployed endpoints and frontend

### ğŸš€ Deployment Process
1. **Automatic Deployment**: Changes auto-deploy to Render (backend) and Vercel (frontend)
2. **Manual Deployment**: Use `vercel --prod` for frontend, Render dashboard for backend
3. **Health Checks**: Verify endpoints with `curl` commands
4. **Monitoring**: Check logs in Render dashboard and Vercel analytics

## ğŸš€ Deployment

### ğŸŒ Production URLs
- **Frontend (Vercel)**: `https://ragaichatbot-dznt1l2mm-shadowdevils-projects-ae938de8.vercel.app`
- **Backend (Render)**: `https://rag-chatbot-api-33r9.onrender.com`

### ğŸ  Local Development
```bash
# Backend (FastAPI)
python server.py
# Runs on http://localhost:8000

# Frontend (React)
cd frontend
npm start
# Runs on http://localhost:3000
```

### â˜ï¸ Production Deployment
- **Backend (Render)**: Automatic deployment from main branch
  - Uses `render_app.py` with `config/render.yaml`
  - Python 3.13.4 runtime
  - Auto-scaling and health monitoring
- **Frontend (Vercel)**: Automatic deployment from main branch
  - Uses `config/vercel.json` configuration
  - Global CDN distribution
  - Automatic HTTPS and custom domains

### ğŸ”§ Environment Configuration
- **Required**: `OPENAI_API_KEY` environment variable
- **Optional**: `RETRIEVAL_TOPK`, `USE_LLM`, `BYPASS_HTTP_PROXY_FOR_OPENAI`
- **Frontend**: `REACT_APP_API_URL` for backend connection

## ğŸ”Œ API Endpoints

### Chat Endpoint
```bash
POST /api/chat
Content-Type: application/json

{
  "message": "Your question here",
  "mode": "auto|de|ai|bi|ae",
  "profile": "auto|profile_a|profile_b",
  "session_id": "default"
}
```

### Health Check
```bash
GET /api/health
# Returns: {"ok": true, "embeddings": true, "error": null}
```

### Audio Transcription
```bash
POST /api/transcribe
Content-Type: application/json

{
  "audio_data": "base64_encoded_audio"
}
```

## ğŸ†• Recent Improvements

### âœ¨ Latest Features (2024)
- **Delete Chat Functionality**: Individual chat deletion with confirmation dialogs
- **Audio Transcription**: Voice-to-text using OpenAI Whisper API
- **Performance Optimization**: 50% faster responses (gpt-4o-mini)
- **Mobile Responsiveness**: Complete mobile optimization for all screen sizes
- **Interview Mode**: Structured APPROACH â†’ CODE â†’ EXPLANATION â†’ PRO TIP format
- **Enhanced Knowledge Base**: 557+ curated chunks with real project examples

### ğŸš€ Performance Improvements
- **Response Time**: Reduced from 10-15s to 5-6s average
- **Model Optimization**: Switched to gpt-4o-mini for speed and cost efficiency
- **Caching**: LRU cache for embeddings and API responses
- **Mobile Performance**: Optimized for mobile networks and touch interactions

### ğŸ¯ Quality Enhancements
- **Zero Generic Intros**: Eliminated repetitive "I'm a [role] with X years..." responses
- **Technical Depth**: Specific tools, technologies, and real project metrics
- **Humanized Output**: Natural, conversational tone with contractions
- **Source Citations**: Transparent source tracking and references

## ğŸ“ License

MIT License - see LICENSE file for details
