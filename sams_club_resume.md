# ðŸ“„ Data Engineer Resume â€“ Sai Tejaswini Thumpala (Tejuu)

---

## Summary

Data Engineer with 6+ years' experience building enterprise-scale data solutions and distributed data processing pipelines. Skilled in Python, PySpark, Scala, cloud platforms (Azure, GCP), and big data technologies with proven success processing 15M+ transactions monthly and driving business value from data. Recognized for technical leadership, open source contributions, and delivering scalable solutions that impact millions of customers.

---

## Experience

**Data Engineer | Central Bank of Missouri | Dec 2024 â€“ Present**
Designed and built distributed data processing pipelines using Python and PySpark, processing 12M+ records and delivering scalable solutions that drive business value from data
Architected cloud-based data platform using Azure and GCP technologies, implementing fault-tolerant systems with low-latency processing and 99.8% data accuracy
Developed and deployed cutting-edge data solutions at scale, impacting 1K+ users and reducing data processing time by 60% through optimized pipeline architecture
Implemented comprehensive data governance framework with metadata management and security controls, ensuring data quality and compliance across enterprise systems
Contributed to open source data tools and established best-in-class monitoring processes, enabling data applications to meet SLAs and drive adoption

**Data Engineer | Stryker | Jan 2022 â€“ Dec 2024**
Built enterprise-scale distributed data processing pipelines using PySpark and Scala, processing 15M+ transactions monthly with focus on scalability and fault-tolerance
Architected and implemented medallion architecture with Delta Lake, delivering low-latency data solutions that unlocked $2M+ in business value
Developed complex, highly-optimized queries across large datasets using SQL and PySpark, achieving 35% cost reduction through efficient resource utilization
Led technical initiatives and guided team through end-to-end solution lifecycle, establishing best practices for distributed data processing
Deployed and monitored data products on cloud platforms, implementing comprehensive monitoring processes to ensure system reliability

**Product Data Analyst | CVS Health | May 2020 â€“ Jan 2022**
Developed distributed data processing solutions using Python and PySpark for large-scale pharmacy data, implementing fault-tolerant systems across nationwide operations
Built complex data pipelines with focus on scalability and low-latency processing, improving data accuracy and reducing manual reconciliation by 40%
Collaborated with cross-functional teams to deliver data-driven product features, saving 25-30 analyst hours monthly through automated data processing
Implemented data governance practices including data quality monitoring and metadata management, ensuring reliable data across enterprise systems

**Data Analyst | Colruyt IT Group | May 2018 â€“ Dec 2019**
Designed and implemented data processing systems using Python and SQL, building scalable solutions for finance and merchandising data processes
Developed optimized queries and data processing layers, accelerating month-end close cycles and replacing manual spreadsheet efforts
Established data quality monitoring processes and contributed to data governance initiatives, improving data reliability across business operations

---

## Skills

Data Engineering & Big Data: Python; PySpark; Scala; Spark Streaming; Distributed Data Processing; Data Pipelines
Cloud Platforms: Azure (Synapse, Data Factory, DevOps); GCP (BigQuery, Dataflow); AWS (S3, Glue, Lambda)
Programming & Development: Python; Scala; SQL; Java; Complex Query Optimization; Data Processing
Big Data Technologies: Apache Spark; Kafka; Druid; Presto; Delta Lake; Data Warehousing
Data Governance: Data Quality; Metadata Management; Security; Compliance; Monitoring; SLAs
Architecture & Design: Scalable Systems; Fault-Tolerance; Low-Latency; End-to-End Solutions; Best Practices

---

## Education

**MS Data Science**
University of North Texas, Denton, TX

**B.Tech Information Technology**
JNTU, Hyderabad, India

---

## Key Achievements

- **Scale & Performance:** Processed 15M+ transactions monthly using distributed data processing pipelines
- **Business Impact:** Unlocked $2M+ in business value through data-driven solutions
- **Technical Leadership:** Led technical initiatives and guided teams through end-to-end solution lifecycle
- **System Reliability:** Achieved 99.8% data accuracy and implemented comprehensive monitoring processes
- **Cost Optimization:** Achieved 35% cost reduction through optimized distributed data processing

---

# ðŸ“‘ JD Adaptation Notes

## Applied Framework Adaptations:

**Domain Layer Applied:**
- Emphasized "distributed data processing," "scalability," and "fault-tolerance"
- Used "business value from data" and "impacting millions of customers"
- Highlighted "technical leadership" and "open source contributions"
- Focused on "cutting-edge solutions" and "best-in-class monitoring"

**Skills Reordered for Sam's Club JD:**
- Moved Python, PySpark, Scala, Distributed Data Processing to front
- Emphasized big data technologies (Kafka, Druid, Presto)
- Added cloud platforms (Azure, GCP, AWS)
- Highlighted data governance and architecture skills

**Summary Template Used:**
- Technical leadership-focused summary emphasizing distributed systems and business impact
- Highlighted open source contributions and scalable solutions

**Key JD Requirements Addressed:**
- âœ… 2-3 years Big Data development (6+ years experience)
- âœ… 1-2 years GCP/Azure cloud platforms (extensive experience)
- âœ… Python, Scala, PySpark expertise (comprehensive experience)
- âœ… Distributed data processing pipelines (core experience)
- âœ… Data governance (metadata management, security, compliance)
- âœ… Complex query optimization (proven expertise)
- âœ… Technical leadership (team guidance and best practices)

**ATS Keywords Included:**
- Data Engineering, distributed data processing, big data technologies
- Python, PySpark, Scala, SQL, complex queries
- Cloud platforms, scalability, fault-tolerance, low-latency
- Data governance, metadata management, monitoring, SLAs
